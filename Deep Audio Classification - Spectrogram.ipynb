{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":3194057,"sourceType":"datasetVersion","datasetId":1939154}],"dockerImageVersionId":30627,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# **1. Prepare Data**","metadata":{}},{"cell_type":"markdown","source":"## **1.1 Basic Imports**","metadata":{}},{"cell_type":"code","source":"# Basic python imports\nimport os\nimport shutil\n!pip install tabulate --quiet\nfrom tabulate import tabulate\n\n# Ignore warnings\nimport warnings\nwarnings.filterwarnings(\"ignore\")","metadata":{"execution":{"iopub.status.busy":"2023-12-19T15:41:28.926134Z","iopub.execute_input":"2023-12-19T15:41:28.92649Z","iopub.status.idle":"2023-12-19T15:41:41.195191Z","shell.execute_reply.started":"2023-12-19T15:41:28.926465Z","shell.execute_reply":"2023-12-19T15:41:41.194048Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## **1.2 View the Audio Composition**","metadata":{}},{"cell_type":"code","source":"# Base path to dataset\ndataset_path = os.path.normpath(\"/kaggle/input/z-by-hp-unlocked-challenge-3-signal-processing\")\n\n# Table headers\ntable_headers = [\"Folder\", \"No. of Audio Files\"]\ntable_data = []\n\n# Traverse over the folders\nfor folder in os.listdir(dataset_path):\n    # Update the list with folder and audio count\n    table_data.append([\n        os.path.join(dataset_path, folder),\n        len(os.listdir(os.path.join(dataset_path, folder)))\n    ])\n\n# Print the table\nprint(tabulate(table_data, table_headers, tablefmt=\"grid\"))","metadata":{"execution":{"iopub.status.busy":"2023-12-19T15:41:41.197019Z","iopub.execute_input":"2023-12-19T15:41:41.197316Z","iopub.status.idle":"2023-12-19T15:41:41.475546Z","shell.execute_reply.started":"2023-12-19T15:41:41.197292Z","shell.execute_reply":"2023-12-19T15:41:41.474652Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# **2. Install and Import Dependencies**","metadata":{}},{"cell_type":"markdown","source":"## **2.1 Install Dependencies**","metadata":{}},{"cell_type":"code","source":"# Install packages\n!pip uninstall tensorflow --quiet --yes\n!pip uninstall tensorflow-io --quiet --yes\n\n!pip install tensorflow==2.10.0 tensorflow-io==0.27.0 --quiet","metadata":{"execution":{"iopub.status.busy":"2023-12-19T15:41:41.476809Z","iopub.execute_input":"2023-12-19T15:41:41.477163Z","iopub.status.idle":"2023-12-19T15:42:45.035163Z","shell.execute_reply.started":"2023-12-19T15:41:41.477131Z","shell.execute_reply":"2023-12-19T15:42:45.034084Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## **2.2 Load Dependencies**","metadata":{}},{"cell_type":"code","source":"# Import requried packages\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\nimport tensorflow_io as tfio","metadata":{"execution":{"iopub.status.busy":"2023-12-19T15:42:45.037553Z","iopub.execute_input":"2023-12-19T15:42:45.037874Z","iopub.status.idle":"2023-12-19T15:42:49.611068Z","shell.execute_reply.started":"2023-12-19T15:42:45.037845Z","shell.execute_reply":"2023-12-19T15:42:49.609773Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# **3. Build Data Loading Function**","metadata":{}},{"cell_type":"markdown","source":"## **3.1 Define Paths to Files**","metadata":{}},{"cell_type":"code","source":"# Define paths\nCAPUCHIN_FILE = os.path.join(dataset_path, \"Parsed_Capuchinbird_Clips\", \"XC3776-3.wav\")\nNON_CAPUCHIN_FILE = os.path.join(dataset_path, \"Parsed_Not_Capuchinbird_Clips\", \"afternoon-birds-song-in-forest-26.wav\")","metadata":{"execution":{"iopub.status.busy":"2023-12-19T15:42:49.613148Z","iopub.execute_input":"2023-12-19T15:42:49.614297Z","iopub.status.idle":"2023-12-19T15:42:49.618355Z","shell.execute_reply.started":"2023-12-19T15:42:49.614269Z","shell.execute_reply":"2023-12-19T15:42:49.617493Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# View paths\nCAPUCHIN_FILE, NON_CAPUCHIN_FILE","metadata":{"execution":{"iopub.status.busy":"2023-12-19T15:42:49.619611Z","iopub.execute_input":"2023-12-19T15:42:49.620291Z","iopub.status.idle":"2023-12-19T15:42:54.618017Z","shell.execute_reply.started":"2023-12-19T15:42:49.620256Z","shell.execute_reply":"2023-12-19T15:42:54.617154Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## **3.2 Build a Dataloading Function**","metadata":{}},{"cell_type":"code","source":"# Function to load and process the data\ndef load_wav_16k_mono(filename):\n    # Load the encoded wav file\n    file_contents = tf.io.read_file(filename)\n\n    # Decode wav (tensors by channels)\n    wav, sample_rate = tf.audio.decode_wav(file_contents, desired_channels=1)\n\n    # Remove trailing axis\n    wav = tf.squeeze(wav, axis=-1)\n\n    # Cast the sample rate\n    sample_rate = tf.cast(sample_rate, dtype=tf.int64)\n\n    # Goes from 44100hz to 16000hz - amplitude of the audio signal\n    wav = tfio.audio.resample(wav, rate_in=sample_rate, rate_out=16000)\n\n    # Return the wave data\n    return wav","metadata":{"execution":{"iopub.status.busy":"2023-12-19T15:42:54.619089Z","iopub.execute_input":"2023-12-19T15:42:54.619368Z","iopub.status.idle":"2023-12-19T15:42:54.627051Z","shell.execute_reply.started":"2023-12-19T15:42:54.619344Z","shell.execute_reply":"2023-12-19T15:42:54.626048Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## **3.3. Plot Wave**","metadata":{}},{"cell_type":"code","source":"# Get the wave data for the files\nwave = load_wav_16k_mono(CAPUCHIN_FILE)\nnwave = load_wav_16k_mono(NON_CAPUCHIN_FILE)","metadata":{"execution":{"iopub.status.busy":"2023-12-19T15:42:54.628186Z","iopub.execute_input":"2023-12-19T15:42:54.628467Z","iopub.status.idle":"2023-12-19T15:42:56.851221Z","shell.execute_reply.started":"2023-12-19T15:42:54.628443Z","shell.execute_reply":"2023-12-19T15:42:56.850381Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Plot the waves\nplt.plot(wave)\nplt.plot(nwave)\nplt.legend([\"Capuchin\", \"Non Capuchin\"])\nplt.title(\"Audio Wave Visualization\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-12-19T15:42:56.85249Z","iopub.execute_input":"2023-12-19T15:42:56.853174Z","iopub.status.idle":"2023-12-19T15:42:57.379994Z","shell.execute_reply.started":"2023-12-19T15:42:56.853138Z","shell.execute_reply":"2023-12-19T15:42:57.379018Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# **4. Create Tensorflow Dataset**","metadata":{}},{"cell_type":"markdown","source":"## **4.1 Define Paths to Positive and Negative Data**","metadata":{}},{"cell_type":"code","source":"# Define the paths\nPOS = os.path.join(dataset_path, \"Parsed_Capuchinbird_Clips\")\nNEG = os.path.join(dataset_path, \"Parsed_Not_Capuchinbird_Clips\")","metadata":{"execution":{"iopub.status.busy":"2023-12-19T15:42:57.384143Z","iopub.execute_input":"2023-12-19T15:42:57.384468Z","iopub.status.idle":"2023-12-19T15:42:57.389137Z","shell.execute_reply.started":"2023-12-19T15:42:57.384441Z","shell.execute_reply":"2023-12-19T15:42:57.388187Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Print the path\nPOS, NEG","metadata":{"execution":{"iopub.status.busy":"2023-12-19T15:42:57.390475Z","iopub.execute_input":"2023-12-19T15:42:57.39084Z","iopub.status.idle":"2023-12-19T15:42:57.401272Z","shell.execute_reply.started":"2023-12-19T15:42:57.390808Z","shell.execute_reply":"2023-12-19T15:42:57.40043Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## **4.2 Create Tensorflow Datasets**","metadata":{}},{"cell_type":"code","source":"# Prepare the dataset\nPOS_DF = tf.data.Dataset.list_files(os.path.join(POS, \"*.wav\"))\nNEG_DF = tf.data.Dataset.list_files(os.path.join(NEG, \"*.wav\"))","metadata":{"execution":{"iopub.status.busy":"2023-12-19T15:42:57.402329Z","iopub.execute_input":"2023-12-19T15:42:57.40269Z","iopub.status.idle":"2023-12-19T15:42:57.557587Z","shell.execute_reply.started":"2023-12-19T15:42:57.402642Z","shell.execute_reply":"2023-12-19T15:42:57.55683Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## **4.3 Add Label and Combine Positive and Negative Samples**","metadata":{}},{"cell_type":"code","source":"# Assign lables to data\npositives = tf.data.Dataset.zip((POS_DF, tf.data.Dataset.from_tensor_slices(tf.ones(len(POS_DF)))))\nnegatives = tf.data.Dataset.zip((NEG_DF, tf.data.Dataset.from_tensor_slices(tf.zeros(len(NEG_DF)))))","metadata":{"execution":{"iopub.status.busy":"2023-12-19T15:42:57.558695Z","iopub.execute_input":"2023-12-19T15:42:57.558955Z","iopub.status.idle":"2023-12-19T15:42:57.572591Z","shell.execute_reply.started":"2023-12-19T15:42:57.558933Z","shell.execute_reply":"2023-12-19T15:42:57.571833Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Join the datasets\ndata = positives.concatenate(negatives)","metadata":{"execution":{"iopub.status.busy":"2023-12-19T15:42:57.573796Z","iopub.execute_input":"2023-12-19T15:42:57.574152Z","iopub.status.idle":"2023-12-19T15:42:57.580898Z","shell.execute_reply.started":"2023-12-19T15:42:57.574118Z","shell.execute_reply":"2023-12-19T15:42:57.579852Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# **5. Determine Average Length of a Capuchinbird Call**","metadata":{}},{"cell_type":"markdown","source":"## **5.1 Calculate Wave Cycle Length**","metadata":{}},{"cell_type":"code","source":"# List to store the lengths of the Capuchin bird calls\nlengths = []\n\n# Traveres over the audio files\nfor file in os.listdir(POS):\n    # Get the wave for the audio file\n    tensor_wave = load_wav_16k_mono(os.path.join(POS, file))\n\n    # Update the list\n    lengths.append(len(tensor_wave))","metadata":{"execution":{"iopub.status.busy":"2023-12-19T15:42:57.582317Z","iopub.execute_input":"2023-12-19T15:42:57.582651Z","iopub.status.idle":"2023-12-19T15:43:09.844937Z","shell.execute_reply.started":"2023-12-19T15:42:57.582621Z","shell.execute_reply":"2023-12-19T15:43:09.844122Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## **5.2 Calculate Mean, Min and Max**","metadata":{}},{"cell_type":"code","source":"# Imports\nimport pandas as pd\n\n# Conver the length to pandas series\nlengths = pd.Series(lengths)","metadata":{"execution":{"iopub.status.busy":"2023-12-19T15:43:09.846076Z","iopub.execute_input":"2023-12-19T15:43:09.846328Z","iopub.status.idle":"2023-12-19T15:43:09.85395Z","shell.execute_reply.started":"2023-12-19T15:43:09.846306Z","shell.execute_reply":"2023-12-19T15:43:09.85301Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Stats for the lengths\nlengths.describe()","metadata":{"execution":{"iopub.status.busy":"2023-12-19T15:43:09.855007Z","iopub.execute_input":"2023-12-19T15:43:09.85526Z","iopub.status.idle":"2023-12-19T15:43:09.87485Z","shell.execute_reply.started":"2023-12-19T15:43:09.855238Z","shell.execute_reply":"2023-12-19T15:43:09.873987Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Stats for the time\n(lengths / 16000).describe()","metadata":{"execution":{"iopub.status.busy":"2023-12-19T15:43:09.875935Z","iopub.execute_input":"2023-12-19T15:43:09.8762Z","iopub.status.idle":"2023-12-19T15:43:09.888538Z","shell.execute_reply.started":"2023-12-19T15:43:09.876177Z","shell.execute_reply":"2023-12-19T15:43:09.887644Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# **6. Build Preprocessing Function to Convert to Spectrogram**","metadata":{}},{"cell_type":"markdown","source":"## **6.1 Build Preprocessing Function**","metadata":{}},{"cell_type":"code","source":"# Function to convert to image\ndef preprocess(file_path, label):\n    # Get the wave\n    wav = load_wav_16k_mono(file_path)\n\n    # Get the first 56000 audio signals for every audio\n    wav = wav[:56000]\n\n    # Create zero padding\n    zero_padding = tf.zeros([56000] - tf.shape(wav), dtype=tf.float32)\n\n    # Join the wave and padding\n    wav = tf.concat([zero_padding, wav], 0)\n\n    # Get the spectrogram\n    # Get the absolute values\n    # Expand the dimensions\n    spectrogram = tf.expand_dims(tf.abs(tf.signal.stft(wav, frame_length=320, frame_step=32)), axis=2)\n\n    # Return the spectrogram and label\n    return spectrogram, label","metadata":{"execution":{"iopub.status.busy":"2023-12-19T15:43:09.890356Z","iopub.execute_input":"2023-12-19T15:43:09.890713Z","iopub.status.idle":"2023-12-19T15:43:09.898174Z","shell.execute_reply.started":"2023-12-19T15:43:09.890681Z","shell.execute_reply":"2023-12-19T15:43:09.897332Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## **6.2 Test Out the Function and Viz the Spectrogram**","metadata":{}},{"cell_type":"code","source":"# Get three random items from the positives\nnum_images = 3\nrandom_positives = positives.shuffle(buffer_size=10000).take(num_images)\npos_iter = random_positives.as_numpy_iterator()\n\n# Create subplots with reduced vertical space\nfig, axs = plt.subplots(num_images, 1, figsize=(30, 5 * num_images))\n\n# Process and visualize each image\nfor i in range(num_images):\n    pos_filepath, pos_label = pos_iter.next()\n    pos_spectrogram, pos_label = preprocess(pos_filepath, pos_label)\n\n    # Display the image\n    axs[i].imshow(tf.transpose(pos_spectrogram)[0])\n    axs[i].axis('off')\n\n# Set title\naxs[0].set_title(\"Capuchin Bird Call\")\n\n# Show the plot\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-12-19T15:43:09.899299Z","iopub.execute_input":"2023-12-19T15:43:09.899566Z","iopub.status.idle":"2023-12-19T15:43:11.177788Z","shell.execute_reply.started":"2023-12-19T15:43:09.899544Z","shell.execute_reply":"2023-12-19T15:43:11.176892Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Get three random items from the negatives\nnum_images = 3\nrandom_negatives = negatives.shuffle(buffer_size=10000).take(num_images)\nneg_iter = random_negatives.as_numpy_iterator()\n\n# Create subplots with reduced vertical space\nfig, axs = plt.subplots(num_images, 1, figsize=(30, 5 * num_images))\n\n# Process and visualize each image\nfor i in range(num_images):\n    neg_filepath, neg_label = neg_iter.next()\n    neg_spectrogram, neg_label = preprocess(neg_filepath, neg_label)\n\n    # Display the image\n    axs[i].imshow(tf.transpose(neg_spectrogram)[0])\n    axs[i].axis('off')\n\n# Set title\naxs[0].set_title(\"Non Capuchin Bird Call\")\n\n# Show the plot\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-12-19T15:43:11.179009Z","iopub.execute_input":"2023-12-19T15:43:11.179337Z","iopub.status.idle":"2023-12-19T15:43:12.36162Z","shell.execute_reply.started":"2023-12-19T15:43:11.179308Z","shell.execute_reply":"2023-12-19T15:43:12.360643Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# **7. Create Training and Testing Partitions**","metadata":{}},{"cell_type":"markdown","source":"## **7.1 Create a Tensorflow Data Pipelines**","metadata":{}},{"cell_type":"code","source":"# Pass the data through all the steps\ndata = data.map(preprocess)\ndata = data.cache()\ndata = data.shuffle(buffer_size=1000)\ndata = data.batch(16)\ndata = data.prefetch(8)","metadata":{"execution":{"iopub.status.busy":"2023-12-19T15:43:12.362761Z","iopub.execute_input":"2023-12-19T15:43:12.363054Z","iopub.status.idle":"2023-12-19T15:43:12.97631Z","shell.execute_reply.started":"2023-12-19T15:43:12.36303Z","shell.execute_reply":"2023-12-19T15:43:12.975324Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## **7.2 Split into Training and Testing Partitions**","metadata":{}},{"cell_type":"code","source":"# Get the train and test sets\ntrain_df = data.take(36)\ntest_df = data.skip(36).take(15)","metadata":{"execution":{"iopub.status.busy":"2023-12-19T15:43:12.977488Z","iopub.execute_input":"2023-12-19T15:43:12.977806Z","iopub.status.idle":"2023-12-19T15:43:12.98512Z","shell.execute_reply.started":"2023-12-19T15:43:12.977782Z","shell.execute_reply":"2023-12-19T15:43:12.984339Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## **7.3 View Sample Batch**","metadata":{}},{"cell_type":"code","source":"# # Get a sample\n# samples, labels = train_df.as_numpy_iterator().next()\n\n# # View the shape of samples\n# samples.shape","metadata":{"execution":{"iopub.status.busy":"2023-12-19T15:43:12.986198Z","iopub.execute_input":"2023-12-19T15:43:12.986563Z","iopub.status.idle":"2023-12-19T15:43:12.994294Z","shell.execute_reply.started":"2023-12-19T15:43:12.986531Z","shell.execute_reply":"2023-12-19T15:43:12.993531Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# **8. Build Deep Learning Model**","metadata":{}},{"cell_type":"markdown","source":"## **8.1 Load Tensorflow Dependencies**","metadata":{}},{"cell_type":"code","source":"# Imports\nfrom tensorflow.keras import *","metadata":{"execution":{"iopub.status.busy":"2023-12-19T15:43:12.995478Z","iopub.execute_input":"2023-12-19T15:43:12.996147Z","iopub.status.idle":"2023-12-19T15:43:13.006036Z","shell.execute_reply.started":"2023-12-19T15:43:12.996115Z","shell.execute_reply":"2023-12-19T15:43:13.005217Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## **8.2 Build Sequential Model, Compile and View Summary**","metadata":{}},{"cell_type":"code","source":"# Initialize a sequential model\nmodel = models.Sequential()\n\n# Convolutional block 1\nmodel.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(1741, 257, 1)))\nmodel.add(layers.BatchNormalization())\nmodel.add(layers.SpatialDropout2D(0.25))\nmodel.add(layers.MaxPooling2D((2, 2)))\n\n# Convolutional block 2\nmodel.add(layers.Conv2D(64, (3, 3), activation='relu'))\nmodel.add(layers.BatchNormalization())\nmodel.add(layers.SpatialDropout2D(0.25))\nmodel.add(layers.MaxPooling2D((2, 2)))\n\n# Convolutional block 3\nmodel.add(layers.Conv2D(128, (3, 3), activation='relu'))\nmodel.add(layers.BatchNormalization())\nmodel.add(layers.SpatialDropout2D(0.25))\nmodel.add(layers.MaxPooling2D((2, 2)))\n\n# Flatten layer to transition from convolutional to dense layers\nmodel.add(layers.Flatten())\n\n# Dense layers with dropout for regularization\nmodel.add(layers.Dense(256, activation='relu'))\nmodel.add(layers.BatchNormalization())\nmodel.add(layers.Dropout(0.5))\n\n# Output layer\nmodel.add(layers.Dense(1, activation='sigmoid'))","metadata":{"execution":{"iopub.status.busy":"2023-12-19T15:43:13.007277Z","iopub.execute_input":"2023-12-19T15:43:13.007603Z","iopub.status.idle":"2023-12-19T15:43:13.179499Z","shell.execute_reply.started":"2023-12-19T15:43:13.007573Z","shell.execute_reply":"2023-12-19T15:43:13.178727Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Using the Adam Optimizer with a specific learning rate\nopt = optimizers.Adam(learning_rate=1e-4)\n\n# Using BinaryCrossentropy as the loss function\nloss_function = losses.BinaryCrossentropy()\n\n# Compiling the model with BinaryCrossentropy loss, Adam optimizer, and additional metrics\nmodel.compile(loss=loss_function, optimizer=opt, metrics=['accuracy', metrics.Precision(), metrics.Recall()])","metadata":{"execution":{"iopub.status.busy":"2023-12-19T15:43:13.18051Z","iopub.execute_input":"2023-12-19T15:43:13.180807Z","iopub.status.idle":"2023-12-19T15:43:13.201963Z","shell.execute_reply.started":"2023-12-19T15:43:13.180783Z","shell.execute_reply":"2023-12-19T15:43:13.201211Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Viewing the summary of the model\nmodel.summary()","metadata":{"execution":{"iopub.status.busy":"2023-12-19T15:43:13.20644Z","iopub.execute_input":"2023-12-19T15:43:13.206737Z","iopub.status.idle":"2023-12-19T15:43:13.254441Z","shell.execute_reply.started":"2023-12-19T15:43:13.206713Z","shell.execute_reply":"2023-12-19T15:43:13.253596Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Plot the model\nutils.plot_model(\n    model,\n    show_shapes=True,\n    show_layer_names=True,\n    expand_nested=True,\n    show_layer_activations=True,\n    dpi=300,\n)","metadata":{"execution":{"iopub.status.busy":"2023-12-19T15:43:13.255506Z","iopub.execute_input":"2023-12-19T15:43:13.255786Z","iopub.status.idle":"2023-12-19T15:43:14.295279Z","shell.execute_reply.started":"2023-12-19T15:43:13.255763Z","shell.execute_reply":"2023-12-19T15:43:14.294291Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## **8.3 Add Model Callbacks**","metadata":{}},{"cell_type":"code","source":"# File Path to store the trained models\nfilepath = \"./CNN-Models/model_{epoch:02d}-{val_accuracy:.2f}.h5\"\n\n# ModelCheckpoint callback to save the best model based on validation accuracy\ncheckpoint = callbacks.ModelCheckpoint(filepath, monitor='val_accuracy', verbose=1, save_best_only=True, mode='max')\n\n# Early stopping callback with patience of 5\nearly_stopping = callbacks.EarlyStopping(monitor='val_accuracy', patience=2, verbose=1)\n\n# Learning rate decay callback using LearningRateScheduler\ndef lr_schedule(epoch):\n    initial_lr = 1e-4\n    decay_factor = 0.9\n    decay_step = 10\n    lr = initial_lr * (decay_factor ** (epoch // decay_step))\n    return lr\n\n# Initialize the learning rate scheduler\nlr_decay = callbacks.LearningRateScheduler(lr_schedule)\n\n# List of callbacks including ModelCheckpoint and LearningRateScheduler\ncallbacks_list = [early_stopping, checkpoint, lr_decay]","metadata":{"execution":{"iopub.status.busy":"2023-12-19T15:43:14.296564Z","iopub.execute_input":"2023-12-19T15:43:14.296893Z","iopub.status.idle":"2023-12-19T15:43:14.303557Z","shell.execute_reply.started":"2023-12-19T15:43:14.296867Z","shell.execute_reply":"2023-12-19T15:43:14.30255Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## **8.4 Train the Model**","metadata":{}},{"cell_type":"code","source":"# Clear RAM\nimport gc\ngc.collect()","metadata":{"execution":{"iopub.status.busy":"2023-12-19T15:43:14.304854Z","iopub.execute_input":"2023-12-19T15:43:14.305236Z","iopub.status.idle":"2023-12-19T15:43:14.501621Z","shell.execute_reply.started":"2023-12-19T15:43:14.305206Z","shell.execute_reply":"2023-12-19T15:43:14.500695Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Triaining the model\nhistory = model.fit(train_df, epochs=5, validation_data=test_df, callbacks=callbacks_list)","metadata":{"execution":{"iopub.status.busy":"2023-12-19T15:43:14.502989Z","iopub.execute_input":"2023-12-19T15:43:14.50378Z","iopub.status.idle":"2023-12-19T15:44:24.216797Z","shell.execute_reply.started":"2023-12-19T15:43:14.50375Z","shell.execute_reply":"2023-12-19T15:44:24.215826Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Clear the RAM\ngc.collect()","metadata":{"execution":{"iopub.status.busy":"2023-12-19T15:44:24.218084Z","iopub.execute_input":"2023-12-19T15:44:24.21847Z","iopub.status.idle":"2023-12-19T15:44:24.434407Z","shell.execute_reply.started":"2023-12-19T15:44:24.218442Z","shell.execute_reply":"2023-12-19T15:44:24.433463Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## **8.5 Visualize Model Performance**","metadata":{}},{"cell_type":"code","source":"# Assuming 'history' is your pandas DataFrame\nmetrics = ['loss', 'accuracy', 'precision', 'recall']\ncolors = ['red', 'blue']\n\n# Create a figure and axis for subplots\nfig, axes = plt.subplots(nrows=2, ncols=2, figsize=(8, 6))\nfig.suptitle('Metrics Over Training')\n\n# Traverse over metrics\nfor i, metric in enumerate(metrics):\n    # Get the quotient and remainder\n    row, col = divmod(i, 2)\n\n    # Plot training values\n    axes[row, col].plot(history.history[metric], color=colors[0], label=f'Training {metric.capitalize()}')\n\n    # Plot validation values\n    axes[row, col].plot(history.history[f'val_{metric}'], color=colors[1], label=f'Validation {metric.capitalize()}')\n\n    # Get the title and legend\n    axes[row, col].set_title(metric.capitalize())\n    axes[row, col].legend()\n\n# Adjust layout and display\nplt.tight_layout(rect=[0, 0, 1, 0.96])\n\n# Show the plot\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-12-19T15:44:24.435571Z","iopub.execute_input":"2023-12-19T15:44:24.435942Z","iopub.status.idle":"2023-12-19T15:44:26.202109Z","shell.execute_reply.started":"2023-12-19T15:44:24.435915Z","shell.execute_reply":"2023-12-19T15:44:26.201246Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# **9. Make a Prediction on a Single Clip**","metadata":{}},{"cell_type":"markdown","source":"## **9.1 Get One Batch and Make a Prediction**","metadata":{}},{"cell_type":"code","source":"# Get a sample from the test set\nX_test, y_test = test_df.as_numpy_iterator().next()","metadata":{"execution":{"iopub.status.busy":"2023-12-19T15:44:26.203365Z","iopub.execute_input":"2023-12-19T15:44:26.203742Z","iopub.status.idle":"2023-12-19T15:44:26.510062Z","shell.execute_reply.started":"2023-12-19T15:44:26.203707Z","shell.execute_reply":"2023-12-19T15:44:26.509212Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# View shape of data\nX_test.shape, y_test.shape","metadata":{"execution":{"iopub.status.busy":"2023-12-19T15:44:26.5113Z","iopub.execute_input":"2023-12-19T15:44:26.512008Z","iopub.status.idle":"2023-12-19T15:44:26.518021Z","shell.execute_reply.started":"2023-12-19T15:44:26.511973Z","shell.execute_reply":"2023-12-19T15:44:26.517112Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Get the predictions\ny_pred = model.predict(X_test)\n\n# View the predictions\ny_pred","metadata":{"execution":{"iopub.status.busy":"2023-12-19T15:44:26.519398Z","iopub.execute_input":"2023-12-19T15:44:26.519796Z","iopub.status.idle":"2023-12-19T15:44:26.806557Z","shell.execute_reply.started":"2023-12-19T15:44:26.519764Z","shell.execute_reply":"2023-12-19T15:44:26.805649Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## **9.2 Convert Logits to Classes**","metadata":{}},{"cell_type":"code","source":"# Import\nimport numpy as np\n\n# Flatten the predictions and convert to classes\ny_pred = np.round(y_pred.flatten())\n\n# View the new y_pred\ny_pred","metadata":{"execution":{"iopub.status.busy":"2023-12-19T15:44:26.807842Z","iopub.execute_input":"2023-12-19T15:44:26.808203Z","iopub.status.idle":"2023-12-19T15:44:26.816155Z","shell.execute_reply.started":"2023-12-19T15:44:26.808169Z","shell.execute_reply":"2023-12-19T15:44:26.815109Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Print out the actual values\ny_test","metadata":{"execution":{"iopub.status.busy":"2023-12-19T15:44:26.817454Z","iopub.execute_input":"2023-12-19T15:44:26.817834Z","iopub.status.idle":"2023-12-19T15:44:26.826594Z","shell.execute_reply.started":"2023-12-19T15:44:26.817809Z","shell.execute_reply":"2023-12-19T15:44:26.825708Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## **9.3 Classification Report**","metadata":{}},{"cell_type":"code","source":"# Import\nfrom sklearn.metrics import classification_report\n\n# Print the classification report\nprint(classification_report(y_test, y_pred))","metadata":{"execution":{"iopub.status.busy":"2023-12-19T15:44:26.827777Z","iopub.execute_input":"2023-12-19T15:44:26.828101Z","iopub.status.idle":"2023-12-19T15:44:27.184919Z","shell.execute_reply.started":"2023-12-19T15:44:26.82807Z","shell.execute_reply":"2023-12-19T15:44:27.183675Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# **10. Load Model**","metadata":{}},{"cell_type":"markdown","source":"## **10.1 Load Pretrained Model**","metadata":{}},{"cell_type":"code","source":"# Imports\nimport glob\n\n# Traverse the folder to find the best model\nbest_model = sorted(glob.glob(\"/kaggle/working/CNN-Models/*.h5\"))[-1]\n\n# Load the model\nmodel = tf.keras.models.load_model(best_model)\n\n# View model summary\nmodel.summary()","metadata":{"execution":{"iopub.status.busy":"2023-12-19T15:58:55.249723Z","iopub.execute_input":"2023-12-19T15:58:55.250554Z","iopub.status.idle":"2023-12-19T15:59:08.860095Z","shell.execute_reply.started":"2023-12-19T15:58:55.250521Z","shell.execute_reply":"2023-12-19T15:59:08.85921Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# **11. Build Forest Parsing Functions**","metadata":{}},{"cell_type":"markdown","source":"## **11.1 Load Up MP3s**","metadata":{}},{"cell_type":"code","source":"# Function to load the MP3 audio files\ndef load_mp3_16k_mono(filename):\n    # Load the audio file\n    res = tfio.audio.AudioIOTensor(filename)\n\n    # Convert to tensor and combine channels\n    tensor =  tf.math.reduce_sum(res.to_tensor(), axis=1) / 2\n\n    # Extract sample rate and cast\n    sample_rate = tf.cast(res.rate, dtype=tf.int64)\n\n    # Resample to 16k hz\n    wav = tfio.audio.resample(tensor, rate_in=sample_rate, rate_out=16000)\n\n    # Return the wav\n    return wav","metadata":{"execution":{"iopub.status.busy":"2023-12-19T15:46:09.492586Z","iopub.execute_input":"2023-12-19T15:46:09.492987Z","iopub.status.idle":"2023-12-19T15:46:09.499101Z","shell.execute_reply.started":"2023-12-19T15:46:09.492957Z","shell.execute_reply":"2023-12-19T15:46:09.498058Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Load a sample file using the function\nwav = load_mp3_16k_mono(\"/kaggle/input/z-by-hp-unlocked-challenge-3-signal-processing/Forest Recordings/recording_00.mp3\")\n\n# View the data\nwav","metadata":{"execution":{"iopub.status.busy":"2023-12-19T15:46:30.331785Z","iopub.execute_input":"2023-12-19T15:46:30.332287Z","iopub.status.idle":"2023-12-19T15:46:30.943222Z","shell.execute_reply.started":"2023-12-19T15:46:30.33224Z","shell.execute_reply":"2023-12-19T15:46:30.942114Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## **11.2 Slick the Sample Audio**","metadata":{}},{"cell_type":"code","source":"# Slice the audio file into multiple segments\naudio_slices = tf.keras.utils.timeseries_dataset_from_array(wav, wav, sequence_length=56000, sequence_stride=56000, batch_size=1)\n\n# Extract the sample from from the slices\nsample, idx = audio_slices.as_numpy_iterator().next()","metadata":{"execution":{"iopub.status.busy":"2023-12-19T15:47:04.73186Z","iopub.execute_input":"2023-12-19T15:47:04.732542Z","iopub.status.idle":"2023-12-19T15:47:04.877518Z","shell.execute_reply.started":"2023-12-19T15:47:04.732508Z","shell.execute_reply":"2023-12-19T15:47:04.876722Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# View the shape of audio and number of audio slices\nsample.shape, len(audio_slices)","metadata":{"execution":{"iopub.status.busy":"2023-12-19T15:47:04.946128Z","iopub.execute_input":"2023-12-19T15:47:04.946931Z","iopub.status.idle":"2023-12-19T15:47:04.953399Z","shell.execute_reply.started":"2023-12-19T15:47:04.946897Z","shell.execute_reply":"2023-12-19T15:47:04.952472Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## **11.3 Function to Convert Clips into Windowed Spectrograms**","metadata":{}},{"cell_type":"code","source":"# Function to preprocess and get the spectrogram\ndef preprocess_mp3(sample, idx):\n    # Get the sample\n    sample = sample[0]\n\n    # Add the zero padding\n    zero_padding = tf.zeros([56000] - tf.shape(sample), dtype=tf.float32)\n\n    # Get the padded wave\n    wav = tf.concat([zero_padding, sample], 0)\n\n    # Get the spectrogram\n    spectrogram = tf.expand_dims(tf.abs(tf.signal.stft(wav, frame_length=320, frame_step=32)), axis=2)\n\n    # Return the spectrogram\n    return spectrogram","metadata":{"execution":{"iopub.status.busy":"2023-12-19T15:47:30.097488Z","iopub.execute_input":"2023-12-19T15:47:30.098197Z","iopub.status.idle":"2023-12-19T15:47:30.105308Z","shell.execute_reply.started":"2023-12-19T15:47:30.098162Z","shell.execute_reply":"2023-12-19T15:47:30.104138Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## **11.4 Convert Longer Clips into Windows**","metadata":{}},{"cell_type":"code","source":"# Slice the audio file into multiple segments\naudio_slices = tf.keras.utils.timeseries_dataset_from_array(wav, wav, sequence_length=56000, sequence_stride=56000, batch_size=1)\n\n# Map the audio slices to the function\naudio_slices = audio_slices.map(preprocess_mp3)\n\n# Batch the slices\naudio_slices = audio_slices.batch(64)","metadata":{"execution":{"iopub.status.busy":"2023-12-19T15:47:49.847552Z","iopub.execute_input":"2023-12-19T15:47:49.847938Z","iopub.status.idle":"2023-12-19T15:47:50.060798Z","shell.execute_reply.started":"2023-12-19T15:47:49.84791Z","shell.execute_reply":"2023-12-19T15:47:50.060016Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# **12. Prediction on Sample**","metadata":{}},{"cell_type":"markdown","source":"## **12.1 Make Predictions on Sample**","metadata":{}},{"cell_type":"code","source":"# Imports\nimport numpy as np\n\n# Get the predictions\npred_labels = model.predict(audio_slices)","metadata":{"execution":{"iopub.status.busy":"2023-12-19T15:48:20.319114Z","iopub.execute_input":"2023-12-19T15:48:20.319476Z","iopub.status.idle":"2023-12-19T15:48:21.77693Z","shell.execute_reply.started":"2023-12-19T15:48:20.319446Z","shell.execute_reply":"2023-12-19T15:48:21.776141Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Round the predictions\npred_labels = np.round(pred_labels.flatten())\n\n# # Round the predictions with increased confidence\n# pred_labels = np.where(pred_labels.flatten() > 0.8, 1, 0)","metadata":{"execution":{"iopub.status.busy":"2023-12-19T15:48:25.196502Z","iopub.execute_input":"2023-12-19T15:48:25.197404Z","iopub.status.idle":"2023-12-19T15:48:25.202174Z","shell.execute_reply.started":"2023-12-19T15:48:25.197359Z","shell.execute_reply":"2023-12-19T15:48:25.201188Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# View the predicted labels\nlen(pred_labels), pred_labels","metadata":{"execution":{"iopub.status.busy":"2023-12-19T15:48:29.411959Z","iopub.execute_input":"2023-12-19T15:48:29.412914Z","iopub.status.idle":"2023-12-19T15:48:29.419429Z","shell.execute_reply.started":"2023-12-19T15:48:29.412877Z","shell.execute_reply":"2023-12-19T15:48:29.418524Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Get the sum to count the number of times the bird sound was found\nnp.sum(pred_labels)","metadata":{"execution":{"iopub.status.busy":"2023-12-19T15:48:34.185183Z","iopub.execute_input":"2023-12-19T15:48:34.185562Z","iopub.status.idle":"2023-12-19T15:48:34.192159Z","shell.execute_reply.started":"2023-12-19T15:48:34.18553Z","shell.execute_reply":"2023-12-19T15:48:34.191111Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## **12.2 Group Consecutive Detections**","metadata":{}},{"cell_type":"code","source":"# Imports\nfrom itertools import groupby\n\n# Apply the function to predictions\npred_labels = [key for key, group in groupby(pred_labels)]\n\n# View the grouped results\npred_labels","metadata":{"execution":{"iopub.status.busy":"2023-12-19T15:49:02.70163Z","iopub.execute_input":"2023-12-19T15:49:02.702358Z","iopub.status.idle":"2023-12-19T15:49:02.708801Z","shell.execute_reply.started":"2023-12-19T15:49:02.702325Z","shell.execute_reply":"2023-12-19T15:49:02.707911Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Get the sum to get the final number of calls\nnp.sum(pred_labels)","metadata":{"execution":{"iopub.status.busy":"2023-12-19T15:49:09.570327Z","iopub.execute_input":"2023-12-19T15:49:09.570716Z","iopub.status.idle":"2023-12-19T15:49:09.577152Z","shell.execute_reply.started":"2023-12-19T15:49:09.570677Z","shell.execute_reply":"2023-12-19T15:49:09.576215Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# **13. Prediction on All Forest Recordings**","metadata":{}},{"cell_type":"markdown","source":"## **13.1 Get the Number of Calls in Each Recording**","metadata":{}},{"cell_type":"code","source":"# Import\nfrom tqdm import tqdm_notebook\n\n# List to store the results\nresults = []\n\n# Traverse over the folder for files\nfor file in tqdm_notebook(os.listdir(\"/kaggle/input/z-by-hp-unlocked-challenge-3-signal-processing/Forest Recordings\")):\n    # Get the path to file\n    file_path = os.path.join(\"/kaggle/input/z-by-hp-unlocked-challenge-3-signal-processing/Forest Recordings\", file)\n\n    # Get the wave for the file\n    wav = load_mp3_16k_mono(file_path)\n\n    # Get the audio slices\n    audio_slices = tf.keras.utils.timeseries_dataset_from_array(\n        wav, wav,\n        sequence_length=56000,\n        sequence_stride=56000,\n        batch_size=1\n    ).map(preprocess_mp3).batch(64)\n\n    # Get the predictions\n    pred_labels = model.predict(audio_slices)\n\n    # Round the predictions\n    pred_labels = np.round(pred_labels.flatten())\n\n    # Group consecutive calls\n    pred_labels = [key for key, group in groupby(pred_labels)]\n\n    # Get the sum to get the final number of calls\n    total_calls = np.sum(pred_labels)\n\n    # Add data to file\n    results.append((file, total_calls))","metadata":{"execution":{"iopub.status.busy":"2023-12-19T15:50:19.702196Z","iopub.execute_input":"2023-12-19T15:50:19.702568Z","iopub.status.idle":"2023-12-19T15:52:34.545981Z","shell.execute_reply.started":"2023-12-19T15:50:19.702539Z","shell.execute_reply":"2023-12-19T15:52:34.544966Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## **13.2 Convert the List to Pandas Dataframe**","metadata":{}},{"cell_type":"code","source":"# Imports\nimport pandas as pd\n\n# Create pandas dataframe\nresult_df = pd.DataFrame(results, columns=[\"recording\", \"capuchin_calls\"])","metadata":{"execution":{"iopub.status.busy":"2023-12-19T15:52:34.547586Z","iopub.execute_input":"2023-12-19T15:52:34.547905Z","iopub.status.idle":"2023-12-19T15:52:34.552956Z","shell.execute_reply.started":"2023-12-19T15:52:34.547879Z","shell.execute_reply":"2023-12-19T15:52:34.552054Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# View the dataframe\nresult_df","metadata":{"execution":{"iopub.status.busy":"2023-12-19T15:52:34.55408Z","iopub.execute_input":"2023-12-19T15:52:34.554417Z","iopub.status.idle":"2023-12-19T15:52:34.576151Z","shell.execute_reply.started":"2023-12-19T15:52:34.554385Z","shell.execute_reply":"2023-12-19T15:52:34.575273Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Sort the data by recordings column and reset index\nresult_df = result_df.sort_values(by=[\"recording\"]).reset_index(drop=True)\n\n# View the dataframe\nresult_df","metadata":{"execution":{"iopub.status.busy":"2023-12-19T15:52:34.577869Z","iopub.execute_input":"2023-12-19T15:52:34.578141Z","iopub.status.idle":"2023-12-19T15:52:34.593121Z","shell.execute_reply.started":"2023-12-19T15:52:34.578117Z","shell.execute_reply":"2023-12-19T15:52:34.592334Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Export to csv file\nresult_df.to_csv(\"capuchinbird_results.csv\", index=False)","metadata":{"execution":{"iopub.status.busy":"2023-12-19T15:52:34.614067Z","iopub.execute_input":"2023-12-19T15:52:34.614911Z","iopub.status.idle":"2023-12-19T15:52:34.622192Z","shell.execute_reply.started":"2023-12-19T15:52:34.614885Z","shell.execute_reply":"2023-12-19T15:52:34.621468Z"},"trusted":true},"outputs":[],"execution_count":null}]}