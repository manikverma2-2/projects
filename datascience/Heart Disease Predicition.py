{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":6237267,"sourceType":"datasetVersion","datasetId":3583338}],"dockerImageVersionId":30527,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"<img src=\"https://github.com/FarzadNekouee/Heart_Disease_Prediction/blob/master/image.jpg?raw=true\" width=\"1800\">","metadata":{}},{"cell_type":"markdown","source":"<div style=\"border-radius:10px; padding: 15px; background-color: #facfc8; font-size:120%; text-align:left\">\n\n<h3 align=\"left\"><font color=red>Problem:</font></h3>\n\nIn this project, we delve into a dataset encapsulating various health metrics from __heart patients__, including age, blood pressure, heart rate, and more. Our goal is to develop a predictive model capable of accurately identifying individuals with heart disease. Given the grave implications of missing a positive diagnosis, our primary emphasis is on ensuring that the model identifies all potential patients, making recall for the positive class a crucial metric.","metadata":{}},{"cell_type":"markdown","source":"<div style=\"border-radius:10px; padding: 15px; background-color: #facfc8; font-size:115%; text-align:left\">\n\n<h3 align=\"left\"><font color=red>Objectives:</font></h3>\n\n* __Explore the Dataset__: Uncover patterns, distributions, and relationships within the data.\n* __Conduct Extensive Exploratory Data Analysis (EDA)__: Dive deep into bivariate relationships against the target.\n* __Preprocessing Steps__:\n  - Remove irrelevant features\n  - Address missing values\n  - Treat outliers\n  - Encode categorical variables\n  - Transform skewed features to achieve normal-like distributions\n* __Model Building__:\n  - Establish pipelines for models that require scaling\n  - Implement and tune classification models including KNN, SVM, Decision Trees, and Random Forest\n  - Emphasize achieving high recall for class 1, ensuring comprehensive identification of heart patients\n* __Evaluate and Compare Model Performance__: Utilize precision, recall, and F1-score to gauge models' effectiveness.\n","metadata":{}},{"cell_type":"markdown","source":"<a id=\"contents_tabel\"></a>    \n<div style=\"border-radius:10px; padding: 15px; background-color: #facfc8; font-size:115%; text-align:left\">\n\n<h3 align=\"left\"><font color=red>Table of Contents:</font></h3>\n    \n* [Step 1 | Import Libraries](#import)\n* [Step 2 | Read Dataset](#read)\n* [Step 3 | Dataset Overview](#overview)\n    - [Step 3.1 | Dataset Basic Information](#basic)\n    - [Step 3.2 | Summary Statistics for Numerical Variables](#num_statistics)\n    - [Step 3.3 | Summary Statistics for Categorical Variables](#cat_statistics)\n* [Step 4 | EDA](#eda)\n    - [Step 4.1 | Univariate Analysis](#univariate)\n        - [Step 4.1.1 | Numerical Variables Univariate Analysis](#num_uni)\n        - [Step 4.1.2 | Categorical Variables Univariate Analysis](#cat_uni)\n    - [Step 4.2 | Bivariate Analysis](#bivariate)\n        - [Step 4.2.1 | Numerical Features vs Target](#num_target)\n        - [Step 4.2.2 | Categorical Features vs Target](#cat_target)\n* [Step 5 | Data Preprocessing](#preprocessing)\n    - [Step 5.1 | Irrelevant Features Removal](#feature_removal)\n    - [Step 5.2 | Missing Value Treatment](#missing)\n    - [Step 5.3 | Outlier Treatment](#outlier)\n    - [Step 5.4 | Categorical Features Encoding](#encoding)\n    - [Step 5.5 | Feature Scaling](#scaling)\n    - [Step 5.6 | Transforming Skewed Features](#transform)\n* [Step 6 | Decision Tree Model Building](#dt)\n    - [Step 6.1 | DT Base Model Definition](#dt_base)\n    - [Step 6.2 | DT Hyperparameter Tuning](#dt_hp)\n    - [Step 6.3 | DT Model Evaluation](#dt_eval)\n* [Step 7 | Random Forest Model Building](#rf)\n    - [Step 7.1 | RF Base Model Definition](#rf_base)\n    - [Step 7.2 | RF Hyperparameter Tuning](#rf_hp)\n    - [Step 7.3 | RF Model Evaluation](#rf_eval)\n* [Step 8 | KNN Model Building](#knn)\n    - [Step 8.1 | KNN Base Model Definition](#knn_base)\n    - [Step 8.2 | KNN Hyperparameter Tuning](#knn_hp)\n    - [Step 8.3 | KNN Model Evaluation](#knn_eval)\n* [Step 9 | SVM Model Building](#svm)\n    - [Step 9.1 | SVM Base Model Definition](#svm_base)\n    - [Step 9.2 | SVM Hyperparameter Tuning](#svm_hp)\n    - [Step 9.3 | SVM Model Evaluation](#svm_eval)\n* [Step 10 | Conclusion](#conclusion)","metadata":{}},{"cell_type":"markdown","source":"<h2 align=\"left\"><font color=red>Let's get started:</font></h2>","metadata":{}},{"cell_type":"markdown","source":"<a id=\"import\"></a>\n# <p style=\"background-color:red; font-family:calibri; color:white; font-size:150%; text-align:center; border-radius:15px 50px;\">Step 1 | Import Libraries</p>\n\n⬆️ [Tabel of Contents](#contents_tabel)","metadata":{}},{"cell_type":"code","source":"import warnings\nwarnings.filterwarnings('ignore')\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom matplotlib.colors import ListedColormap\nfrom sklearn.model_selection import train_test_split\nfrom scipy.stats import boxcox\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.model_selection import GridSearchCV, StratifiedKFold\nfrom sklearn.metrics import classification_report, accuracy_score\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\n\n%matplotlib inline","metadata":{"execution":{"iopub.status.busy":"2023-08-11T13:07:40.079225Z","iopub.execute_input":"2023-08-11T13:07:40.07971Z","iopub.status.idle":"2023-08-11T13:07:40.954605Z","shell.execute_reply.started":"2023-08-11T13:07:40.079667Z","shell.execute_reply":"2023-08-11T13:07:40.953454Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Set the resolution of the plotted figures\nplt.rcParams['figure.dpi'] = 200\n\n# Configure Seaborn plot styles: Set background color and use dark grid\nsns.set(rc={'axes.facecolor': '#faded9'}, style='darkgrid')","metadata":{"execution":{"iopub.status.busy":"2023-08-11T13:07:40.956084Z","iopub.execute_input":"2023-08-11T13:07:40.956464Z","iopub.status.idle":"2023-08-11T13:07:40.964324Z","shell.execute_reply.started":"2023-08-11T13:07:40.956429Z","shell.execute_reply":"2023-08-11T13:07:40.962876Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<a id=\"read\"></a>\n# <p style=\"background-color:red ; font-family:calibri; color:white; font-size:150%; text-align:center; border-radius:15px 50px;\">Step 2 | Read Dataset</p>\n\n⬆️ [Tabel of Contents](#contents_tabel)","metadata":{}},{"cell_type":"markdown","source":"<div style=\"border-radius:10px; padding: 15px; background-color: #facfc8; font-size:115%; text-align:left\">\n\nFirst of all, let's load the dataset:","metadata":{}},{"cell_type":"code","source":"# Read dataset\ndf = pd.read_csv('/kaggle/input/heartcsv/heart.csv')\ndf ","metadata":{"execution":{"iopub.status.busy":"2023-08-11T13:07:40.965953Z","iopub.execute_input":"2023-08-11T13:07:40.96635Z","iopub.status.idle":"2023-08-11T13:07:41.011495Z","shell.execute_reply.started":"2023-08-11T13:07:40.966318Z","shell.execute_reply":"2023-08-11T13:07:41.009979Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<div style=\"border-radius:10px; padding: 15px; background-color: #facfc8; font-size:120%; text-align:left\">\n\n<h2 align=\"left\"><font color=red>Dataset Description:</font></h2>\n    \n| __Variable__ | __Description__ |\n|     :---      |       :---      |      \n| __age__ | Age of the patient in years |\n| __sex__ | Gender of the patient (0 = male, 1 = female) |\n| __cp__ | Chest pain type: <br> 0: Typical angina <br> 1: Atypical angina <br> 2: Non-anginal pain <br> 3: Asymptomatic |\n| __trestbps__ | Resting blood pressure in mm Hg |\n| __chol__ | Serum cholesterol in mg/dl |                     \n| __fbs__ | Fasting blood sugar level, categorized as above 120 mg/dl (1 = true, 0 = false) |\n| __restecg__ | Resting electrocardiographic results: <br> 0: Normal <br> 1: Having ST-T wave abnormality <br> 2: Showing probable or definite left ventricular hypertrophy |  \n| __thalach__ | Maximum heart rate achieved during a stress test |                      \n| __exang__ | Exercise-induced angina (1 = yes, 0 = no) |\n| __oldpeak__ | ST depression induced by exercise relative to rest |\n| __slope__ | Slope of the peak exercise ST segment: <br> 0: Upsloping <br> 1: Flat <br> 2: Downsloping |                      \n| __ca__ | Number of major vessels (0-4) colored by fluoroscopy |              \n| __thal__ | Thalium stress test result: <br> 0: Normal <br> 1: Fixed defect <br> 2: Reversible defect <br> 3: Not described  |\n| __target__ | Heart disease status (0 = no disease, 1 = presence of disease) |","metadata":{}},{"cell_type":"markdown","source":"<a id=\"overview\"></a>\n# <p style=\"background-color:red; font-family:calibri; color:white; font-size:150%; text-align:center; border-radius:15px 50px;\">Step 3 | Dataset Overview</p>\n\n⬆️ [Tabel of Contents](#contents_tabel)","metadata":{}},{"cell_type":"markdown","source":"<div style=\"border-radius:10px; padding: 15px; background-color: #facfc8; font-size:115%; text-align:left\">\n    \nNext, I'll delve into a detailed exploration of the dataset:","metadata":{}},{"cell_type":"markdown","source":"<a id=\"basic\"></a>\n# <b><span style='color:#ff826e'>Step 3.1 |</span><span style='color:red'> Dataset Basic Information</span></b>","metadata":{}},{"cell_type":"code","source":"# Display a concise summary of the dataframe\ndf.info()","metadata":{"execution":{"iopub.status.busy":"2023-08-11T13:07:41.015361Z","iopub.execute_input":"2023-08-11T13:07:41.015829Z","iopub.status.idle":"2023-08-11T13:07:41.034243Z","shell.execute_reply.started":"2023-08-11T13:07:41.015792Z","shell.execute_reply":"2023-08-11T13:07:41.032954Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<div style=\"border-radius:10px; padding: 15px; background-color: #facfc8; font-size:115%; text-align:left\">\n\n<h2 align=\"left\"><font color=red>Inferences:</font></h2>\n\n* __Number of Entries__: The dataset consists of __303 entries__, ranging from index 0 to 302.\n    \n    \n* __Columns__: There are __14 columns__ in the dataset corresponding to various attributes of the patients and results of tests.\n    \n    \n* __Data Types__:\n    - Most of the columns (13 out of 14) are of the __int64__ data type.\n    - Only the oldpeak column is of the float64 data type.\n    \n    \n* __Missing Values__: There don't appear to be any missing values in the dataset as each column has 303 non-null entries.","metadata":{}},{"cell_type":"markdown","source":"<div style=\"border-radius:10px; padding: 15px; background-color: #facfc8; font-size:110%; text-align:left\">\n\n__<span style=\"font-size:130%; color:red\"> Note: </span>__ Based on the data types and the feature explanations we had earlier, we can see that __9 columns__ (`sex`, `cp`, `fbs`, `restecg`, `exang`, `slope`, `ca`, `thal`, and `target`) are indeed __numerical__ in terms of data type, but __categorical__ in terms of their semantics. These features should be converted to string (__object__) data type for proper analysis and interpretation:","metadata":{}},{"cell_type":"code","source":"# Define the continuous features\ncontinuous_features = ['age', 'trestbps', 'chol', 'thalach', 'oldpeak']\n\n# Identify the features to be converted to object data type\nfeatures_to_convert = [feature for feature in df.columns if feature not in continuous_features]\n\n# Convert the identified features to object data type\ndf[features_to_convert] = df[features_to_convert].astype('object')\n\ndf.dtypes","metadata":{"execution":{"iopub.status.busy":"2023-08-11T13:07:41.036068Z","iopub.execute_input":"2023-08-11T13:07:41.037378Z","iopub.status.idle":"2023-08-11T13:07:41.053922Z","shell.execute_reply.started":"2023-08-11T13:07:41.037331Z","shell.execute_reply":"2023-08-11T13:07:41.052666Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<a id=\"num_statistics\"></a>\n# <b><span style='color:#ff826e'>Step 3.2 |</span><span style='color:red'> Summary Statistics for Numerical Variables</span></b>","metadata":{}},{"cell_type":"markdown","source":"<div style=\"border-radius:10px; padding: 15px; background-color: #facfc8; font-size:115%; text-align:left\">\n\nLet's delve into the summary statistics for our __numerical__ attributes:","metadata":{}},{"cell_type":"code","source":"# Get the summary statistics for numerical variables\ndf.describe().T","metadata":{"execution":{"iopub.status.busy":"2023-08-11T13:07:41.05528Z","iopub.execute_input":"2023-08-11T13:07:41.056324Z","iopub.status.idle":"2023-08-11T13:07:41.092976Z","shell.execute_reply.started":"2023-08-11T13:07:41.056289Z","shell.execute_reply":"2023-08-11T13:07:41.09159Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<div style=\"border-radius:10px; padding: 15px; background-color: #facfc8; font-size:115%; text-align:left\">\n\n<h3 align=\"left\"><font color=red>Numerical Features:</font></h3>\n\n* __`age`__: The average age of the patients is approximately 54.4 years, with the youngest being 29 and the oldest 77 years.\n* __`trestbps`__: The average resting blood pressure is about 131.62 mm Hg, ranging from 94 to 200 mm Hg.\n* __`chol`__: The average cholesterol level is approximately 246.26 mg/dl, with a minimum of 126 and a maximum of 564 mg/dl.\n* __`thalach`__: The average maximum heart rate achieved is around 149.65, with a range from 71 to 202.\n* __`oldpeak`__: The average ST depression induced by exercise relative to rest is about 1.04, with values ranging from 0 to 6.2.","metadata":{}},{"cell_type":"markdown","source":"<a id=\"cat_statistics\"></a>\n# <b><span style='color:#ff826e'>Step 3.3 |</span><span style='color:red'> Summary Statistics for Categorical  Variables</span></b>","metadata":{}},{"cell_type":"markdown","source":"<div style=\"border-radius:10px; padding: 15px; background-color: #facfc8; font-size:115%; text-align:left\">\n\nAfterward, let's look at the summary statistics of the categorical features:","metadata":{}},{"cell_type":"code","source":"# Get the summary statistics for categorical variables\ndf.describe(include='object')","metadata":{"execution":{"iopub.status.busy":"2023-08-11T13:07:41.094821Z","iopub.execute_input":"2023-08-11T13:07:41.095342Z","iopub.status.idle":"2023-08-11T13:07:41.127401Z","shell.execute_reply.started":"2023-08-11T13:07:41.095308Z","shell.execute_reply":"2023-08-11T13:07:41.125881Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<div style=\"border-radius:10px; padding: 15px; background-color: #facfc8; font-size:115%; text-align:left\">\n\n<h3 align=\"left\"><font color=red>Categorical Features (object data type):</font></h3>\n\n* __`sex`__: There are two unique values, with males (denoted as 0) being the most frequent category, occurring 207 times out of 303 entries.\n* __`cp`__: Four unique types of chest pain are present. The most common type is \"__0__\", occurring 143 times.\n* __`fbs`__: There are two categories, and the most frequent one is \"__0__\" (indicating fasting blood sugar less than 120 mg/dl), which appears 258 times.\n* __`restecg`__: Three unique results are present. The most common result is \"__1__\", appearing 152 times.\n* __`exang`__: There are two unique values. The most frequent value is \"__0__\" (indicating no exercise-induced angina), which is observed 204 times.\n* __`slope`__: Three unique slopes are present. The most frequent slope type is \"__2__\", which occurs 142 times.\n* __`ca`__: There are five unique values for the number of major vessels colored by fluoroscopy, with \"__0__\" being the most frequent, occurring 175 times.\n* __`thal`__: Four unique results are available. The most common type is \"__2__\" (indicating a reversible defect), observed 166 times.\n* __`target`__: Two unique values indicate the presence or absence of heart disease. The value \"__1__\" (indicating the presence of heart disease) is the most frequent, observed in 165 entries.","metadata":{}},{"cell_type":"markdown","source":"<a id=\"eda\"></a>\n# <p style=\"background-color:red; font-family:calibri; color:white; font-size:150%; text-align:center; border-radius:15px 50px;\">Step 4 | EDA</p>\n\n⬆️ [Tabel of Contents](#contents_tabel)","metadata":{}},{"cell_type":"markdown","source":"<div style=\"border-radius:10px; padding: 15px; background-color: #facfc8; font-size:115%; text-align:left\">\n\nFor our __Exploratory Data Analysis (EDA)__, we'll take it in two main steps:\n\n>__1. Univariate Analysis__: Here, we'll focus on one feature at a time to understand its distribution and range.\n>\n>__2. Bivariate Analysis__: In this step, we'll explore the relationship between each feature and the target variable. This helps us figure out the importance and influence of each feature on the target outcome.\n\nWith these two steps, we aim to gain insights into the individual characteristics of the data and also how each feature relates to our main goal: __predicting the target variable__.","metadata":{}},{"cell_type":"markdown","source":"<a id=\"univariate\"></a>\n# <b><span style='color:#ff826e'>Step 4.1 |</span><span style='color:red'> Univariate Analysis</span></b>","metadata":{}},{"cell_type":"markdown","source":"<div style=\"border-radius:10px; padding: 15px; background-color: #facfc8; font-size:115%; text-align:left\">\n\nWe undertake univariate analysis on the dataset's features, based on their datatype:\n\n* For __continuous data__: We employ histograms to gain insight into the distribution of each feature. This allows us to understand the central tendency, spread, and shape of the dataset's distribution.\n\n    \n* For __categorical data__: Bar plots are utilized to visualize the frequency of each category. This provides a clear representation of the prominence of each category within the respective feature.\n\nBy employing these visualization techniques, we're better positioned to understand the individual characteristics of each feature in the dataset.","metadata":{}},{"cell_type":"markdown","source":"<a id=\"num_uni\"></a>\n### <b><span style='color:#ff826e'>Step 4.1.1 |</span><span style='color:red'> Numerical Variables Univariate Analysis</span></b>  ","metadata":{}},{"cell_type":"code","source":"# Filter out continuous features for the univariate analysis\ndf_continuous = df[continuous_features]\n\n# Set up the subplot\nfig, ax = plt.subplots(nrows=2, ncols=3, figsize=(15, 10))\n\n# Loop to plot histograms for each continuous feature\nfor i, col in enumerate(df_continuous.columns):\n    x = i // 3\n    y = i % 3\n    values, bin_edges = np.histogram(df_continuous[col], \n                                     range=(np.floor(df_continuous[col].min()), np.ceil(df_continuous[col].max())))\n    \n    graph = sns.histplot(data=df_continuous, x=col, bins=bin_edges, kde=True, ax=ax[x, y],\n                         edgecolor='none', color='red', alpha=0.6, line_kws={'lw': 3})\n    ax[x, y].set_xlabel(col, fontsize=15)\n    ax[x, y].set_ylabel('Count', fontsize=12)\n    ax[x, y].set_xticks(np.round(bin_edges, 1))\n    ax[x, y].set_xticklabels(ax[x, y].get_xticks(), rotation=45)\n    ax[x, y].grid(color='lightgrey')\n    \n    for j, p in enumerate(graph.patches):\n        ax[x, y].annotate('{}'.format(p.get_height()), (p.get_x() + p.get_width() / 2, p.get_height() + 1),\n                          ha='center', fontsize=10, fontweight=\"bold\")\n    \n    textstr = '\\n'.join((\n        r'$\\mu=%.2f$' % df_continuous[col].mean(),\n        r'$\\sigma=%.2f$' % df_continuous[col].std()\n    ))\n    ax[x, y].text(0.75, 0.9, textstr, transform=ax[x, y].transAxes, fontsize=12, verticalalignment='top',\n                  color='white', bbox=dict(boxstyle='round', facecolor='#ff826e', edgecolor='white', pad=0.5))\n\nax[1,2].axis('off')\nplt.suptitle('Distribution of Continuous Variables', fontsize=20)\nplt.tight_layout()\nplt.subplots_adjust(top=0.92)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-08-11T13:07:41.129389Z","iopub.execute_input":"2023-08-11T13:07:41.130073Z","iopub.status.idle":"2023-08-11T13:07:44.504508Z","shell.execute_reply.started":"2023-08-11T13:07:41.130038Z","shell.execute_reply":"2023-08-11T13:07:44.5034Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<div style=\"border-radius:10px; padding: 15px; background-color: #facfc8; font-size:115%; text-align:left\">\n\n<h2 align=\"left\"><font color=red>Inferences:</font></h2>\n\n* __Age (`age`)__: The distribution is somewhat uniform, but there's a peak around the late 50s. The mean age is approximately 54.37 years with a standard deviation of 9.08 years.\n    \n    \n* __Resting Blood Pressure (`trestbps`)__: The resting blood pressure for most individuals is concentrated around 120-140 mm Hg, with a mean of approximately 131.62 mm Hg and a standard deviation of 17.54 mm Hg.\n    \n    \n* __Serum Cholesterol (`chol`)__: Most individuals have cholesterol levels between 200 and 300 mg/dl. The mean cholesterol level is around 246.26 mg/dl with a standard deviation of 51.83 mg/dl.\n    \n    \n* __Maximum Heart Rate Achieved (`thalach`)__: The majority of the individuals achieve a heart rate between 140 and 170 bpm during a stress test. The mean heart rate achieved is approximately 149.65 bpm with a standard deviation of 22.91 bpm.\n    \n    \n* __ST Depression Induced by Exercise (`oldpeak`)__: Most of the values are concentrated towards 0, indicating that many individuals did not experience significant ST depression during exercise. The mean ST depression value is 1.04 with a standard deviation of 1.16.\n    \n____\n\nUpon reviewing the histograms of the continuous features and cross-referencing them with the provided feature descriptions, everything appears consistent and within expected ranges. __There doesn't seem to be any noticeable noise or implausible values among the continuous variables.__","metadata":{}},{"cell_type":"markdown","source":"<a id=\"cat_uni\"></a>\n### <b><span style='color:#ff826e'>Step 4.1.2 |</span><span style='color:red'> Categorical Variables Univariate Analysis</span></b>  ","metadata":{}},{"cell_type":"code","source":"# Filter out categorical features for the univariate analysis\ncategorical_features = df.columns.difference(continuous_features)\ndf_categorical = df[categorical_features]","metadata":{"execution":{"iopub.status.busy":"2023-08-11T13:07:44.505651Z","iopub.execute_input":"2023-08-11T13:07:44.506042Z","iopub.status.idle":"2023-08-11T13:07:44.514206Z","shell.execute_reply.started":"2023-08-11T13:07:44.506004Z","shell.execute_reply":"2023-08-11T13:07:44.512699Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Set up the subplot for a 4x2 layout\nfig, ax = plt.subplots(nrows=5, ncols=2, figsize=(15, 18))\n\n# Loop to plot bar charts for each categorical feature in the 4x2 layout\nfor i, col in enumerate(categorical_features):\n    row = i // 2\n    col_idx = i % 2\n    \n    # Calculate frequency percentages\n    value_counts = df[col].value_counts(normalize=True).mul(100).sort_values()\n    \n    # Plot bar chart\n    value_counts.plot(kind='barh', ax=ax[row, col_idx], width=0.8, color='red')\n    \n    # Add frequency percentages to the bars\n    for index, value in enumerate(value_counts):\n        ax[row, col_idx].text(value, index, str(round(value, 1)) + '%', fontsize=15, weight='bold', va='center')\n    \n    ax[row, col_idx].set_xlim([0, 95])\n    ax[row, col_idx].set_xlabel('Frequency Percentage', fontsize=12)\n    ax[row, col_idx].set_title(f'{col}', fontsize=20)\n\nax[4,1].axis('off')\nplt.suptitle('Distribution of Categorical Variables', fontsize=22)\nplt.tight_layout()\nplt.subplots_adjust(top=0.95)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-08-11T13:07:44.515626Z","iopub.execute_input":"2023-08-11T13:07:44.516008Z","iopub.status.idle":"2023-08-11T13:07:48.33577Z","shell.execute_reply.started":"2023-08-11T13:07:44.515978Z","shell.execute_reply":"2023-08-11T13:07:48.334637Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<div style=\"border-radius:10px; padding: 15px; background-color: #facfc8; font-size:112%; text-align:left\">\n\n<h2 align=\"left\"><font color=red>Inferences:</font></h2>\n    \n* __Gender (`sex`)__: The dataset is predominantly female, constituting a significant majority.\n\n    \n* __Type of Chest Pain (`cp`)__: The dataset shows varied chest pain types among patients. Type 0 (Typical angina) seems to be the most prevalent, but an exact distribution among the types can be inferred from the bar plots.\n\n    \n* __Fasting Blood Sugar (`fbs`)__: A significant majority of the patients have their fasting blood sugar level below 120 mg/dl, indicating that high blood sugar is not a common condition in this dataset.\n\n    \n* __Resting Electrocardiographic Results (`restecg`)__: The results show varied resting electrocardiographic outcomes, with certain types being more common than others. The exact distribution can be gauged from the plots.\n\n    \n* __Exercise-Induced Angina (`exang`)__: A majority of the patients do not experience exercise-induced angina, suggesting that it might not be a common symptom among the patients in this dataset.\n\n    \n* __Slope of the Peak Exercise ST Segment (`slope`)__: The dataset shows different slopes of the peak exercise ST segment. A specific type might be more common, and its distribution can be inferred from the bar plots.\n\n    \n* __Number of Major Vessels Colored by Fluoroscopy (`ca`)__: Most patients have fewer major vessels colored by fluoroscopy, with '0' being the most frequent.\n\n    \n* __Thalium Stress Test Result (`thal`)__: The dataset displays a variety of thalium stress test results. One particular type seems to be more prevalent, but the exact distribution can be seen in the plots.\n\n    \n* __Presence of Heart Disease (`target`)__: The __dataset is nearly balanced__ in terms of heart disease presence, with about 54.5% having it and 45.5% not having it.","metadata":{}},{"cell_type":"markdown","source":"____\n<a id=\"bivariate\"></a>\n# <b><span style='color:#ff826e'>Step 4.2 |</span><span style='color:red'> Bivariate Analysis</span></b>","metadata":{}},{"cell_type":"markdown","source":"<div style=\"border-radius:10px; padding: 15px; background-color: #facfc8; font-size:115%; text-align:left\">\n\nFor our __bivariate analysis__ on the dataset's features with respect to the target variable:\n\n* For __continuous data__: I am going to use __bar plots__ to showcase the average value of each feature for the different target classes, and __KDE plots__ to understand the distribution of each feature across the target classes. This aids in discerning how each feature varies between the two target outcomes.\n\n    \n* For __categorical data__: I am going to employ __100% stacked bar plots__ to depict the proportion of each category across the target classes. This offers a comprehensive view of how different categories within a feature relate to the target.\n\nThrough these visualization techniques, we are going to gain a deeper understanding of the relationship between individual features and the target, revealing potential predictors for heart disease.","metadata":{}},{"cell_type":"markdown","source":"<a id=\"num_target\"></a>\n### <b><span style='color:#ff826e'>Step 4.2.1 |</span><span style='color:red'> Numerical Features vs Target</span></b>  ","metadata":{}},{"cell_type":"markdown","source":"<div style=\"border-radius:10px; padding: 15px; background-color: #facfc8; font-size:115%; text-align:left\">\n    \nI am going to visualize each continuous feature against the target using two types of charts: \n\n* __Bar plots__ - showing the mean values.\n* __KDE plots__ - displaying the distribution for each target category.","metadata":{}},{"cell_type":"code","source":"# Set color palette\nsns.set_palette(['#ff826e', 'red'])\n\n# Create the subplots\nfig, ax = plt.subplots(len(continuous_features), 2, figsize=(15,15), gridspec_kw={'width_ratios': [1, 2]})\n\n# Loop through each continuous feature to create barplots and kde plots\nfor i, col in enumerate(continuous_features):\n    # Barplot showing the mean value of the feature for each target category\n    graph = sns.barplot(data=df, x=\"target\", y=col, ax=ax[i,0])\n    \n    # KDE plot showing the distribution of the feature for each target category\n    sns.kdeplot(data=df[df[\"target\"]==0], x=col, fill=True, linewidth=2, ax=ax[i,1], label='0')\n    sns.kdeplot(data=df[df[\"target\"]==1], x=col, fill=True, linewidth=2, ax=ax[i,1], label='1')\n    ax[i,1].set_yticks([])\n    ax[i,1].legend(title='Heart Disease', loc='upper right')\n    \n    # Add mean values to the barplot\n    for cont in graph.containers:\n        graph.bar_label(cont, fmt='         %.3g')\n        \n# Set the title for the entire figure\nplt.suptitle('Continuous Features vs Target Distribution', fontsize=22)\nplt.tight_layout()                     \nplt.show()\n","metadata":{"execution":{"iopub.status.busy":"2023-08-11T13:07:48.337681Z","iopub.execute_input":"2023-08-11T13:07:48.33839Z","iopub.status.idle":"2023-08-11T13:07:52.327414Z","shell.execute_reply.started":"2023-08-11T13:07:48.338347Z","shell.execute_reply":"2023-08-11T13:07:52.326258Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<div style=\"border-radius:10px; padding: 15px; background-color: #facfc8; font-size:115%; text-align:left\">\n\n<h2 align=\"left\"><font color=red>Inferences:</font></h2>\n    \n* __Age (`age`)__: The distributions show a slight shift with patients having heart disease being a bit younger on average than those without. The mean age for patients without heart disease is higher.\n\n    \n* __Resting Blood Pressure (`trestbps`)__: Both categories display overlapping distributions in the KDE plot, with nearly identical mean values, indicating limited differentiating power for this feature.\n\n    \n* __Serum Cholesterol (`chol`)__: The distributions of cholesterol levels for both categories are quite close, but the mean cholesterol level for patients with heart disease is slightly lower.\n \n    \n* __Maximum Heart Rate Achieved (`thalach`)__: There's a noticeable difference in distributions. Patients with heart disease tend to achieve a higher maximum heart rate during stress tests compared to those without.\n\n    \n* __ST Depression (`oldpeak`)__: The ST depression induced by exercise relative to rest is notably lower for patients with heart disease. Their distribution peaks near zero, whereas the non-disease category has a wider spread.\n    \n____\n    \nBased on the visual difference in distributions and mean values, __Maximum Heart Rate (`thalach`)__ seems to have the most impact on the heart disease status, followed by __ST Depression (`oldpeak`)__ and __Age (`age`)__.","metadata":{}},{"cell_type":"markdown","source":"____\n<a id=\"cat_target\"></a>\n### <b><span style='color:#ff826e'>Step 4.2.2 |</span><span style='color:red'> Categorical Features vs Target</span></b>  ","metadata":{}},{"cell_type":"markdown","source":"<div style=\"border-radius:10px; padding: 15px; background-color: #facfc8; font-size:115%; text-align:left\">\n\nI am going to display __100% stacked bar plots__ for each categorical feature illustrating the proportion of each category across the two target classes, complemented by the exact counts and percentages on the bars.","metadata":{}},{"cell_type":"code","source":"# Remove 'target' from the categorical_features\ncategorical_features = [feature for feature in categorical_features if feature != 'target']","metadata":{"execution":{"iopub.status.busy":"2023-08-11T13:07:52.329305Z","iopub.execute_input":"2023-08-11T13:07:52.330116Z","iopub.status.idle":"2023-08-11T13:07:52.336518Z","shell.execute_reply.started":"2023-08-11T13:07:52.33007Z","shell.execute_reply":"2023-08-11T13:07:52.335576Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"fig, ax = plt.subplots(nrows=2, ncols=4, figsize=(15,10))\n\nfor i,col in enumerate(categorical_features):\n    \n    # Create a cross tabulation showing the proportion of purchased and non-purchased loans for each category of the feature\n    cross_tab = pd.crosstab(index=df[col], columns=df['target'])\n    \n    # Using the normalize=True argument gives us the index-wise proportion of the data\n    cross_tab_prop = pd.crosstab(index=df[col], columns=df['target'], normalize='index')\n\n    # Define colormap\n    cmp = ListedColormap(['#ff826e', 'red'])\n    \n    # Plot stacked bar charts\n    x, y = i//4, i%4\n    cross_tab_prop.plot(kind='bar', ax=ax[x,y], stacked=True, width=0.8, colormap=cmp,\n                        legend=False, ylabel='Proportion', sharey=True)\n    \n    # Add the proportions and counts of the individual bars to our plot\n    for idx, val in enumerate([*cross_tab.index.values]):\n        for (proportion, count, y_location) in zip(cross_tab_prop.loc[val],cross_tab.loc[val],cross_tab_prop.loc[val].cumsum()):\n            ax[x,y].text(x=idx-0.3, y=(y_location-proportion)+(proportion/2)-0.03,\n                         s = f'    {count}\\n({np.round(proportion * 100, 1)}%)', \n                         color = \"black\", fontsize=9, fontweight=\"bold\")\n    \n    # Add legend\n    ax[x,y].legend(title='target', loc=(0.7,0.9), fontsize=8, ncol=2)\n    # Set y limit\n    ax[x,y].set_ylim([0,1.12])\n    # Rotate xticks\n    ax[x,y].set_xticklabels(ax[x,y].get_xticklabels(), rotation=0)\n    \n            \nplt.suptitle('Categorical Features vs Target Stacked Barplots', fontsize=22)\nplt.tight_layout()                     \nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-08-11T13:07:52.342164Z","iopub.execute_input":"2023-08-11T13:07:52.342719Z","iopub.status.idle":"2023-08-11T13:07:55.202194Z","shell.execute_reply.started":"2023-08-11T13:07:52.342688Z","shell.execute_reply":"2023-08-11T13:07:55.201254Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<div style=\"border-radius:10px; padding: 15px; background-color: #facfc8; font-size:115%; text-align:left\">\n\n<h2 align=\"left\"><font color=red>Inferences:</font></h2>\n\n* __Number of Major Vessels (`ca`)__: The majority of patients with heart disease have fewer major vessels colored by fluoroscopy. As the number of colored vessels increases, the proportion of patients with heart disease tends to decrease. Especially, patients with 0 vessels colored have a higher proportion of heart disease presence.\n\n    \n* __Chest Pain Type (`cp`)__: Different types of chest pain present varied proportions of heart disease. Notably, types 1, 2, and 3 have a higher proportion of heart disease presence compared to type 0. This suggests the type of chest pain can be influential in predicting the disease.\n\n    \n* __Exercise Induced Angina (`exang`)__: Patients who did not experience exercise-induced angina (0) show a higher proportion of heart disease presence compared to those who did (1). This feature seems to have a significant impact on the target.\n\n    \n* __Fasting Blood Sugar (`fbs`)__: The distribution between those with fasting blood sugar > 120 mg/dl (1) and those without (0) is relatively similar, suggesting `fbs` might have limited impact on heart disease prediction.\n\n    \n* __Resting Electrocardiographic Results (`restecg`)__: Type 1 displays a higher proportion of heart disease presence, indicating that this feature might have some influence on the outcome.\n\n    \n* __Sex (`sex`)__: Females (1) exhibit a lower proportion of heart disease presence compared to males (0). This indicates gender as an influential factor in predicting heart disease.\n\n    \n* __Slope of the Peak Exercise ST Segment (`slope`)__: The slope type 2 has a notably higher proportion of heart disease presence, indicating its potential as a significant predictor.\n\n    \n* __Thalium Stress Test Result (`thal`)__: The reversible defect category (2) has a higher proportion of heart disease presence compared to the other categories, emphasizing its importance in prediction.\n\n____\n\nIn summary, based on the visual representation:\n\n* __Higher Impact on Target: `ca`, `cp`, `exang`, `sex`, `slope`, and `thal`__\n* __Moderate Impact on Target: `restecg`__\n* __Lower Impact on Target: `fbs`__","metadata":{}},{"cell_type":"markdown","source":"<a id=\"preprocessing\"></a>\n# <p style=\"background-color:red; font-family:calibri; color:white; font-size:150%; text-align:center; border-radius:15px 50px;\">Step 5 | Data Preprocessing</p>\n\n⬆️ [Tabel of Contents](#contents_tabel)","metadata":{}},{"cell_type":"markdown","source":"<a id=\"feature_removal\"></a>\n# <b><span style='color:#ff826e'>Step 5.1 |</span><span style='color:red'> Irrelevant Features Removal</span></b>","metadata":{}},{"cell_type":"markdown","source":"<div style=\"border-radius:10px; padding: 15px; background-color: #facfc8; font-size:115%; text-align:left\">\n\nAll features in the dataset appear to be relevant based on our __EDA__. No columns seem redundant or irrelevant. Thus, we'll retain all features, ensuring no valuable information is lost, especially given the dataset's small size.","metadata":{}},{"cell_type":"markdown","source":"____\n<a id=\"missing\"></a>\n# <b><span style='color:#ff826e'>Step 5.2 |</span><span style='color:red'> Missing Value Treatment</span></b>","metadata":{}},{"cell_type":"code","source":"# Check for missing values in the dataset\ndf.isnull().sum().sum()","metadata":{"execution":{"iopub.status.busy":"2023-08-11T13:07:55.203416Z","iopub.execute_input":"2023-08-11T13:07:55.204144Z","iopub.status.idle":"2023-08-11T13:07:55.212727Z","shell.execute_reply.started":"2023-08-11T13:07:55.204112Z","shell.execute_reply":"2023-08-11T13:07:55.211827Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<div style=\"border-radius:10px; padding: 15px; background-color: #facfc8; font-size:115%; text-align:left\">\n    \nUpon our above inspection, it is obvious that there are no missing values in our dataset. This is ideal as it means we don't have to make decisions about imputation or removal, which can introduce bias or reduce our already limited dataset size.","metadata":{}},{"cell_type":"markdown","source":"____\n<a id=\"outlier\"></a>\n# <b><span style='color:#ff826e'>Step 5.3 |</span><span style='color:red'> Outlier Treatment</span></b>","metadata":{}},{"cell_type":"markdown","source":"<div style=\"border-radius:10px; padding: 15px; background-color: #facfc8; font-size:115%; text-align:left\">\n\nI am going to check for outliers using the __IQR method__ for the continuous features:","metadata":{}},{"cell_type":"code","source":"continuous_features","metadata":{"execution":{"iopub.status.busy":"2023-08-11T13:07:55.214064Z","iopub.execute_input":"2023-08-11T13:07:55.214694Z","iopub.status.idle":"2023-08-11T13:07:55.232931Z","shell.execute_reply.started":"2023-08-11T13:07:55.214665Z","shell.execute_reply":"2023-08-11T13:07:55.231504Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"Q1 = df[continuous_features].quantile(0.25)\nQ3 = df[continuous_features].quantile(0.75)\nIQR = Q3 - Q1\noutliers_count_specified = ((df[continuous_features] < (Q1 - 1.5 * IQR)) | (df[continuous_features] > (Q3 + 1.5 * IQR))).sum()\n\noutliers_count_specified","metadata":{"execution":{"iopub.status.busy":"2023-08-11T13:07:55.235828Z","iopub.execute_input":"2023-08-11T13:07:55.236277Z","iopub.status.idle":"2023-08-11T13:07:55.259059Z","shell.execute_reply.started":"2023-08-11T13:07:55.236234Z","shell.execute_reply":"2023-08-11T13:07:55.257859Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<div style=\"border-radius:10px; padding: 15px; background-color: #facfc8; font-size:115%; text-align:left\">\n    \nUpon identifying outliers for the specified continuous features, we found the following:\n\n* __`trestbps`__: 9 outliers\n* __`chol`__: 5 outliers\n* __`thalach`__: 1 outlier\n* __`oldpeak`__: 5 outliers\n* __`age`__: No outliers","metadata":{}},{"cell_type":"markdown","source":"<div style=\"border-radius:10px; padding: 15px; background-color: #facfc8; font-size:115%; text-align:left\">\n\n<h3 align=\"left\"><font color=red>Sensitivity to Outliers:</font></h3>\n\n* __SVM (Support Vector Machine)__: SVMs can be sensitive to outliers. While the decision boundary is determined primarily by the support vectors, outliers can influence which data points are chosen as support vectors, potentially leading to suboptimal classification.\n    \n    \n* __Decision Trees (DT) and Random Forests (RF)__: These tree-based algorithms are generally robust to outliers. They make splits based on feature values, and outliers often end up in leaf nodes, having minimal impact on the overall decision-making process.\n    \n    \n* __K-Nearest Neighbors (KNN)__: KNN is sensitive to outliers because it relies on distances between data points to make predictions. Outliers can distort these distances.\n    \n    \n* __AdaBoost:__ This ensemble method, which often uses decision trees as weak learners, is generally robust to outliers. However, the iterative nature of AdaBoost can sometimes lead to overemphasis on outliers, making the final model more sensitive to them.","metadata":{}},{"cell_type":"markdown","source":"<div style=\"border-radius:10px; padding: 15px; background-color: #facfc8; font-size:115%; text-align:left\">\n\n<h3 align=\"left\"><font color=red>Approaches for Outlier Treatment:</font></h3>\n    \n* __Removal of Outliers__: Directly discard data points that fall outside of a defined range, typically based on a method like the Interquartile Range (IQR).\n\n    \n* __Capping Outliers__: Instead of removing, we can limit outliers to a certain threshold, such as the 1st or 99th percentile.\n    \n    \n* __Transformations__: Applying transformations like log or Box-Cox can reduce the impact of outliers and make the data more Gaussian-like.\n    \n    \n* __Robust Scaling__: Techniques like the RobustScaler in Scikit-learn can be used, which scales features using statistics that are robust to outliers.","metadata":{}},{"cell_type":"markdown","source":"<div style=\"border-radius:10px; padding: 15px; background-color: #facfc8; font-size:120%; text-align:left\">\n\n<h3 align=\"left\"><font color=red>Conclusion:</font></h3>\n\nGiven __the nature of the algorithms (especially SVM and KNN)__ and __the small size of our dataset__, direct removal of outliers might not be the best approach. Instead, __we'll focus on applying transformations like Box-Cox in the subsequent steps__ to reduce the impact of outliers and make the data more suitable for modeling.","metadata":{}},{"cell_type":"markdown","source":"____\n<a id=\"encoding\"></a>\n# <b><span style='color:#ff826e'>Step 5.4 |</span><span style='color:red'> Categorical Features Encoding</span></b>","metadata":{}},{"cell_type":"markdown","source":"<div style=\"border-radius:10px; padding: 15px; background-color: #facfc8; font-size:112%; text-align:left\">\n\n<h3 align=\"left\"><font color=red>One-hot Encoding Decision:</font></h3>\n    \nBased on the feature descriptions, let's decide on one-hot encoding:\n\n1. __Nominal Variables__: These are variables with no inherent order. They should be one-hot encoded because using them as numbers might introduce an unintended order to the model.\n\n2. __Ordinal Variables__: These variables have an inherent order. They don't necessarily need to be one-hot encoded since their order can provide meaningful information to the model.\n\nGiven the above explanation:\n\n- __`sex`__: This is a binary variable with two categories (male and female), so it doesn't need one-hot encoding.\n \n    \n- __`cp`__: Chest pain type can be considered as nominal because there's no clear ordinal relationship among the different types of chest pain (like Typical angina, Atypical angina, etc.). It should be one-hot encoded.\n  \n    \n- __`fbs`__: This is a binary variable (true or false), so it doesn't need one-hot encoding.\n\n    \n- __`restecg`__: This variable represents the resting electrocardiographic results. The results, such as \"Normal\", \"Having ST-T wave abnormality\", and \"Showing probable or definite left ventricular hypertrophy\", don't seem to have an ordinal relationship. Therefore, it should be one-hot encoded.\n\n    \n- __`exang`__: This is a binary variable (yes or no), so it doesn't need one-hot encoding.\n\n    \n- __`slope`__: This represents the slope of the peak exercise ST segment. Given the descriptions (Upsloping, Flat, Downsloping), it seems to have an ordinal nature, suggesting a particular order. Therefore, it doesn't need to be one-hot encoded.\n\n    \n- __`ca`__: This represents the number of major vessels colored by fluoroscopy. As it indicates a count, it has an inherent ordinal relationship. Therefore, it doesn't need to be one-hot encoded.\n\n    \n- __`thal`__: This variable represents the result of a thalium stress test. The different states, like \"Normal\", \"Fixed defect\", and \"Reversible defect\", suggest a nominal nature. Thus, it should be one-hot encoded.\n\n<h4 align=\"left\">Summary:</h4>\n\n- __Need One-Hot Encoding__: __`cp`__, __`restecg`__, __`thal`__\n- __Don't Need One-Hot Encoding__: __`sex`__, __`fbs`__, __`exang`__, __`slope`__, __`ca`__","metadata":{}},{"cell_type":"code","source":"# Implementing one-hot encoding on the specified categorical features\ndf_encoded = pd.get_dummies(df, columns=['cp', 'restecg', 'thal'], drop_first=True)\n\n# Convert the rest of the categorical variables that don't need one-hot encoding to integer data type\nfeatures_to_convert = ['sex', 'fbs', 'exang', 'slope', 'ca', 'target']\nfor feature in features_to_convert:\n    df_encoded[feature] = df_encoded[feature].astype(int)\n\ndf_encoded.dtypes","metadata":{"execution":{"iopub.status.busy":"2023-08-11T13:07:55.261135Z","iopub.execute_input":"2023-08-11T13:07:55.261676Z","iopub.status.idle":"2023-08-11T13:07:55.283185Z","shell.execute_reply.started":"2023-08-11T13:07:55.261628Z","shell.execute_reply":"2023-08-11T13:07:55.281971Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Displaying the resulting DataFrame after one-hot encoding\ndf_encoded.head()","metadata":{"execution":{"iopub.status.busy":"2023-08-11T13:07:55.285094Z","iopub.execute_input":"2023-08-11T13:07:55.285796Z","iopub.status.idle":"2023-08-11T13:07:55.316497Z","shell.execute_reply.started":"2023-08-11T13:07:55.285756Z","shell.execute_reply":"2023-08-11T13:07:55.315328Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"____\n<a id=\"scaling\"></a>\n# <b><span style='color:#ff826e'>Step 5.5 |</span><span style='color:red'> Feature Scaling</span></b>","metadata":{}},{"cell_type":"markdown","source":"<div style=\"border-radius:10px; padding: 15px; background-color: #facfc8; font-size:115%; text-align:left\">\n\n__Feature Scaling__ is a crucial preprocessing step __for algorithms that are sensitive to the magnitude or scale of features__. Models like __SVM__, __KNN__, and many linear models rely on distances or gradients, making them susceptible to variations in feature scales. __Scaling ensures that all features contribute equally to the model's decision rather than being dominated by features with larger magnitudes.__\n\n____\n<h3 align=\"left\"><font color=red>Why We Skip It Now:</font></h3>\n\nWhile feature scaling is vital for some models, not all algorithms require scaled data. For instance, __Decision Tree-based models__ are scale-invariant. Given our intent to use a mix of models (some requiring scaling, others not), __we've chosen to handle scaling later using pipelines__. This approach lets us apply scaling specifically for models that benefit from it, ensuring flexibility and efficiency in our modeling process.\n","metadata":{}},{"cell_type":"markdown","source":"____\n<a id=\"transform\"></a>\n# <b><span style='color:#ff826e'>Step 5.6 |</span><span style='color:red'> Transforming Skewed Features</span></b>","metadata":{}},{"cell_type":"markdown","source":"<div style=\"border-radius:10px; padding: 15px; background-color: #facfc8; font-size:115%; text-align:left\">\n\n__Box-Cox__ transformation is a powerful method to stabilize variance and make the data more normal-distribution-like. It's particularly useful when you're unsure about the exact nature of the distribution you're dealing with, as it can adapt itself to the best power transformation. However, the Box-Cox transformation only works for positive data, so one must be cautious when applying it to features that contain zeros or negative values.","metadata":{}},{"cell_type":"markdown","source":"<div style=\"border-radius:10px; padding: 15px; background-color: #facfc8; font-size:115%; text-align:left\">\n    \n<h3 align=\"left\"><font color=red>Transforming Skewed Features & Data Leakage Concerns:</font></h3>\n    \nWhen preprocessing data, especially applying transformations like the Box-Cox, it's essential to be wary of __data leakage__. __Data leakage__ refers to a mistake in the preprocessing of data in which information from outside the training dataset is used to transform or train the model. This can lead to overly optimistic performance metrics.\n\n\n<h3 align=\"left\"><font color=red>To avoid data leakage and ensure our model generalizes well to unseen data:</font></h3>\n\n__1- Data Splitting:__ We'll first split our dataset into a training set and a test set. This ensures that we have a separate set of data to evaluate our model's performance, untouched during the training and preprocessing phases.\n\n__2- Box-Cox Transformation:__ We'll examine the distribution of the continuous features in the training set. If they appear skewed, we'll apply the Box-Cox transformation to stabilize variance and make the data more normal-distribution-like. Importantly, we'll determine the Box-Cox transformation parameters solely based on the training data.\n\n__3- Applying Transformations to Test Data:__ Once our transformation parameters are determined from the training set, we'll use these exact parameters to transform our validation/test set. This approach ensures that no information from the validation/test set leaks into our training process.\n\n__4. Hyperparameter Tuning & Cross-Validation:__ Given our dataset's size, to make the most of the available data during the model training phase, we'll employ __cross-validation on the training set for hyperparameter tuning__. This allows us to get a better sense of how our model might perform on unseen data, without actually using the test set. The test set remains untouched during this phase and is only used to evaluate the final model's performance.\n\n\nBy following this structured approach, we ensure a rigorous training process, minimize the risk of data leakage, and set ourselves up to get a realistic measure of our model's performance on unseen data.","metadata":{}},{"cell_type":"code","source":"# Define the features (X) and the output labels (y)\nX = df_encoded.drop('target', axis=1)\ny = df_encoded['target'] ","metadata":{"execution":{"iopub.status.busy":"2023-08-11T13:07:55.317845Z","iopub.execute_input":"2023-08-11T13:07:55.319116Z","iopub.status.idle":"2023-08-11T13:07:55.330825Z","shell.execute_reply.started":"2023-08-11T13:07:55.319078Z","shell.execute_reply":"2023-08-11T13:07:55.330001Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Splitting data into train and test sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0, stratify=y)","metadata":{"execution":{"iopub.status.busy":"2023-08-11T13:07:55.332102Z","iopub.execute_input":"2023-08-11T13:07:55.332482Z","iopub.status.idle":"2023-08-11T13:07:55.35153Z","shell.execute_reply.started":"2023-08-11T13:07:55.332447Z","shell.execute_reply":"2023-08-11T13:07:55.349994Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"continuous_features","metadata":{"execution":{"iopub.status.busy":"2023-08-11T13:07:55.354255Z","iopub.execute_input":"2023-08-11T13:07:55.35467Z","iopub.status.idle":"2023-08-11T13:07:55.372381Z","shell.execute_reply.started":"2023-08-11T13:07:55.354638Z","shell.execute_reply":"2023-08-11T13:07:55.370978Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<div style=\"border-radius:10px; padding: 15px; background-color: #facfc8; font-size:115%; text-align:left\">\n    \nThe Box-Cox transformation requires all data to be strictly positive. To transform the `oldpeak` feature using Box-Cox, we can add a small constant (e.g., 0.001) to ensure all values are positive:","metadata":{}},{"cell_type":"code","source":"# Adding a small constant to 'oldpeak' to make all values positive\nX_train['oldpeak'] = X_train['oldpeak'] + 0.001\nX_test['oldpeak'] = X_test['oldpeak'] + 0.001","metadata":{"execution":{"iopub.status.busy":"2023-08-11T13:07:55.374284Z","iopub.execute_input":"2023-08-11T13:07:55.374734Z","iopub.status.idle":"2023-08-11T13:07:55.386862Z","shell.execute_reply.started":"2023-08-11T13:07:55.374689Z","shell.execute_reply":"2023-08-11T13:07:55.385948Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Checking the distribution of the continuous features\nfig, ax = plt.subplots(2, 5, figsize=(15,10))\n\n# Original Distributions\nfor i, col in enumerate(continuous_features):\n    sns.histplot(X_train[col], kde=True, ax=ax[0,i], color='#ff826e').set_title(f'Original {col}')\n    \n\n# Applying Box-Cox Transformation\n# Dictionary to store lambda values for each feature\nlambdas = {}\n\nfor i, col in enumerate(continuous_features):\n    # Only apply box-cox for positive values\n    if X_train[col].min() > 0:\n        X_train[col], lambdas[col] = boxcox(X_train[col])\n        # Applying the same lambda to test data\n        X_test[col] = boxcox(X_test[col], lmbda=lambdas[col]) \n        sns.histplot(X_train[col], kde=True, ax=ax[1,i], color='red').set_title(f'Transformed {col}')\n    else:\n        sns.histplot(X_train[col], kde=True, ax=ax[1,i], color='green').set_title(f'{col} (Not Transformed)')\n\nfig.tight_layout()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-08-11T13:07:55.388143Z","iopub.execute_input":"2023-08-11T13:07:55.388909Z","iopub.status.idle":"2023-08-11T13:07:59.779493Z","shell.execute_reply.started":"2023-08-11T13:07:55.388858Z","shell.execute_reply":"2023-08-11T13:07:59.77832Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<div style=\"border-radius:10px; padding: 15px; background-color: #facfc8; font-size:115%; text-align:left\">\n    \n<h2 align=\"left\"><font color=red>Inference:</font></h2>\n    \n\n__1- `age`__: The transformation has made the `age` distribution more symmetric, bringing it closer to a normal distribution.\n\n__2- `Trestbps`__: The distribution of `trestbps` post-transformation appears to be more normal-like, with reduced skewness.\n\n__3- `Chol`__: After applying the Box-Cox transformation, `chol` exhibits a shape that's more aligned with a normal distribution.\n\n__4- `Thalach`__: The `thalach` feature was already fairly symmetric before the transformation, and post-transformation, it continues to show a similar shape, indicating its original distribution was close to normal.\n\n__5- `Oldpeak`__: The transformation improved the `oldpeak` distribution, but it still doesn't perfectly resemble a normal distribution. This could be due to the inherent nature of the data or the presence of outliers. To enhance its normality, we could consider utilizing advanced transformations such as the Yeo-Johnson transformation, which can handle zero and negative values directly.","metadata":{}},{"cell_type":"markdown","source":"<div style=\"border-radius:10px; padding: 15px; background-color: #facfc8; font-size:115%; text-align:left\">\n    \n<h2 align=\"left\"><font color=red>Conclusion:</font></h2>\n\nTransforming features to be more normal-like primarily helps in mitigating the impact of outliers, which is particularly beneficial for distance-based algorithms like __SVM__ and __KNN__. By reducing the influence of __outliers__, we ensure that these algorithms can compute distances more effectively and produce more reliable results.","metadata":{}},{"cell_type":"code","source":"X_train.head()","metadata":{"execution":{"iopub.status.busy":"2023-08-11T13:07:59.781069Z","iopub.execute_input":"2023-08-11T13:07:59.781451Z","iopub.status.idle":"2023-08-11T13:07:59.803301Z","shell.execute_reply.started":"2023-08-11T13:07:59.781418Z","shell.execute_reply":"2023-08-11T13:07:59.801914Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<a id=\"dt\"></a>\n# <p style=\"background-color:red; font-family:calibri; color:white; font-size:150%; text-align:center; border-radius:15px 50px;\">Step 6 | Decision Tree Model Building</p>\n\n⬆️ [Tabel of Contents](#contents_tabel)","metadata":{}},{"cell_type":"markdown","source":"____\n<a id=\"dt_base\"></a>\n# <b><span style='color:#ff826e'>Step 6.1 |</span><span style='color:red'> DT Base Model Definition</span></b>","metadata":{}},{"cell_type":"markdown","source":"<div style=\"border-radius:10px; padding: 15px; background-color: #facfc8; font-size:115%; text-align:left\">\n\nFirst, let's define the base DT model:","metadata":{}},{"cell_type":"code","source":"# Define the base DT model\ndt_base = DecisionTreeClassifier(random_state=0)","metadata":{"execution":{"iopub.status.busy":"2023-08-11T13:07:59.805208Z","iopub.execute_input":"2023-08-11T13:07:59.805696Z","iopub.status.idle":"2023-08-11T13:07:59.819711Z","shell.execute_reply.started":"2023-08-11T13:07:59.805649Z","shell.execute_reply":"2023-08-11T13:07:59.818408Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"____\n<a id=\"dt_hp\"></a>\n# <b><span style='color:#ff826e'>Step 6.2 |</span><span style='color:red'> DT  Hyperparameter Tuning</span></b>","metadata":{}},{"cell_type":"markdown","source":"<div style=\"border-radius:10px; padding: 15px; background-color: #facfc8; font-size:120%; text-align:left\">\n\n__🔍<span style=\"font-size:130%; color:red\"> Note: </span>__ In medical scenarios, especially in the context of diagnosing illnesses, it's often more important __to have a high recall (sensitivity) for the positive class (patients with the condition)__. A high recall ensures that most of the actual positive cases are correctly identified, even if it means some false positives (cases where healthy individuals are misclassified as having the condition). The rationale is that it's generally better to have a few false alarms than to miss out on diagnosing a patient with a potential illness.","metadata":{}},{"cell_type":"markdown","source":"<div style=\"border-radius:10px; padding: 15px; background-color: #facfc8; font-size:115%; text-align:left\">\n\nI am establishing a function to determine the optimal set of hyperparameters that yield the highest __recall__ for the model. This approach ensures a reusable framework for hyperparameter tuning of subsequent models:","metadata":{}},{"cell_type":"code","source":"def tune_clf_hyperparameters(clf, param_grid, X_train, y_train, scoring='recall', n_splits=3):\n    '''\n    This function optimizes the hyperparameters for a classifier by searching over a specified hyperparameter grid. \n    It uses GridSearchCV and cross-validation (StratifiedKFold) to evaluate different combinations of hyperparameters. \n    The combination with the highest recall for class 1 is selected as the default scoring metric. \n    The function returns the classifier with the optimal hyperparameters.\n    '''\n    \n    # Create the cross-validation object using StratifiedKFold to ensure the class distribution is the same across all the folds\n    cv = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=0)\n\n    # Create the GridSearchCV object\n    clf_grid = GridSearchCV(clf, param_grid, cv=cv, scoring=scoring, n_jobs=-1)\n\n    # Fit the GridSearchCV object to the training data\n    clf_grid.fit(X_train, y_train)\n\n    # Get the best hyperparameters\n    best_hyperparameters = clf_grid.best_params_\n    \n    # Return best_estimator_ attribute which gives us the best model that has been fitted to the training data\n    return clf_grid.best_estimator_, best_hyperparameters","metadata":{"execution":{"iopub.status.busy":"2023-08-11T13:07:59.821378Z","iopub.execute_input":"2023-08-11T13:07:59.822082Z","iopub.status.idle":"2023-08-11T13:07:59.838987Z","shell.execute_reply.started":"2023-08-11T13:07:59.822049Z","shell.execute_reply":"2023-08-11T13:07:59.83766Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<div style=\"border-radius:10px; padding: 15px; background-color: #facfc8; font-size:115%; text-align:left\">\n\nI'll set up the hyperparameters grid and utilize the __tune_clf_hyperparameters__ function to pinpoint the optimal hyperparameters for our DT model:","metadata":{}},{"cell_type":"code","source":"# Hyperparameter grid for DT\nparam_grid_dt = {\n    'criterion': ['gini', 'entropy'],\n    'max_depth': [2,3],\n    'min_samples_split': [2, 3, 4],\n    'min_samples_leaf': [1, 2]\n}","metadata":{"execution":{"iopub.status.busy":"2023-08-11T13:07:59.840703Z","iopub.execute_input":"2023-08-11T13:07:59.841642Z","iopub.status.idle":"2023-08-11T13:07:59.856213Z","shell.execute_reply.started":"2023-08-11T13:07:59.841609Z","shell.execute_reply":"2023-08-11T13:07:59.855186Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Call the function for hyperparameter tuning\nbest_dt, best_dt_hyperparams = tune_clf_hyperparameters(dt_base, param_grid_dt, X_train, y_train)","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2023-08-11T13:07:59.85729Z","iopub.execute_input":"2023-08-11T13:07:59.857647Z","iopub.status.idle":"2023-08-11T13:08:02.255506Z","shell.execute_reply.started":"2023-08-11T13:07:59.857618Z","shell.execute_reply":"2023-08-11T13:08:02.254438Z"},"_kg_hide-output":true,"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print('DT Optimal Hyperparameters: \\n', best_dt_hyperparams)","metadata":{"execution":{"iopub.status.busy":"2023-08-11T13:08:02.25724Z","iopub.execute_input":"2023-08-11T13:08:02.257793Z","iopub.status.idle":"2023-08-11T13:08:02.263416Z","shell.execute_reply.started":"2023-08-11T13:08:02.257759Z","shell.execute_reply":"2023-08-11T13:08:02.262582Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"____\n<a id=\"dt_eval\"></a>\n# <b><span style='color:#ff826e'>Step 6.3 |</span><span style='color:red'> DT Model Evaluation</span></b>","metadata":{}},{"cell_type":"markdown","source":"<div style=\"border-radius:10px; padding: 15px; background-color: #facfc8; font-size:115%; text-align:left\">\n\nNow let's evaluate our DT model performance on both the training and test datasets:","metadata":{}},{"cell_type":"code","source":"# Evaluate the optimized model on the train data\nprint(classification_report(y_train, best_dt.predict(X_train)))","metadata":{"execution":{"iopub.status.busy":"2023-08-11T13:08:02.264799Z","iopub.execute_input":"2023-08-11T13:08:02.265442Z","iopub.status.idle":"2023-08-11T13:08:02.290666Z","shell.execute_reply.started":"2023-08-11T13:08:02.265411Z","shell.execute_reply":"2023-08-11T13:08:02.289391Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Evaluate the optimized model on the test data\nprint(classification_report(y_test, best_dt.predict(X_test)))","metadata":{"execution":{"iopub.status.busy":"2023-08-11T13:08:02.292374Z","iopub.execute_input":"2023-08-11T13:08:02.292881Z","iopub.status.idle":"2023-08-11T13:08:02.3165Z","shell.execute_reply.started":"2023-08-11T13:08:02.292842Z","shell.execute_reply":"2023-08-11T13:08:02.314938Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<div style=\"border-radius:10px; padding: 15px; background-color: #facfc8; font-size:115%; text-align:left\">\n\nGiven that the metric values for both the training and test datasets are closely aligned and not significantly different, the model doesn't appear to be overfitting.","metadata":{}},{"cell_type":"markdown","source":"<div style=\"border-radius:10px; padding: 15px; background-color: #facfc8; font-size:115%; text-align:left\">\n\nLet's create a function that consolidates each model's metrics into a dataframe, facilitating an end-to-end comparison of all models later:","metadata":{}},{"cell_type":"code","source":"def evaluate_model(model, X_test, y_test, model_name):\n    \"\"\"\n    Evaluates the performance of a trained model on test data using various metrics.\n    \"\"\"\n    # Make predictions\n    y_pred = model.predict(X_test)\n    \n    # Get classification report\n    report = classification_report(y_test, y_pred, output_dict=True)\n    \n    # Extracting metrics\n    metrics = {\n        \"precision_0\": report[\"0\"][\"precision\"],\n        \"precision_1\": report[\"1\"][\"precision\"],\n        \"recall_0\": report[\"0\"][\"recall\"],\n        \"recall_1\": report[\"1\"][\"recall\"],\n        \"f1_0\": report[\"0\"][\"f1-score\"],\n        \"f1_1\": report[\"1\"][\"f1-score\"],\n        \"macro_avg_precision\": report[\"macro avg\"][\"precision\"],\n        \"macro_avg_recall\": report[\"macro avg\"][\"recall\"],\n        \"macro_avg_f1\": report[\"macro avg\"][\"f1-score\"],\n        \"accuracy\": accuracy_score(y_test, y_pred)\n    }\n    \n    # Convert dictionary to dataframe\n    df = pd.DataFrame(metrics, index=[model_name]).round(2)\n    \n    return df","metadata":{"execution":{"iopub.status.busy":"2023-08-11T13:08:02.318369Z","iopub.execute_input":"2023-08-11T13:08:02.319649Z","iopub.status.idle":"2023-08-11T13:08:02.331143Z","shell.execute_reply.started":"2023-08-11T13:08:02.319605Z","shell.execute_reply":"2023-08-11T13:08:02.330129Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"dt_evaluation = evaluate_model(best_dt, X_test, y_test, 'DT')\ndt_evaluation","metadata":{"execution":{"iopub.status.busy":"2023-08-11T13:08:02.332744Z","iopub.execute_input":"2023-08-11T13:08:02.333433Z","iopub.status.idle":"2023-08-11T13:08:02.386113Z","shell.execute_reply.started":"2023-08-11T13:08:02.333392Z","shell.execute_reply":"2023-08-11T13:08:02.385179Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<a id=\"rf\"></a>\n# <p style=\"background-color:red; font-family:calibri; color:white; font-size:150%; text-align:center; border-radius:15px 50px;\">Step 7 | Random Forest Model Building</p>\n\n⬆️ [Tabel of Contents](#contents_tabel)","metadata":{}},{"cell_type":"markdown","source":"____\n<a id=\"rf_base\"></a>\n# <b><span style='color:#ff826e'>Step 7.1 |</span><span style='color:red'> RF Base Model Definition</span></b>","metadata":{}},{"cell_type":"markdown","source":"<div style=\"border-radius:10px; padding: 15px; background-color: #facfc8; font-size:115%; text-align:left\">\n\nFirst, let's define the base RF model:","metadata":{}},{"cell_type":"code","source":"rf_base = RandomForestClassifier(random_state=0)","metadata":{"execution":{"iopub.status.busy":"2023-08-11T13:08:02.389991Z","iopub.execute_input":"2023-08-11T13:08:02.390375Z","iopub.status.idle":"2023-08-11T13:08:02.396763Z","shell.execute_reply.started":"2023-08-11T13:08:02.390344Z","shell.execute_reply":"2023-08-11T13:08:02.395036Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"____\n<a id=\"rf_hp\"></a>\n# <b><span style='color:#ff826e'>Step 7.2 |</span><span style='color:red'> RF Hyperparameter Tuning</span></b>","metadata":{}},{"cell_type":"markdown","source":"<div style=\"border-radius:10px; padding: 15px; background-color: #facfc8; font-size:115%; text-align:left\">\n    \nAfterward, I am setting up the hyperparameters grid and utilize the __tune_clf_hyperparameters__ function to pinpoint the optimal hyperparameters for our RF model:","metadata":{}},{"cell_type":"code","source":"param_grid_rf = {\n    'n_estimators': [10, 30, 50, 70, 100],\n    'criterion': ['gini', 'entropy'],\n    'max_depth': [2, 3, 4],\n    'min_samples_split': [2, 3, 4, 5],\n    'min_samples_leaf': [1, 2, 3],\n    'bootstrap': [True, False]\n}","metadata":{"execution":{"iopub.status.busy":"2023-08-11T13:08:02.39865Z","iopub.execute_input":"2023-08-11T13:08:02.39941Z","iopub.status.idle":"2023-08-11T13:08:02.411135Z","shell.execute_reply.started":"2023-08-11T13:08:02.399378Z","shell.execute_reply":"2023-08-11T13:08:02.409777Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Using the tune_clf_hyperparameters function to get the best estimator\nbest_rf, best_rf_hyperparams = tune_clf_hyperparameters(rf_base, param_grid_rf, X_train, y_train)\nprint('RF Optimal Hyperparameters: \\n', best_rf_hyperparams)","metadata":{"execution":{"iopub.status.busy":"2023-08-11T13:08:02.412802Z","iopub.execute_input":"2023-08-11T13:08:02.413187Z","iopub.status.idle":"2023-08-11T13:09:34.252546Z","shell.execute_reply.started":"2023-08-11T13:08:02.413159Z","shell.execute_reply":"2023-08-11T13:09:34.250933Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"____\n<a id=\"rf_eval\"></a>\n# <b><span style='color:#ff826e'>Step 7.3 |</span><span style='color:red'> RF Model Evaluation</span></b>","metadata":{}},{"cell_type":"markdown","source":"<div style=\"border-radius:10px; padding: 15px; background-color: #facfc8; font-size:115%; text-align:left\">\n\nFinally, I am evaluating the model's performance on both the training and test datasets:","metadata":{}},{"cell_type":"code","source":"# Evaluate the optimized model on the train data\nprint(classification_report(y_train, best_rf.predict(X_train)))","metadata":{"execution":{"iopub.status.busy":"2023-08-11T13:09:34.255063Z","iopub.execute_input":"2023-08-11T13:09:34.255593Z","iopub.status.idle":"2023-08-11T13:09:34.283446Z","shell.execute_reply.started":"2023-08-11T13:09:34.255546Z","shell.execute_reply":"2023-08-11T13:09:34.282Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Evaluate the optimized model on the test data\nprint(classification_report(y_test, best_rf.predict(X_test)))","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2023-08-11T13:09:34.28539Z","iopub.execute_input":"2023-08-11T13:09:34.286602Z","iopub.status.idle":"2023-08-11T13:09:34.310759Z","shell.execute_reply.started":"2023-08-11T13:09:34.286548Z","shell.execute_reply":"2023-08-11T13:09:34.308995Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<div style=\"border-radius:10px; padding: 15px; background-color: #facfc8; font-size:115%; text-align:left\">\n\nThe RF model's similar performance on both training and test data suggests it isn't overfitting.","metadata":{}},{"cell_type":"code","source":"rf_evaluation = evaluate_model(best_rf, X_test, y_test, 'RF')\nrf_evaluation","metadata":{"execution":{"iopub.status.busy":"2023-08-11T13:09:34.313258Z","iopub.execute_input":"2023-08-11T13:09:34.313825Z","iopub.status.idle":"2023-08-11T13:09:34.352269Z","shell.execute_reply.started":"2023-08-11T13:09:34.313777Z","shell.execute_reply":"2023-08-11T13:09:34.350866Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<a id=\"knn\"></a>\n# <p style=\"background-color:red; font-family:calibri; color:white; font-size:150%; text-align:center; border-radius:15px 50px;\">Step 8 | KNN Model Building</p>\n\n⬆️ [Tabel of Contents](#contents_tabel)","metadata":{}},{"cell_type":"markdown","source":"____\n<a id=\"knn_base\"></a>\n# <b><span style='color:#ff826e'>Step 8.1 |</span><span style='color:red'> KNN Base Model Definition</span></b>","metadata":{}},{"cell_type":"markdown","source":"<div style=\"border-radius:10px; padding: 15px; background-color: #facfc8; font-size:115%; text-align:left\">\n\nFirst of all, let's define the base KNN model and set up the pipeline with scaling:","metadata":{}},{"cell_type":"code","source":"# Define the base KNN model and set up the pipeline with scaling\nknn_pipeline = Pipeline([\n    ('scaler', StandardScaler()),\n    ('knn', KNeighborsClassifier())\n])","metadata":{"execution":{"iopub.status.busy":"2023-08-11T13:09:34.363153Z","iopub.execute_input":"2023-08-11T13:09:34.363584Z","iopub.status.idle":"2023-08-11T13:09:34.369575Z","shell.execute_reply.started":"2023-08-11T13:09:34.363553Z","shell.execute_reply":"2023-08-11T13:09:34.36793Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"____\n<a id=\"knn_hp\"></a>\n# <b><span style='color:#ff826e'>Step 8.2 |</span><span style='color:red'> KNN Hyperparameter Tuning</span></b>","metadata":{}},{"cell_type":"markdown","source":"<div style=\"border-radius:10px; padding: 15px; background-color: #facfc8; font-size:115%; text-align:left\">\n\nI'll set up the hyperparameters grid and utilize the __tune_clf_hyperparameters__ function to pinpoint the optimal hyperparameters for our KNN pipeline:","metadata":{}},{"cell_type":"code","source":"# Hyperparameter grid for KNN\nknn_param_grid = {\n    'knn__n_neighbors': list(range(1, 12)),\n    'knn__weights': ['uniform', 'distance'],\n    'knn__p': [1, 2]  # 1: Manhattan distance, 2: Euclidean distance\n}","metadata":{"execution":{"iopub.status.busy":"2023-08-11T13:09:34.372027Z","iopub.execute_input":"2023-08-11T13:09:34.372536Z","iopub.status.idle":"2023-08-11T13:09:34.389396Z","shell.execute_reply.started":"2023-08-11T13:09:34.372491Z","shell.execute_reply":"2023-08-11T13:09:34.38822Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Hyperparameter tuning for KNN\nbest_knn, best_knn_hyperparams = tune_clf_hyperparameters(knn_pipeline, knn_param_grid, X_train, y_train)\nprint('KNN Optimal Hyperparameters: \\n', best_knn_hyperparams)","metadata":{"execution":{"iopub.status.busy":"2023-08-11T13:09:34.391006Z","iopub.execute_input":"2023-08-11T13:09:34.392278Z","iopub.status.idle":"2023-08-11T13:09:35.157065Z","shell.execute_reply.started":"2023-08-11T13:09:34.392239Z","shell.execute_reply":"2023-08-11T13:09:35.155843Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"____\n<a id=\"knn_eval\"></a>\n# <b><span style='color:#ff826e'>Step 8.3 |</span><span style='color:red'> KNN Model Evaluation</span></b>","metadata":{}},{"cell_type":"markdown","source":"<div style=\"border-radius:10px; padding: 15px; background-color: #facfc8; font-size:115%; text-align:left\">\n\nLet's evaluate the model's performance on both the training and test datasets:","metadata":{}},{"cell_type":"code","source":"# Evaluate the optimized model on the train data\nprint(classification_report(y_train, best_knn.predict(X_train)))","metadata":{"execution":{"iopub.status.busy":"2023-08-11T13:09:35.15844Z","iopub.execute_input":"2023-08-11T13:09:35.158788Z","iopub.status.idle":"2023-08-11T13:09:35.320144Z","shell.execute_reply.started":"2023-08-11T13:09:35.158757Z","shell.execute_reply":"2023-08-11T13:09:35.318153Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Evaluate the optimized model on the test data\nprint(classification_report(y_test, best_knn.predict(X_test)))","metadata":{"execution":{"iopub.status.busy":"2023-08-11T13:09:35.322796Z","iopub.execute_input":"2023-08-11T13:09:35.323332Z","iopub.status.idle":"2023-08-11T13:09:35.351531Z","shell.execute_reply.started":"2023-08-11T13:09:35.323287Z","shell.execute_reply":"2023-08-11T13:09:35.350661Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<div style=\"border-radius:10px; padding: 15px; background-color: #facfc8; font-size:115%; text-align:left\">\n\nThe KNN model's consistent scores across training and test sets indicate no overfitting.","metadata":{}},{"cell_type":"code","source":"knn_evaluation = evaluate_model(best_knn, X_test, y_test, 'KNN')\nknn_evaluation","metadata":{"execution":{"iopub.status.busy":"2023-08-11T13:09:35.352931Z","iopub.execute_input":"2023-08-11T13:09:35.353463Z","iopub.status.idle":"2023-08-11T13:09:35.391137Z","shell.execute_reply.started":"2023-08-11T13:09:35.353431Z","shell.execute_reply":"2023-08-11T13:09:35.390266Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<a id=\"svm\"></a>\n# <p style=\"background-color:red; font-family:calibri; color:white; font-size:150%; text-align:center; border-radius:15px 50px;\">Step 9 | SVM Model Building</p>\n\n⬆️ [Tabel of Contents](#contents_tabel)","metadata":{}},{"cell_type":"markdown","source":"____\n<a id=\"svm_base\"></a>\n# <b><span style='color:#ff826e'>Step 9.1 |</span><span style='color:red'> SVM Base Model Definition</span></b>","metadata":{}},{"cell_type":"markdown","source":"<div style=\"border-radius:10px; padding: 15px; background-color: #facfc8; font-size:115%; text-align:left\">\n\nFirst, let's define the base SVM model and set up the pipeline with scaling:","metadata":{}},{"cell_type":"code","source":"svm_pipeline = Pipeline([\n    ('scaler', StandardScaler()),\n    ('svm', SVC(probability=True)) \n])","metadata":{"execution":{"iopub.status.busy":"2023-08-11T13:09:35.392499Z","iopub.execute_input":"2023-08-11T13:09:35.392831Z","iopub.status.idle":"2023-08-11T13:09:35.397584Z","shell.execute_reply.started":"2023-08-11T13:09:35.392802Z","shell.execute_reply":"2023-08-11T13:09:35.396684Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"____\n<a id=\"svm_hp\"></a>\n# <b><span style='color:#ff826e'>Step 9.2 |</span><span style='color:red'> SVM Hyperparameter Tuning</span></b>","metadata":{}},{"cell_type":"markdown","source":"<div style=\"border-radius:10px; padding: 15px; background-color: #facfc8; font-size:115%; text-align:left\">\n    \nLet's configure the hyperparameters grid and employ the __tune_clf_hyperparameters__ function to determine the best hyperparameters for our SVM pipeline:","metadata":{}},{"cell_type":"code","source":"param_grid_svm = {\n    'svm__C': [0.0011, 0.005, 0.01, 0.05, 0.1, 1, 10, 20],\n    'svm__kernel': ['linear', 'rbf', 'poly'],\n    'svm__gamma': ['scale', 'auto', 0.1, 0.5, 1, 5],  \n    'svm__degree': [2, 3, 4]\n}","metadata":{"execution":{"iopub.status.busy":"2023-08-11T13:09:35.399168Z","iopub.execute_input":"2023-08-11T13:09:35.399492Z","iopub.status.idle":"2023-08-11T13:09:35.41351Z","shell.execute_reply.started":"2023-08-11T13:09:35.399464Z","shell.execute_reply":"2023-08-11T13:09:35.412231Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Call the function for hyperparameter tuning\nbest_svm, best_svm_hyperparams = tune_clf_hyperparameters(svm_pipeline, param_grid_svm, X_train, y_train)\nprint('SVM Optimal Hyperparameters: \\n', best_svm_hyperparams)","metadata":{"execution":{"iopub.status.busy":"2023-08-11T13:09:35.414783Z","iopub.execute_input":"2023-08-11T13:09:35.415857Z","iopub.status.idle":"2023-08-11T13:09:48.777691Z","shell.execute_reply.started":"2023-08-11T13:09:35.41582Z","shell.execute_reply":"2023-08-11T13:09:48.776323Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"____\n<a id=\"svm_eval\"></a>\n# <b><span style='color:#ff826e'>Step 9.3 |</span><span style='color:red'> SVM Model Evaluation</span></b>","metadata":{}},{"cell_type":"markdown","source":"<div style=\"border-radius:10px; padding: 15px; background-color: #facfc8; font-size:115%; text-align:left\">\n\nLet's evaluate our SVM model's performance on both the training and test datasets:","metadata":{}},{"cell_type":"code","source":"# Evaluate the optimized model on the train data\nprint(classification_report(y_train, best_svm.predict(X_train)))","metadata":{"execution":{"iopub.status.busy":"2023-08-11T13:09:48.779375Z","iopub.execute_input":"2023-08-11T13:09:48.779786Z","iopub.status.idle":"2023-08-11T13:09:48.801701Z","shell.execute_reply.started":"2023-08-11T13:09:48.779752Z","shell.execute_reply":"2023-08-11T13:09:48.799732Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Evaluate the optimized model on the test data\nprint(classification_report(y_test, best_svm.predict(X_test)))","metadata":{"execution":{"iopub.status.busy":"2023-08-11T13:09:48.803537Z","iopub.execute_input":"2023-08-11T13:09:48.803964Z","iopub.status.idle":"2023-08-11T13:09:48.822659Z","shell.execute_reply.started":"2023-08-11T13:09:48.80388Z","shell.execute_reply":"2023-08-11T13:09:48.821345Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<div style=\"border-radius:10px; padding: 15px; background-color: #facfc8; font-size:120%; text-align:left\">\n    \n<h2 align=\"left\"><font color=red>✅ Inference:</font></h2>\n\nThe __recall of 0.97 for class 1__ indicates that almost all the __true positive cases (i.e., patients with heart disease)__ are correctly identified. This high recall is of utmost importance in a medical context, where missing a patient with potential heart disease could have dire consequences.\n\nHowever, it's also worth noting the balanced performance of the model. With an __F1-score of 0.83 for class 1__, it's evident that the model doesn't merely focus on maximizing recall at the expense of precision. This means the reduction in False Negatives hasn't significantly increased the False Positives, ensuring that the cost and effort of examining healthy individuals are not unnecessarily high.\n\nOverall, the model's performance is promising for medical diagnostics, especially when prioritizing the accurate identification of patients with heart disease without overburdening the system with false alarms.","metadata":{}},{"cell_type":"code","source":"svm_evaluation = evaluate_model(best_svm, X_test, y_test, 'SVM')\nsvm_evaluation","metadata":{"execution":{"iopub.status.busy":"2023-08-11T13:09:48.824538Z","iopub.execute_input":"2023-08-11T13:09:48.824924Z","iopub.status.idle":"2023-08-11T13:09:48.861648Z","shell.execute_reply.started":"2023-08-11T13:09:48.82487Z","shell.execute_reply":"2023-08-11T13:09:48.860754Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<a id=\"conclusion\"></a>\n# <p style=\"background-color:red; font-family:calibri; color:white; font-size:150%; text-align:center; border-radius:15px 50px;\">Step 10 | Conclusion</p>\n\n⬆️ [Tabel of Contents](#contents_tabel)","metadata":{}},{"cell_type":"markdown","source":"<div style=\"border-radius:10px; padding: 15px; background-color: #facfc8; font-size:120%; text-align:left\">\n\nIn the critical context of diagnosing heart disease, our primary objective is __to ensure a high recall for the positive class__. It's imperative to accurately identify every potential heart disease case, as even one missed diagnosis could have dire implications. However, while striving for this high recall, it's essential to maintain a balanced performance to avoid unnecessary medical interventions for healthy individuals. We'll now evaluate our models against these crucial medical benchmarks.\n\n","metadata":{}},{"cell_type":"code","source":"# Concatenate the dataframes\nall_evaluations = [dt_evaluation, rf_evaluation, knn_evaluation, svm_evaluation]\nresults = pd.concat(all_evaluations)\n\n# Sort by 'recall_1'\nresults = results.sort_values(by='recall_1', ascending=False).round(2)\nresults","metadata":{"execution":{"iopub.status.busy":"2023-08-11T13:09:48.86303Z","iopub.execute_input":"2023-08-11T13:09:48.863423Z","iopub.status.idle":"2023-08-11T13:09:48.892826Z","shell.execute_reply.started":"2023-08-11T13:09:48.863391Z","shell.execute_reply":"2023-08-11T13:09:48.890269Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Sort values based on 'recall_1'\nresults.sort_values(by='recall_1', ascending=True, inplace=True)\nrecall_1_scores = results['recall_1']\n\n# Plot the horizontal bar chart\nfig, ax = plt.subplots(figsize=(12, 7), dpi=70)\nax.barh(results.index, recall_1_scores, color='red')\n\n# Annotate the values and indexes\nfor i, (value, name) in enumerate(zip(recall_1_scores, results.index)):\n    ax.text(value + 0.01, i, f\"{value:.2f}\", ha='left', va='center', fontweight='bold', color='red', fontsize=15)\n    ax.text(0.1, i, name, ha='left', va='center', fontweight='bold', color='white', fontsize=25)\n\n# Remove yticks\nax.set_yticks([])\n\n# Set x-axis limit\nax.set_xlim([0, 1.2])\n\n# Add title and xlabel\nplt.title(\"Recall for Positive Class across Models\", fontweight='bold', fontsize=22)\nplt.xlabel('Recall Value', fontsize=16)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-08-11T13:09:48.895202Z","iopub.execute_input":"2023-08-11T13:09:48.895871Z","iopub.status.idle":"2023-08-11T13:09:49.259096Z","shell.execute_reply.started":"2023-08-11T13:09:48.895825Z","shell.execute_reply":"2023-08-11T13:09:49.25823Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<div style=\"border-radius:10px; padding: 15px; background-color: #facfc8; font-size:120%; text-align:left\">\n\n__The SVM model demonstrates a commendable capability in recognizing potential heart patients. With a recall of 0.97 for class 1, it's evident that almost all patients with heart disease are correctly identified. This is of paramount importance in a medical setting. However, the model's balanced performance ensures that while aiming for high recall, it doesn't compromise on precision, thereby not overburdening the system with unnecessary alerts.__\n\n","metadata":{}},{"cell_type":"markdown","source":"<div style=\"display: flex; align-items: center; justify-content: center; border-radius: 10px; padding: 20px; background-color: #facfc8; font-size: 120%; text-align: center;\">\n\n<strong>🎯 If you need more information or want to explore the code, feel free to visit the project repository on <a href=\"https://github.com/FarzadNekouee/Heart_Disease_Prediction\">GitHub</a> 🎯</strong>\n</div>\n","metadata":{}},{"cell_type":"markdown","source":"<h2 align=\"left\"><font color='red'>Best Regards!</font></h2>","metadata":{}}]}