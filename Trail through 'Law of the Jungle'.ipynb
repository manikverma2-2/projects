{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":16394,"databundleVersionId":827241,"sourceType":"competition"},{"sourceId":23724,"databundleVersionId":1691923,"sourceType":"competition"},{"sourceId":31480,"databundleVersionId":2754275,"sourceType":"competition"},{"sourceId":39462,"databundleVersionId":4458699,"sourceType":"competition"},{"sourceId":161079,"sourceType":"datasetVersion","datasetId":70947},{"sourceId":4184141,"sourceType":"datasetVersion","datasetId":2316397},{"sourceId":4582626,"sourceType":"datasetVersion","datasetId":2657736},{"sourceId":11976737,"sourceType":"datasetVersion","datasetId":2626311},{"sourceId":112036811,"sourceType":"kernelVersion"}],"dockerImageVersionId":30301,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\nimport matplotlib.pyplot as plt\nfrom matplotlib.patches import Arc\nfrom matplotlib.patches import Rectangle\nfrom matplotlib.patches import Circle\n\nimport matplotlib.ticker as mtick\nfrom matplotlib.patches import ConnectionPatch\nfrom matplotlib.patches import FancyBboxPatch\nimport matplotlib\nimport matplotlib.ticker as mtick\nimport seaborn as sns\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2022-11-28T16:01:30.837904Z","iopub.execute_input":"2022-11-28T16:01:30.838576Z","iopub.status.idle":"2022-11-28T16:01:32.20204Z","shell.execute_reply.started":"2022-11-28T16:01:30.83844Z","shell.execute_reply":"2022-11-28T16:01:32.200842Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Kaggle Surveys from 2018 to 2022\ndf_18 = pd.read_csv('/kaggle/input/kaggle-survey-2018/multipleChoiceResponses.csv') ; df_18.drop([0], inplace=True)\ndf_19 = pd.read_csv('/kaggle/input/kaggle-survey-2019/multiple_choice_responses.csv') ; df_19.drop([0], inplace=True)\ndf_20 = pd.read_csv('/kaggle/input/kaggle-survey-2020/kaggle_survey_2020_responses.csv') ; df_20.drop([0], inplace=True)\ndf_21 = pd.read_csv('/kaggle/input/kaggle-survey-2021/kaggle_survey_2021_responses.csv') ; df_21.drop([0], inplace=True)\ndf_22 = pd.read_csv('/kaggle/input/kaggle-survey-2022/kaggle_survey_2022_responses.csv') ; df_22.drop([0], inplace=True)\n\n# A Helper Spreadsheet I've made which should work with next iterations of the competition as well,\n# It makes looking up columns easy\n# It was created over a span of 5-7 Days whilst I was studying the Data, Previous Competitions Entries, Expecations and whiteboarding\n\nlook_up = pd.read_csv('/kaggle/input/kaggle-survey-analytics-helper/Look_up.csv')\nlook_up.reset_index(inplace=True) ; look_up.fillna(0, inplace=True)\n\n# Internet prices around the World, Per Gigabyte\n# Credits to cable.co.uk for originally publishing this Dataset\n# https://www.cable.co.uk/mobiles/worldwide-data-pricing/\ninternet_prices = pd.read_csv('/kaggle/input/1-gb-internet-price/worldwide internet prices in 2022 - IN 2022.csv')\n\n# Layoffs since Covid-19\n# Thanks to Roger Lee @Twitter.com/roger_lee at (https://layoffs.fyi/) for originally compiling this Information\nlayoffs = pd.read_csv('/kaggle/input/layoffs-2022/layoffs.csv')\n\n# Thanks to Google (For everything thats not mentioned :) \ntrend = pd.read_csv('/kaggle/input/kaggle-survey-analytics-helper/trends_becoming_analyst_scientist.csv') # Google_Trends_Query\ntrend.reset_index(inplace=True)\ntrend = trend.rename(columns=trend.iloc[0])\ntrend.drop([0], inplace=True)\ntrend.columns = ['Month', 'Data_Analyst', 'Data_Scientist']","metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2022-11-28T16:01:32.204274Z","iopub.execute_input":"2022-11-28T16:01:32.205094Z","iopub.status.idle":"2022-11-28T16:01:39.110955Z","shell.execute_reply.started":"2022-11-28T16:01:32.205058Z","shell.execute_reply":"2022-11-28T16:01:39.109613Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def value_counts(dataframe, query, year=False, append_year=False):\n    \"\"\"\n    Prefer passing through fetch_ts\n    Function returns a Dataframe having columns with matched query\n    Parameters\n    ----------\n        dataframe : pandas dataframe\n            Dataframe to look into\n        year : numeric | Default : False\n            i.e No Year will be Appended to the Columns | Year to prepend : (Year)_Market_Share & (Year)_Values\n        append_year : string | Default = False\n            Value passed will be set as value for a new column named 'Year'\n    \n    Example :\n        value_counts(df_22, 12, year=2022)\n    *NOTE*\n    Should not be used for 'Percent_of_Respondents' if the option 'Select all that apply' is enabled.\n    In a scenario where a respondent has select more than 1 option, Their vote will be counted more than once as well\n    Resulting into false statistics when you sum() the column\n    \n    Changelog :\n    IMP. query = Question in dataframe, MUST BE IN THE STRING FORMAT 'Q2' / 'Q27_A' / 'Q30_B', Inclusive of 'Q' > Will be improved later\n    ^ Fixed. Simply enter the number ^\n    \"\"\"\n\n    if isinstance(query, int):\n        query = 'Q'+str(query)\n\n    msp = 'Market_Share_Percent'\n    ps = 'Percent_of_respondents'\n\n    # if append_year == False:\n    #     msp = 'Market_Share_Percent'\n    #     val = 'Value'\n    #     year = ''\n    #     ps = 'Percent_of_respondents'\n    \n    temp_frame = pd.DataFrame()\n    for x in dataframe[[col for col in dataframe.columns if str(query) in col]].columns:\n        temp_frame = pd.concat(\n            [temp_frame, dataframe[x].value_counts()] )\n    temp_frame.reset_index(inplace=True)\n    temp_frame.columns = [ 'Element','Value' ]\n    \n    if append_year != False:\n        temp_frame[ str(append_year)+'_'+msp ] = ( temp_frame.Value / temp_frame.Value.sum() ) *100\n        temp_frame[ str(append_year)+'_'+ps  ] = ( temp_frame.Value / len(dataframe) ) *100\n        # The year is already appended in the column name\n        # temp_frame['Year_' + str(append_year)] = append_year\n        \n        temp_frame.rename( {'Value':str(append_year)+'_Value'}, axis=1, inplace=True )\n        temp_frame.sort_values( by=str(append_year)+'_Value', ascending=False, inplace=True )\n    else:\n        temp_frame[ msp ] = ( temp_frame.Value / temp_frame.Value.sum() ) *100\n        temp_frame[ ps ] = ( temp_frame.Value / len(dataframe) ) * 100\n        temp_frame['Year'] = year\n        temp_frame.sort_values( by='Value', ascending=False, inplace=True )    \n\n    # if append_year != False:\n    #     if year == False:\n    #         temp_frame['Year'] = append_year\n    #     else : temp_frame[ 'Year_'+str(year) ] = int( year ) # else it will be an object, save memory, save storage, Ultimately save the planet, Thats our motto.\n    return temp_frame","metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2022-11-28T16:01:39.112672Z","iopub.execute_input":"2022-11-28T16:01:39.112987Z","iopub.status.idle":"2022-11-28T16:01:39.126874Z","shell.execute_reply.started":"2022-11-28T16:01:39.11296Z","shell.execute_reply":"2022-11-28T16:01:39.125272Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def fetch_ts(frame, temp_query, year, merge=False, append_year=False):\n    \"\"\"\n    Function Returns a dataframe for TimeSeries analysys   \n    Parameters\n    ----------\n        frame : list\n            List of Dataframes to look into\n        temp_query : String or List\n            Index can be found in Look_up\n            List of Query i.e Either the [ Index of desired Question ] or [ Index of desired Question, 'Name of Element' to keep as string ]\n        year : list\n            List of Years to prepend to Each years : (Year)_Market_Share & (Year)_Values\n    Example :\n        fetch_ts([df_20, df_21, df_22], 12, ['2020', '2021', '2022'], merge=False, append_year=False)\n    *NOTE*\n    Should not be used for 'Percent_of_Respondents' if the option 'Select all that apply' is enabled.\n    In a scenario where a respondent has select more than 1 option, Their vote will be counted more than once as well\n    Resulting into false statistics when you sum() the column\n    \"\"\"\n    if len(frame) != len(year):\n        return print( 'Error : Dataframe & Year list not of same size' )\n    \n    if isinstance(temp_query, list):\n        query = temp_query[0]\n    else:\n        query = temp_query\n\n    temp_frame = pd.DataFrame()\n    \n    for x in range(0,len(frame)):\n        index_in_frame = 'Q'+str(look_up.loc[query, year[x]])\n        if index_in_frame == 'Q0':\n            print(f'Iteration Skipped, Query not Found in Year : '+str(year[x]))\n            continue\n        if append_year != False:\n            temp_frame_2 = value_counts(pd.DataFrame(frame[x]), index_in_frame, append_year=year[x])\n        else:\n            temp_frame_2 = value_counts(pd.DataFrame(frame[x]), index_in_frame, year=year[x]) #apy = false on default\n        \n        if merge != False:\n            if x == 0:\n                temp_frame = temp_frame_2\n            if x > 0:\n                temp_frame = temp_frame.merge(temp_frame_2, on='Element', how='outer')\n        else : temp_frame = pd.concat([temp_frame, temp_frame_2], ignore_index=True)\n\n    if isinstance(temp_query, list):\n        temp_frame = temp_frame[temp_frame['Element'] == temp_query[1]] # Return only the filtered Element if requested\n    return temp_frame","metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2022-11-28T16:01:39.130166Z","iopub.execute_input":"2022-11-28T16:01:39.131625Z","iopub.status.idle":"2022-11-28T16:01:39.1494Z","shell.execute_reply.started":"2022-11-28T16:01:39.131548Z","shell.execute_reply":"2022-11-28T16:01:39.147645Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def fetch_ts_by_category(frames, year, cat_query, ele_query):\n    \"\"\"\n    This enables you to compare any 2 columns in the Dataframe\n    Kudos to Look Up table, and 7 days spent on figuring out the data\n\n    Function Returns a dataframe for TimeSeries analysys   \n    Parameters\n    ----------\n        frames : list\n            List of Dataframes to look into\n        year : list\n            List of Years to prepend to Each years : (Year)_Market_Share & (Year)_Values\n        Frames & year should be of the same length and in the same Order\n        ele_query : int\n            Index can be found in Look_up\n            Index of desired Question\n        cat_query : int\n            Category you would like to group on\n            Index can be found in Look_Up\n    Example:\n        Which BI Tools were used in past 3 years\n        + Group them by Country\n        fetch_ts_by_category(frames=[df_20, df_21, df_22], year=['2020', '2021', '2022'], cat_query=3, ele_query=45)\n    \"\"\"\n\n    compiled = pd.DataFrame()\n\n    for i in range(0,len(year)):\n        dataframe = pd.DataFrame( frames[i] )\n        category_index_in_frame = 'Q'+str(look_up.loc[cat_query, year[i]])\n        category_list = list(dataframe[category_index_in_frame].unique())\n        # Test Case\n        # print(category_index_in_frame)\n        # print(category_list)\n\n        for cat in category_list:\n            # Test Case\n            # print('In cat loop', cat)\n            index_in_frame = 'Q'+str(look_up.loc[cat_query, year[i]])\n            temp_df = dataframe[dataframe[index_in_frame] == cat]\n            d_22 = fetch_ts([temp_df], ele_query, year=[year[i]], merge=False, append_year=False) # 45 = BI Tools\n            d_22['Category'] = cat\n            compiled = pd.concat([compiled, d_22], ignore_index=True)\n    return compiled","metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2022-11-28T16:01:39.151009Z","iopub.execute_input":"2022-11-28T16:01:39.151432Z","iopub.status.idle":"2022-11-28T16:01:39.172032Z","shell.execute_reply.started":"2022-11-28T16:01:39.151393Z","shell.execute_reply":"2022-11-28T16:01:39.170486Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def give_me_role(row):\n    if 'Analyst' in row['Element']:\n        return 'Data Analyst'\n    if 'Scientist' in row['Element']:\n         return 'Data Scientist'\n    return 'Data Engineer'\n\n\"\"\"\nIts purpuse is to have these roles on the same Y-Level, Same line & X would make a difference in their value as we go across\n\"\"\"\ndef give_me_Y(row):\n    if 'Analyst' in row['Role']:\n        return 3\n    if 'Scientist' in row['Role']:\n         return 2\n    return 1\n\ndef give_me_company(row):\n    if 'Amazon' in row['Element']:\n        return 'Amazon'\n    if 'Google' in row['Element']:\n         return 'Google'\n    if 'Microsoft' in row['Element']:\n         return 'Microsoft'\n    if 'IBM' in row['Element']:\n         return 'IBM'\n    return 'None or Other'","metadata":{"_kg_hide-input":true,"jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2022-11-28T16:01:39.174113Z","iopub.execute_input":"2022-11-28T16:01:39.175364Z","iopub.status.idle":"2022-11-28T16:01:39.192608Z","shell.execute_reply.started":"2022-11-28T16:01:39.175315Z","shell.execute_reply":"2022-11-28T16:01:39.190704Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def share_of_year(row, temp_df):\n    temp_df = pd.DataFrame(temp_df)\n    \"\"\"\n    Useful after performing / slicing on Market_Share\n    Else, It will be same as 'Percent_of_respondents' after fetching a time-series slice\n    Returns\n        Market Share in percent for that 'Year'\n    ----------\n    Requirements :\n        Columns -> Percent_of_respondents, Year\n        type -> Numeric\n    \"\"\"\n    return (\n            row['Percent_of_respondents'] / temp_df[temp_df.Year == row['Year']].Percent_of_respondents.sum()\n            ) * 100","metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2022-11-28T16:01:39.194205Z","iopub.execute_input":"2022-11-28T16:01:39.194685Z","iopub.status.idle":"2022-11-28T16:01:39.211971Z","shell.execute_reply.started":"2022-11-28T16:01:39.194642Z","shell.execute_reply":"2022-11-28T16:01:39.210662Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def atleast_one(dataframe, query, year):\n    \"\"\"\n    This function with calculate the percentage of unique respondents for subset of columns matching that Query\n    Use with learning_curve\n    Parameters\n    ----------\n        dataframe : pandas Dataframe\n        query : int\n            Query from Look_up index\n        year : year\n    \"\"\"\n    if query != False:\n        index_in_frame = 'Q'+str(look_up.loc[query, year])\n        # print(index_in_frame)\n        dataframe = pd.DataFrame(dataframe)\n        dataframe = dataframe[[col for col in dataframe.columns if index_in_frame in col]]\n        # dataframe = dataframe.fillna(0)\n    x = False\n    for x in dataframe.columns:\n                l = dataframe[x].unique()\n                if 'None' in l:\n                        dataframe.drop(columns=[x], inplace=True)\n                        x = True\n                        break\n                # print('Checking')\n    if x == False:\n           print('Drop fail')\n    dataframe = dataframe.notna() # converts to True and False values\n    dataframe_len = len(dataframe[dataframe.any(axis=1)])\n    return  ( dataframe_len / len(dataframe) ) * 100\n\n# Test_Case\n# Total number of votes for a tech (36 in look_up)\n# Values after None matches the Total of -> dataframe = dataframe.notna()\n                # t1 = test.notna()\n                # t1 = t1[t1.any(axis=1)]\n                # l = []\n                # for x in t1.columns:\n                #     l.append(\n                #         t1[x].sum()\n                #     )\n                # sum(l)\n\n# test_2 = fetch_ts([df_22], 36, year=['2022'], merge=False, append_year=False)# ['Value'].sum()\n# test_2[test_2.Element != 'None']['Value'].sum()","metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2022-11-28T16:01:39.213195Z","iopub.execute_input":"2022-11-28T16:01:39.21411Z","iopub.status.idle":"2022-11-28T16:01:39.230112Z","shell.execute_reply.started":"2022-11-28T16:01:39.214067Z","shell.execute_reply":"2022-11-28T16:01:39.228405Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def learning_curve(frames, query, year):\n    \"\"\"\n    Returns : pandas Dataframe\n        Number of respondents claiming to know that tech in the 2 years in Percent % & respondents who already know that tech, Without double counting their votes (Options with select all that apply)\n    Parameters\n    ----------\n        frames : pandas dataframe\n            Dataframe to look into\n        query : list (int)\n            List of query i.e Index in Look_Up for Will learn in next 2 yrs, People who already know\n        year : list\n            List of Year\n\n    IMP NOTE: All parameters must be of the same size & in the same order (Query too (Will_Learn , Knows_Already) ) -> Look_Up\n        # example :\n        #   learning_curve([df_20, df_21], [35, 36], year=['2020','2021'])\n    \"\"\"\n\n    if len(frames) != len(year):\n        return print( 'Error : Dataframe & Year list not of same size' )\n    \n    df = pd.DataFrame( columns=['Element', 'Value', 'Year'] )\n\n    for x,query in enumerate(query):\n        \n\n        if x == 0: # Will_Know\n            # For query Will_Know aka first element of Array\n            # Go through every year and frame ( using index to directly reference, save silicon? )\n            for i in range(0, len(year)):\n                # For_Testing\n                # print('x=0', i, year, query)\n                will_know = atleast_one(pd.DataFrame(frames[i]), query=query, year=year[i])\n                \n                df.loc[len(df.index)] = ['Will_Know', will_know, year[i]]\n        \n        if x == 1: # Already_Knows            \n            for i in range(0, len(year)):\n                # For_Testing\n                # print('x=1', i, year, query)\n                know_already = atleast_one(pd.DataFrame(frames[i]), query=query, year=year[i])\n\n                df.loc[len(df.index)] = ['Know_Already', know_already, year[i]]\n        \n        # return df.melt(id_vars = ['Year'], var_name = 'Element', value_name = 'Value')\n    return df","metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2022-11-28T16:01:39.231678Z","iopub.execute_input":"2022-11-28T16:01:39.232081Z","iopub.status.idle":"2022-11-28T16:01:39.251945Z","shell.execute_reply.started":"2022-11-28T16:01:39.232048Z","shell.execute_reply":"2022-11-28T16:01:39.249956Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"pip install pycountry_convert","metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2022-11-28T16:01:39.256771Z","iopub.execute_input":"2022-11-28T16:01:39.257211Z","iopub.status.idle":"2022-11-28T16:01:54.413786Z","shell.execute_reply.started":"2022-11-28T16:01:39.257172Z","shell.execute_reply":"2022-11-28T16:01:54.411887Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Adapted 100% from -> https://www.youtube.com/watch?v=VMIRK36d-EY\n\nimport pycountry_convert as pc\n\ndef convert(row):\n    cn_code = pc.country_name_to_country_alpha2(row['Category'], cn_name_format='default')\n    conti_code = pc.country_alpha2_to_continent_code(cn_code)\n    return conti_code\n\nconti_names = {'AS':'Asia',\n                'SA':'South America',\n                'OC':'Oceania',\n                'EU':'Europe',\n                'NA':'North America',\n                'AF':'Africa'}","metadata":{"_kg_hide-input":true,"jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2022-11-28T16:01:54.415554Z","iopub.execute_input":"2022-11-28T16:01:54.41596Z","iopub.status.idle":"2022-11-28T16:01:54.428073Z","shell.execute_reply.started":"2022-11-28T16:01:54.415923Z","shell.execute_reply":"2022-11-28T16:01:54.42692Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Helper Viz Functions\n\ndef waterfall_chart(dataframe, show_till, bx_scale, by_scale, group_on=False): #args has Element, Value cols\n    \"\"\"\n    This function will return a create a progress_chart / waterfall_chart\n    Parameters\n    ----------\n        dataframe : list\n            Dataframe with 2 Columns namely -> [Element, Value]\n        show_till : Numeric\n            How many of values do you want to show\n        bx_scale : Numeric\n            Bar width in Percent (0 to 100)\n        by_scale :Numeric\n            Bar Height in Percent (0 to 100) with respect to x_lim (x_lim has been fixed to 100 for this specific project)\n    \n    Please visit my Notebook 'Pranavs custom viz' for an updated version\n    \"\"\"\n    df_col = [col for col in dataframe.columns if 'Value' in col][0]\n    if group_on != False:\n        x_lim = len(dataframe)/2\n        x_tick_labels = list(dataframe.group_on)\n    else :\n        x_lim = len(dataframe)\n        x_tick_labels = list(dataframe.Element)\n\n    show_total_tick=''\n    bar_range = show_till\n    if show_till == len(dataframe):\n        show_total_tick = 'Total'\n        bar_range = bar_range-1\n    \n    for x in range(show_till+1, len(dataframe)):\n        x_tick_labels[x] = ''\n\n    ax.set_ylim(0,100)\n    ax.set_xlim(0,x_lim+2)\n\n    ax.set_xticks([x for x in range(0,int(max(ax.get_xlim())+1))])\n    ax.set_xticklabels(['']+x_tick_labels+[show_total_tick,''])\n\n    sum = 0 # To hold previous bars Height\n\n    for x in range(0, bar_range+1): # +1 since range is exclusive\n        if x == 0:\n            sum = 0\n        else:\n            sum = sum + dataframe[df_col][x-1] / dataframe[df_col].sum() * by_scale # by_scale = Percentage scale in terms of x_lim\n        height = ( dataframe[df_col][x] / dataframe[df_col].sum() ) * by_scale\n        ax.add_patch(Rectangle(( x+0.65, sum ), bx_scale/100, height)) # bx_scale = bx_scale/100 to convert into bar_width between 0 to 1\n\n    \n    # # Total_Patch\n    if show_till == len(dataframe):\n        ax.add_patch(Rectangle(( (len(dataframe)+1)-0.45, 0), bx_scale/100, by_scale))\n    return None # not plt.show() so that we can modify the Title, Subtitle, etc","metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2022-11-28T16:01:54.429237Z","iopub.execute_input":"2022-11-28T16:01:54.429538Z","iopub.status.idle":"2022-11-28T16:01:54.449804Z","shell.execute_reply.started":"2022-11-28T16:01:54.429513Z","shell.execute_reply":"2022-11-28T16:01:54.448932Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Waterfall chart for grouped +ive and -ive results\n\ndef wc_group(d_list, show_till, bx_scale, by_scale, x_tick_to_set, bar_color, ax,): #args has Element, Value cols\n    \"\"\"\n    Parameters\n    ----------\n        d_list : list\n            List of 2 Dataframes, [Postive, Negative]\n            They Should have a Column named 'Value' or having 'Value'\n        show_till : numeric\n            Till what x_tick do you want to show, Use this for a great experience, but dont get carried away ;)\n        bx_scale : Numeric (0-100)\n            Length of bar\n        by_scale : Numeric (0-100)\n            Height of bar in perspective to Y-Axis, If you are working on a Frame with a total value of 100 | Set it to 100\n    \"\"\"\n    \n    show_till = show_till - 1\n    dataframe_1=pd.DataFrame(d_list[0])\n    dataframe_2=pd.DataFrame(d_list[1])\n    df_col = [col for col in dataframe_1.columns if 'Value' in col][0]\n    \n    x_lim = len(x_tick_to_set.unique())\n    x_tick_labels = list(x_tick_to_set.unique())\n\n    show_total_tick=''\n\n    bar_range = show_till\n    if show_till+1 == len(dataframe_1):\n        show_total_tick = 'Total'\n        bar_range = bar_range = len(x_tick_to_set.unique())-1\n    \n    for x in range(show_till+1, len(dataframe_1)):\n        x_tick_labels[x] = ''\n\n    ax.set_ylim(0,50)\n    ax.set_xlim(0,x_lim+2)\n    \n    # ax.set_xticks([x for x in range(0,int(max(axt_xlim())+1))])\n    ax.set_xticks([x for x in range(0, dataframe_1.Continent.nunique()+3)])\n\n    ax.set_xticklabels(['']+x_tick_labels+[show_total_tick,''])\n\n    sum = 0 # To hold previous bars Height\n    total_val = (dataframe_1[df_col].sum()+dataframe_2[df_col].sum())\n\n    for x in range(0, bar_range+1): # +1 since range is exclusive\n        if x == 0:\n            sum = 0\n        else:\n            sum = sum + dataframe_1[df_col][x-1] / total_val * by_scale # by_scale = Percentage scale in terms of x_lim\n        \n        height_1 = ( dataframe_1[df_col][x] / total_val ) * by_scale\n        height_2 = ( dataframe_2[df_col][x] / total_val ) * by_scale\n        right_bar_start=sum+height_1\n        \n        # Hatched\n        if dataframe_2[df_col][x] == dataframe_2[df_col].max():\n            facecolor=HEAL[5]\n            # if show_till+1 == len(dataframe_1):\n            #     ax.add_patch(Rectangle(( (len(dataframe_1)+1)-0.25, right_bar_start), bx_scale/100, -height_2, facecolor=facecolor, edgecolor='black', hatch='////', lw=0.4, zorder=2) ) # Total Patch\n            # ax.add_patch(Rectangle((0, right_bar_start ), bx_scale, -height_2, facecolor=facecolor, edgecolor='black', lw=1, ls='--', zorder=0, alpha=0.5)) # hspan\n        \n        ax.add_patch(Rectangle(( x + 0.99, right_bar_start ), bx_scale/100, -height_2, facecolor='none', edgecolor='black', hatch='//', lw=0.5,  zorder=0) )\n        \n        # Non-hatched\n        if dataframe_1[df_col][x] == dataframe_1[df_col].max():\n            facecolor='#39B54A'\n            # if show_till+1 == len(dataframe_1):\n            #     # ax.add_patch(Rectangle(( (len(dataframe_1)+1)-0.25, sum), bx_scale/100, height_1, facecolor=facecolor, edgecolor='black', lw=0.4, zorder=2) ) # Total Patch\n            # ax.add_patch(Rectangle((0, sum ), bx_scale, height_1, facecolor=facecolor, edgecolor='black', lw=1, ls='--', zorder=0, alpha=0.5)) # hspan\n        else :\n            facecolor=bar_color\n        ax.add_patch(Rectangle(( x+0.5, sum ), bx_scale/100, height_1, facecolor=facecolor, edgecolor='black', lw=0.5, zorder=1)) # bx_scale = bx_scale/100 to convert into bar_width between 0 to 1\n        \n        # ax.text(x+0.75, right_bar_start, str(round(dataframe_1[df_col][x],1))+'%', ha='center', color='green')\n        # Test_Case\n        # print('Itt > '+str(x+1))\n        # print('Y-cor > '+str(sum), height_1)\n        # print('Frame 2')\n        # print('Y-cor > '+str(right_bar_start), height_2)\n    \n    # Total_Patch\n    if show_till+1 == len(dataframe_1):\n        ax.add_patch(Rectangle(( (len(dataframe_1)+1)-0.25, 0), bx_scale/100, right_bar_start, facecolor=HEAL[1], edgecolor='black', lw=.2))\n    return None # not plt.show() so that we can modify plot properties","metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2022-11-28T16:01:54.451336Z","iopub.execute_input":"2022-11-28T16:01:54.451871Z","iopub.status.idle":"2022-11-28T16:01:54.47246Z","shell.execute_reply.started":"2022-11-28T16:01:54.451827Z","shell.execute_reply":"2022-11-28T16:01:54.470564Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Adapted with tiny changes from > https://matplotlib.org/stable/gallery/lines_bars_and_markers/horizontal_barchart_distribution.html\n\ndef discrete_distribution_horizontal(results, category_names, cmap='RdYlGn'):\n    \"\"\"\n    Parameters\n    ----------\n    results : dict\n        A mapping from question labels to a list of answers per category.\n        It is assumed all lists contain the same number of entries and that\n        it matches the length of *category_names*.\n    category_names : list of str\n        The category labels.\n    \"\"\"\n    labels = list(results.keys())\n    data = np.array(list(results.values()))\n    data_cum = data.cumsum(axis=1)\n    \n    category_colors = plt.colormaps[cmap](\n        np.linspace(0.15, 0.85, data.shape[1]))\n\n    ax.invert_yaxis()\n    ax.xaxis.set_visible(False)\n    \n    ax.spines.bottom.set_visible(False)\n    ax.spines.top.set_visible(False)\n    ax.set_xlim(0, np.sum(data, axis=1).max())\n\n    for i, (colname, color) in enumerate(zip(category_names, category_colors)):\n        widths = data[:, i]\n        starts = data_cum[:, i] - widths\n        rects = ax.barh(labels, widths, left=starts, height=0.9,\n                        label=colname, color=color, lw=.75, edgecolor=BG)\n\n        r, g, b, _ = color\n        text_color = 'white' if r * g * b < 0.5 else 'darkgrey'\n        ax.bar_label(rects, label_type='center', color=text_color)\n    ax.legend(ncol=len(category_names), bbox_to_anchor=(0, 1),\n              loc='lower left', fontsize='medium')\n\n    return fig, ax","metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2022-11-28T16:01:54.474176Z","iopub.execute_input":"2022-11-28T16:01:54.474494Z","iopub.status.idle":"2022-11-28T16:01:54.491633Z","shell.execute_reply.started":"2022-11-28T16:01:54.474468Z","shell.execute_reply":"2022-11-28T16:01:54.490186Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# function adapted 100% from\n# https://www.statology.org/seaborn-barplot-show-values/\n\ndef show_values(axs, orient=\"v\", space=.01):\n    def _single(ax):\n        if orient == \"v\":\n            for p in ax.patches:\n                _x = p.get_x() + p.get_width() / 2\n                _y = p.get_y() + p.get_height() + (p.get_height()*0.01)\n                value = '{:.1f}'.format(p.get_height())\n                ax.text(_x, _y, value, ha=\"center\") \n        elif orient == \"h\":\n            for p in ax.patches:\n                _x = p.get_x() + p.get_width() + float(space)\n                _y = p.get_y() + p.get_height() - (p.get_height()*0.5)\n                value = '{:.1f}'.format(p.get_width())\n                ax.text(_x, _y, value, ha=\"left\")\n\n    if isinstance(axs, np.ndarray):\n        for idx, ax in np.ndenumerate(axs):\n            _single(ax)\n    else:\n        _single(axs)","metadata":{"_kg_hide-input":true,"jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2022-11-28T16:01:54.493948Z","iopub.execute_input":"2022-11-28T16:01:54.494727Z","iopub.status.idle":"2022-11-28T16:01:54.509945Z","shell.execute_reply.started":"2022-11-28T16:01:54.494681Z","shell.execute_reply":"2022-11-28T16:01:54.508994Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Common_Color_Palete\n\nGBYT = ['#afc7d3','#4aaad3','#1a84c5','#3fbda4','#61bb46','#c8da38','#c8da38']\nSTA_HEATMAP = ['#afc7d3','#5cc9e5','#3daae1','#1a84c5','#e8b3b3']\nCMAP_PALLET = ['#003f5c','#2f4b7c','#665191','#a05195','#d45087','#f95d6a','#ff7c43','#ffa600']\n\ncolor_vals = ['#DAEEE2', '#A8D8BA', '#6FC59E', '#1EAB89']\nHEAL = ['#E6EAEA', '#58595B', '#272628', '#6FA4B5', '#F3723F', '#9C1C1F']\n\nBG_WHITE = \"#F3F1ED\"\nGREY_LIGHT = \"#b4aea9\"\nGREY50 = \"#7F7F7F\"\nBLUE_DARK = \"#1B2838\"\nBLUE = \"#2a475e\"\nBLACK = \"#282724\"\nGREY_DARK = \"#747473\"\nRED_DARK = \"#850e00\"\n\nBG = '#E6EAEA'","metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2022-11-28T16:01:54.511199Z","iopub.execute_input":"2022-11-28T16:01:54.511494Z","iopub.status.idle":"2022-11-28T16:01:54.529242Z","shell.execute_reply.started":"2022-11-28T16:01:54.511466Z","shell.execute_reply":"2022-11-28T16:01:54.527837Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<center>\n<a href=\"https://www.kaggle.com/pranav941\" rel=\"some text\"><img src=\"https://imgur.com/lXdsrSP.jpg\" width=\"1200\"/></a></center>","metadata":{}},{"cell_type":"markdown","source":"___\n# <div style=\"font-family: Trebuchet MS; background-color: #272628; color: #FFFFFF; padding: 20px; line-height: 1.5;\">Welcome to the Jungle!</div>  \n#### <div style=\"font-family: Trebuchet MS; background-color: #272628; color: #FFFFFF; padding: 20px; line-height: 1.5;\">When we talk about the fast paced evolution technology, We find that the jungle is cluttered with hundreds of tools, From low-code to no-code, Spreadsheets to Frames and from Local to cloud</div>  \n<div style=\"font-family: Trebuchet MS; background-color: #FFFFFF; color: #000000; padding: 12px; line-height: 1.5;\">As day passes by, More people than ever are trying to learn a sub field of Data, May it be a student or a working professional making a career change. One of the biggest issue faced is, Finding the right tool for right job, which is in the case for both sides of the coin i.e The Learner and The Working Professional<br>We will try to leverage the Industry Trend to strategically plan our path through the Jungle and choose whats right for us, Having the right tool would pay off in the long run</div>    ","metadata":{}},{"cell_type":"markdown","source":"___","metadata":{}},{"cell_type":"code","source":"# Plot on Google_Trends\n\ntrend_da = trend[['Month','Data_Analyst']]\ntrend_da.rename({'Data_Analyst':'value'}, axis=1, inplace=True)\ntrend_da['Element'] = 'Data_Analyst'\n\ntrend_ds = trend[['Month','Data_Scientist']]\ntrend_ds.rename({'Data_Scientist':'value'}, axis=1, inplace=True)\ntrend_ds['Element'] = 'Data_Scientist'\n\nquick_pivot = pd.concat([trend_da,trend_ds], ignore_index=True)\nquick_pivot.Month = pd.to_datetime(quick_pivot.Month) # Year, Month, Day\nquick_pivot.value = pd.to_numeric(quick_pivot.value)","metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2022-11-28T16:01:54.531389Z","iopub.execute_input":"2022-11-28T16:01:54.531815Z","iopub.status.idle":"2022-11-28T16:01:54.55381Z","shell.execute_reply.started":"2022-11-28T16:01:54.531779Z","shell.execute_reply":"2022-11-28T16:01:54.552218Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"fig,ax = plt.subplots(figsize = (16,4), dpi=90)\nfig.set_facecolor(BG)\nax.set_facecolor(BG)\n\nsns.lineplot(data=quick_pivot, x=quick_pivot.Month.dt.year, y='value', hue='Element', ci=None, lw=2, marker='o', ax=ax,)\n\nax.set_ylim(0,100)\nax.set_xlim(ax.get_xticks().min()+0.5)\n\n# ax.spines.left.set_visible(False)\nax.spines.right.set_visible(False)\nax.spines.top.set_visible(False)\n\n# de-clutter from obvious information\n\nax.set_xlabel('')\nax.set_ylabel('Interest over time')\n\nplt.suptitle('Now more than ever, people are looking up\\n\"How to become a Data Analyst\" or Scientist', fontsize=15, fontname='Bahnschrift', weight='bold', color=HEAL[2])\nplt.title('', fontsize=25, fontname='Bahnschrift', weight='bold', loc='center', color=HEAL[2])\n\n# x_ticks and y_ticks are more often than not, Not of the same size,\n# For that reason we cannot zip them  & add lines individually\n\nfor x in ax.get_yticks():\n    ax.axhline(x, lw=1, color='grey', alpha=0.1, zorder=0)\nfor x in ax.get_xticks():\n    ax.axvline(x, lw=1, color='grey', alpha=0.1, zorder=0)\n\nax.add_patch(\n    Rectangle(\n    (2021,0), 2, 100, facecolor='none', hatch='//', edgecolor='grey', lw= 0.5, zorder=0, alpha=0.7,\n    )\n)\nax.text(\n    2019.5,60, 'We are here\\nHighest ever!', fontsize=12, ha='left',\n)\nax.annotate('', xy=(2020.3, 60), xytext=(2021.7, 80),\n            arrowprops=dict(arrowstyle=\"<-\"))\n\nplt.figtext(0.1, 0, 'source: Google Trends, 2022', wrap=True, horizontalalignment='left', fontsize=8, style='oblique')\n\nax.legend(labels=['Data Analyst','Data Scientist'])\nax.legend(loc='upper left', facecolor=BG)","metadata":{"_kg_hide-input":true,"jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2022-11-28T16:01:54.555107Z","iopub.execute_input":"2022-11-28T16:01:54.555452Z","iopub.status.idle":"2022-11-28T16:01:54.960852Z","shell.execute_reply.started":"2022-11-28T16:01:54.555423Z","shell.execute_reply":"2022-11-28T16:01:54.959682Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"___","metadata":{}},{"cell_type":"markdown","source":"<center>\n<a href=\"https://hbr.org/2012/10/data-scientist-the-sexiest-job-of-the-21st-century\" rel=\"some text\"><img src=\"https://imgur.com/9fhg4wC.jpg\" width=\"800\"/></a></center>","metadata":{}},{"cell_type":"markdown","source":"___","metadata":{}},{"cell_type":"markdown","source":"### <div style=\"font-family: Trebuchet MS; background-color: #272628; color: #FFFFFF; padding: 20px; line-height: 1.5;\">Are companies seeking more Analysts or is it Kaggle only?</div>  \n#### <div style=\"font-family: Trebuchet MS; background-color: #ffffff; color: #000000; padding: 12px; line-height: 1.5;\">Data in the 21st Century is like Oil in the 18th Century: an immensely, untapped valuable asset. Like oil, for those who see Data’s fundamental value and learn to extract and use it there will be huge rewards.<br>We’re in a digital economy where data is more valuable than ever. It’s the key to the smooth functionality of everything from the government to local companies. Without it, progress would halt.</div>  \n<div style=\"font-family: Trebuchet MS; background-color: #6FA4B5; color: #FFFFFF; padding: 10px; line-height: 1;\">-Wired[4]</div>  \n  \n<br>\nIn the long term, it would probably be unwise to bet against data science as career move, especially when you widen the field to include related positions like research engineers and machine learning engineers. <b>The U.S. Bureau of Labor Statistics sees strong growth for data science jobs skills in its prediction that the data science field will grow about 28% through 2026</b>. Also, as technology improves, companies have been able to increase the sophistication of their data operations and analysis. Increasingly, that means inserting artificial intelligence (AI) capabilities into the business processes of regular companies (i.e. non-tech giants). And that means demand for data scientists (average salary in USA $111,100) and related positions (research scientists and machine learning engineer will also go up. While the tools are getting better, data scientists looking to accel in the marketplace will still need to have a solid understanding of the basics, including data modeling, relational databases, and basic statistical analysis. Those are critical skills that are likely to survive any future shifts in data science job functions.<br>\n<b>-Bernhard Schroeder, Forbes</b>[9]","metadata":{}},{"cell_type":"markdown","source":"___","metadata":{}},{"cell_type":"code","source":"# Let me know in the comments if you know 'How to embed Google Trends result' here\n\n# from IPython.display import Javascript\n# #runs arbitrary javascript, client-side\n# Javascript(\"\"\"\n#            <script type=\"text/javascript\" src=\"https://ssl.gstatic.com/trends_nrtr/3140_RC01/embed_loader.js\"></script> <script type=\"text/javascript\"> trends.embed.renderExploreWidget(\"TIMESERIES\", {\"comparisonItem\":[{\"keyword\":\"Data Science\",\"geo\":\"\",\"time\":\"2004-01-01 2022-11-20\"},{\"keyword\":\"Machine Learning\",\"geo\":\"\",\"time\":\"2004-01-01 2022-11-20\"},{\"keyword\":\"Data Visualization\",\"geo\":\"\",\"time\":\"2004-01-01 2022-11-20\"},{\"keyword\":\"Artificial Intelligence\",\"geo\":\"\",\"time\":\"2004-01-01 2022-11-20\"},{\"keyword\":\"Deep Learning\",\"geo\":\"\",\"time\":\"2004-01-01 2022-11-20\"}],\"category\":0,\"property\":\"\"}, {\"exploreQuery\":\"date=all&q=Data%20Science,Machine%20Learning,Data%20Visualization,Artificial%20Intelligence,Deep%20Learning\",\"guestPath\":\"https://trends.google.co.in:443/trends/embed/\"}); </script>\n#            \"\"\")\n\n# Try #4\n# %%js\n# <script type=\"text/javascript\" src=\"https://ssl.gstatic.com/trends_nrtr/3140_RC01/embed_loader.js\">\n#     </script>\n# <script type=\"text/javascript\">\n#     trends.embed.renderExploreWidget(\"TIMESERIES\", {\"comparisonItem\":[{\"keyword\":\"/m/0fp_8fm\",\"geo\":\"IN\",\"time\":\"today 12-m\"}],\"category\":0,\"property\":\"\"}, {\"exploreQuery\":\"q=%2Fm%2F0fp_8fm&geo=IN&date=today 12-m\",\"guestPath\":\"https://trends.google.com:443/trends/embed/\"});\n# </script>","metadata":{"_kg_hide-output":true,"_kg_hide-input":true,"jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2022-11-28T16:01:54.962167Z","iopub.execute_input":"2022-11-28T16:01:54.962927Z","iopub.status.idle":"2022-11-28T16:01:54.966603Z","shell.execute_reply.started":"2022-11-28T16:01:54.962896Z","shell.execute_reply":"2022-11-28T16:01:54.965922Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"demo = fetch_ts([df_20, df_21, df_22], 27, year=['2020','2021', '2022'])\ndemo = demo.groupby([demo.Year,demo.Element]).agg({'Percent_of_respondents':'sum'}).reset_index().sort_values(by='Percent_of_respondents', ascending=False)\ndemo_d = demo[demo.Element.str.contains(pat='Data')]","metadata":{"_kg_hide-input":true,"jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2022-11-28T16:01:54.967732Z","iopub.execute_input":"2022-11-28T16:01:54.968402Z","iopub.status.idle":"2022-11-28T16:01:55.016718Z","shell.execute_reply.started":"2022-11-28T16:01:54.968375Z","shell.execute_reply":"2022-11-28T16:01:55.015587Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"demo_d['Role'] = demo_d.apply(lambda row: give_me_role(row), axis=1)\ndemo_d = demo_d.groupby(['Year','Role'])['Percent_of_respondents'].sum().reset_index()\ndemo_d['Year_Sum'] = demo_d.apply(lambda row: share_of_year(row, demo_d), axis=1)","metadata":{"_kg_hide-input":true,"jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2022-11-28T16:01:55.018165Z","iopub.execute_input":"2022-11-28T16:01:55.018691Z","iopub.status.idle":"2022-11-28T16:01:55.035959Z","shell.execute_reply.started":"2022-11-28T16:01:55.018659Z","shell.execute_reply":"2022-11-28T16:01:55.035076Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"category_names = demo_d['Role'].unique()\n\ndistribution_values = {}\n# For every Year -> For every Role's value\nfor year in demo_d['Year'].unique():\n    temp_list = []\n    for role in demo_d['Role'].unique():\n        temp_list.append(\n            demo_d[(demo_d['Year']==year) & (demo_d['Role']==role)].Year_Sum.values[0]\n        )\n    distribution_values[year] = temp_list","metadata":{"_kg_hide-input":true,"jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2022-11-28T16:01:55.037017Z","iopub.execute_input":"2022-11-28T16:01:55.037286Z","iopub.status.idle":"2022-11-28T16:01:55.056835Z","shell.execute_reply.started":"2022-11-28T16:01:55.037263Z","shell.execute_reply":"2022-11-28T16:01:55.055953Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"fig, ax = plt.subplots(figsize = (16,4), dpi=90)\nfig.set_facecolor(BG)\nax.set_facecolor(BG)\ndiscrete_distribution_horizontal(distribution_values, category_names, cmap='crest')\n\n# plt.suptitle('Data Analysts have a highet growth Rate', fontsize=15, fontname='Bahnschrift', weight='bold', color=HEAL[2])\nplt.title('Number of Analysts are growing by ~3.5% YoY, where as\\nNumber of Data Scientists are decreasing by 3.5% YoY', fontsize=12, fontname='Bahnschrift', weight='bold', loc='left', color=HEAL[2])\nplt.figtext(0.1, 0.05, 'source: Kaggle Survey (2020-22)', wrap=True, horizontalalignment='left', fontsize=7, style='oblique')\n\nax.legend(loc='upper right', facecolor=BG, ncol=3)\nplt.show()","metadata":{"_kg_hide-input":true,"jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2022-11-28T16:01:55.057994Z","iopub.execute_input":"2022-11-28T16:01:55.0588Z","iopub.status.idle":"2022-11-28T16:01:55.255332Z","shell.execute_reply.started":"2022-11-28T16:01:55.058768Z","shell.execute_reply":"2022-11-28T16:01:55.254193Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"___","metadata":{}},{"cell_type":"markdown","source":"<center>\n<a href=\"https://hbr.org/2012/10/data-scientist-the-sexiest-job-of-the-21st-century\" rel=\"some text\"><img src=\"https://imgur.com/oKJQX89.jpg\" width=\"1100\"/></a></center>","metadata":{}},{"cell_type":"markdown","source":"___","metadata":{}},{"cell_type":"markdown","source":"### <div style=\"font-family: Trebuchet MS; background-color: #272628; color: #FFFFFF; padding: 20px; line-height: 1.5;\">Fast Forward from 2020 to 2022, Effect of Post Covid</div>  \n\n<div style=\"font-family: Trebuchet MS; background-color: #ffffff; color: #000000; padding: 12px; line-height: 1.5;\"><b>\"COVID-19 is upending data analytics practices, sidelining predictive analytics, and driving firms to external data and other economic indicators.\"</b><br>- MIT.edu<br>A representative also mentioned, <b>\"if the past is no longer a guide to the future, we’re going to have a tough time doing any sort of predictive analytics.”</b><br>For that reason, Firms are reverting to descriptive analytics and pausing machine learning, That could be one possible explanation for the increase in demand for Data analysts.<br><b>\"The simplest predictive model is what happened yesterday\"</b><br>- MIT.edu representative [10]</div>","metadata":{}},{"cell_type":"code","source":"# Use of ML vs No / Do not know Use of ML over the years, The gap is of concern is'nt it\n# Good for me, im no champ when it comes to ds :( or :-)\n\nuse_of_ml = fetch_ts([df_18, df_19, df_20, df_21, df_22], 31, ['2018', '2019', '2020','2021', '2022'], merge=False, append_year=False)\n\nreported_use_of_ml = use_of_ml[~use_of_ml.Element.isin(['No (we do not use ML methods)','I do not know'])]\nreported_use_of_ml['Element'] = 'Incorporate ML at some level'\nno_use_of_ml = use_of_ml[use_of_ml.Element.isin(['No (we do not use ML methods)','I do not know'])]\nno_use_of_ml['Element'] = 'Do not use or know'\n\nreported_use_of_ml = reported_use_of_ml.groupby([reported_use_of_ml.Year, reported_use_of_ml.Element]).agg({'Percent_of_respondents':'mean'}).reset_index()\nno_use_of_ml = no_use_of_ml.groupby([ no_use_of_ml.Year, no_use_of_ml.Element]).agg({'Percent_of_respondents':'mean'}).reset_index()","metadata":{"_kg_hide-input":true,"jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2022-11-28T16:01:55.256855Z","iopub.execute_input":"2022-11-28T16:01:55.257713Z","iopub.status.idle":"2022-11-28T16:01:55.313943Z","shell.execute_reply.started":"2022-11-28T16:01:55.257669Z","shell.execute_reply":"2022-11-28T16:01:55.312485Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"fig, ax = plt.subplots(figsize=(16,5), dpi=90)\nfig.set_facecolor(BG)\nax.set_facecolor(BG)\n\nax.set_xlim(0,4) # Edge to edge fill\nax.set_ylim(0,20)\n\n_ = plt.fill_between(no_use_of_ml.Year, no_use_of_ml.Percent_of_respondents, color=CMAP_PALLET[4], zorder=1, edgecolor='black', lw=1)\n_ = plt.fill_between(reported_use_of_ml.Year, reported_use_of_ml.Percent_of_respondents, color=STA_HEATMAP[3], alpha = 1, zorder=2, edgecolor='black', lw=1)\n\nax.spines.right.set_visible(False)\nax.spines.top.set_visible(False)\nax.yaxis.set_major_formatter(mtick.PercentFormatter())\n\nax.add_patch(\n        Rectangle(\n            (1.5,0), 2.5, 20, facecolor=BG, zorder=0, hatch='//', edgecolor='black', lw=1, alpha=0.1,\n        ) )\n\nplt.figtext(0.1, 0, 'source: Kaggle Survey (2018-22)', wrap=True, horizontalalignment='left', fontsize=9, style='oblique')\nplt.figtext(0.1, 1,'Did Covid-19 kill the Data Scientist Hype?\\nOr a switch to low code-No code were made? Whats in it for you', fontsize=17, fontname='Bahnschrift', weight='bold', color=HEAL[2])\n\nax.text(2.4, 15, 'People reporting \"No use of ML\" increased by ~%50 YoY,\\nA change in trend occurs?', color=HEAL[2], fontsize=11, weight='light')\n\n# Adapted from > https://matplotlib.org/stable/gallery/text_labels_and_annotations/custom_legends.html\nfrom matplotlib.patches import Patch\nlegend_elements = [Patch(facecolor=CMAP_PALLET[4], label='Reported - No usage of ML / Did not know'),\n                Patch(facecolor=STA_HEATMAP[3], label='Reported - Usage of ML')]\nax.legend(handles=legend_elements, loc='upper left')\n\nfor x in ax.get_xticks():\n    ax.axvline(x, lw=1, color='grey', alpha=0.5, zorder=0)\n\nax.hlines(5.5, 1.5, 4, lw=1, color='black', ls='--')\nax.hlines(11.7, 1.5, 4, lw=1, color='black', ls='--')\nax.vlines(1.5, 5.5, 11.7, lw=1, color='black', ls='--')\nax.vlines(3, 11.7, 15, lw=1, color='black', ls='--')","metadata":{"_kg_hide-input":true,"jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2022-11-28T16:01:55.315901Z","iopub.execute_input":"2022-11-28T16:01:55.316367Z","iopub.status.idle":"2022-11-28T16:01:55.589834Z","shell.execute_reply.started":"2022-11-28T16:01:55.316329Z","shell.execute_reply":"2022-11-28T16:01:55.588234Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"___","metadata":{}},{"cell_type":"code","source":"use_of_ml = fetch_ts_by_category(frames=[df_18, df_19, df_20, df_21, df_22], year=['2018', '2019', '2020','2021', '2022'], cat_query=3, ele_query=31)\n\n# remove 'Other' country\nuse_of_ml = use_of_ml[~(use_of_ml['Category'] == 'Other')]\nuse_of_ml = use_of_ml[~(use_of_ml['Category'] == 'I do not wish to disclose my location')]\n\nuse_of_ml.Category.replace(['Iran, Islamic Republic of...','United Kingdom of Great Britain and Northern Ireland','Hong Kong (S.A.R.)','Republic of Korea'], ['Iran','United Kingdom','Hong Kong','South Korea'], inplace=True)\n\nuse_of_ml['Continent'] = use_of_ml.apply(convert, axis=1)\nuse_of_ml['Continent'] = use_of_ml['Continent'].map(conti_names)","metadata":{"_kg_hide-input":true,"jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2022-11-28T16:01:55.591931Z","iopub.execute_input":"2022-11-28T16:01:55.592346Z","iopub.status.idle":"2022-11-28T16:02:01.986955Z","shell.execute_reply.started":"2022-11-28T16:01:55.592317Z","shell.execute_reply":"2022-11-28T16:02:01.985301Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"use_ml = use_of_ml[~use_of_ml.Element.isin(['No (we do not use ML methods)','I do not know'])]\nuse_ml['Element'] = 'Incorporate ML at some level'\ndont_use_ml = use_of_ml[use_of_ml.Element.isin(['No (we do not use ML methods)','I do not know'])]\ndont_use_ml['Element'] = 'Do not use or know'","metadata":{"_kg_hide-input":true,"jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2022-11-28T16:02:01.988545Z","iopub.execute_input":"2022-11-28T16:02:01.989205Z","iopub.status.idle":"2022-11-28T16:02:01.998089Z","shell.execute_reply.started":"2022-11-28T16:02:01.989174Z","shell.execute_reply":"2022-11-28T16:02:01.996787Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"use_ml = use_ml.groupby([use_ml.Continent, use_ml.Element]).agg({'Percent_of_respondents':'mean'}).reset_index() #'Value':'sum', 'Percent_of_respondents':'sum'\ndont_use_ml = dont_use_ml.groupby([ dont_use_ml.Continent, dont_use_ml.Element]).agg({'Percent_of_respondents':'mean'}).reset_index() # 'Value':'sum', 'Percent_of_respondents':'sum'\n\nuse_ml['change'] = use_ml['Percent_of_respondents'].pct_change()\nuse_ml.fillna(0, inplace=True)\nuse_ml.rename({'Percent_of_respondents':'Value'}, axis=1, inplace=True)\ndont_use_ml['change'] = dont_use_ml['Percent_of_respondents'].pct_change()\ndont_use_ml.fillna(0, inplace=True)\ndont_use_ml.rename({'Percent_of_respondents':'Value'}, axis=1, inplace=True)","metadata":{"_kg_hide-input":true,"jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2022-11-28T16:02:02.004687Z","iopub.execute_input":"2022-11-28T16:02:02.005225Z","iopub.status.idle":"2022-11-28T16:02:02.028358Z","shell.execute_reply.started":"2022-11-28T16:02:02.005185Z","shell.execute_reply":"2022-11-28T16:02:02.026577Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"fig,ax = plt.subplots(tight_layout=True , figsize=(16,5), dpi=72)\nfig.set_facecolor(BG)\n\n# Top plot\nax.set_facecolor(BG)\nwc_group([use_ml, dont_use_ml], len(use_ml), 50, 100, x_tick_to_set=use_ml.Continent, ax=ax, bar_color = CMAP_PALLET[1])\n\n# ax.spines.left.set_visible(False)\nax.spines.right.set_visible(False)\nax.spines.top.set_visible(False)\n# ax.get_yaxis().set_visible(False)\n\n## Annotations\nax.add_patch(\n    Rectangle(\n    (0.5,0),2,25, facecolor=GREY_LIGHT, zorder=-1, alpha = 0.3, hatch='//', edgecolor='grey', lw=1,\n    )\n)\n\nplt.suptitle(\"Even tho' use of machine Learning increased in the west,\\nYoY decrease outnumbered it\", fontsize=20, fontname='Bahnschrift', weight='bold', color=HEAL[2])\nplt.title('', fontsize=25, fontname='Bahnschrift', weight='bold', loc='center', color=HEAL[2])\n\nax.text(0.75,32, 'Asia and Africa Grew the least,', fontsize=12)\nax.hlines(30, 0.5, 2.5, lw=1, color='black')\nax.vlines(0.5, 28.5, 30, lw=1, color='black')\nax.vlines(2.5, 28.5, 30, lw=1, color='black')\n\nfor x in ax.get_yticks():\n    ax.axhline(x, lw=1, color='grey', alpha=0.1, zorder=0)\n\nax.annotate('', xy=(7, 0), xytext=(7, 40),\n            arrowprops=dict(arrowstyle=\"<-\", color='white'))\n\nax.text(6.8,42,'avg: 12%', color='white')\nplt.figtext(0., 0, 'source: Kaggle Survey (2018 to 2022)', wrap=True, horizontalalignment='left', fontsize=8, style='oblique')\n\nax.text(4.5,20, 'Oceania saw the highest Growth', fontsize=12)\nax.text(4.6,35, '~20%', color='white', fontsize=12, weight='heavy')\nax.hlines(27, 4.6, 5.5, lw=1, color='black')\nax.vlines(4.6, 27, 29, lw=1, color='black')\nax.vlines(5.5, 27, 29, lw=1, color='black')\n\nlegend_elements = [Patch(facecolor=CMAP_PALLET[1], label='Average Increase in usage of ML'),\n                Patch(facecolor='none', hatch='///', edgecolor='black', lw=1,  label='Average Decrease in usage of ML')]\nax.legend(handles=legend_elements, loc='upper left', facecolor=BG)","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-11-28T16:02:02.030286Z","iopub.execute_input":"2022-11-28T16:02:02.031Z","iopub.status.idle":"2022-11-28T16:02:02.357031Z","shell.execute_reply.started":"2022-11-28T16:02:02.030965Z","shell.execute_reply":"2022-11-28T16:02:02.355204Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Introduction to layoffs\n\nlayoffs['date'] = pd.to_datetime(layoffs['date'])\nlayoffs['year'] = layoffs['date'].dt.year","metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2022-11-28T16:02:02.359038Z","iopub.execute_input":"2022-11-28T16:02:02.359734Z","iopub.status.idle":"2022-11-28T16:02:02.374066Z","shell.execute_reply.started":"2022-11-28T16:02:02.359674Z","shell.execute_reply":"2022-11-28T16:02:02.371737Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"layoff_stage = layoffs.groupby(layoffs.stage).agg({'percentage_laid_off':'sum'}).reset_index().sort_values(by='percentage_laid_off', ascending=False)\nlayoff_stage['contribution'] = ( layoff_stage['percentage_laid_off'] / layoff_stage['percentage_laid_off'].sum() ) * 100","metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2022-11-28T16:02:02.375739Z","iopub.execute_input":"2022-11-28T16:02:02.376078Z","iopub.status.idle":"2022-11-28T16:02:02.390777Z","shell.execute_reply.started":"2022-11-28T16:02:02.376047Z","shell.execute_reply":"2022-11-28T16:02:02.388479Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"layoff_country = layoffs.groupby(layoffs.country).agg({'percentage_laid_off':'sum'}).reset_index().sort_values(by='percentage_laid_off', ascending=False)\nlayoff_country['contribution'] = ( layoff_country['percentage_laid_off'] / layoff_country['percentage_laid_off'].sum() ) * 100","metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2022-11-28T16:02:02.394758Z","iopub.execute_input":"2022-11-28T16:02:02.395209Z","iopub.status.idle":"2022-11-28T16:02:02.417235Z","shell.execute_reply.started":"2022-11-28T16:02:02.395173Z","shell.execute_reply":"2022-11-28T16:02:02.415721Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"lay_off_agg = layoffs.groupby([layoffs.year, layoffs.industry]).agg({'percentage_laid_off':'sum'}).sort_values(by='percentage_laid_off', ascending=False).reset_index()\nlay_off_agg['contribution'] = ( lay_off_agg['percentage_laid_off'] / lay_off_agg['percentage_laid_off'].sum() ) * 100\n# lay_off_agg.groupby(lay_off_agg.industry).agg({'contribution':'sum'}).reset_index().sort_values(by='contribution', ascending=False).contribution[:12].sum(0)\n# -> Out of 26 industries, These 10 contribute to ~75% of the lay-off's","metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2022-11-28T16:02:02.418761Z","iopub.execute_input":"2022-11-28T16:02:02.420229Z","iopub.status.idle":"2022-11-28T16:02:02.437747Z","shell.execute_reply.started":"2022-11-28T16:02:02.420166Z","shell.execute_reply":"2022-11-28T16:02:02.435794Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"y_total = lay_off_agg.groupby(lay_off_agg.year)['contribution'].sum().reset_index()\ny_total['industry'] = 'Total'\n\nx_total = lay_off_agg.groupby(lay_off_agg.industry)['contribution'].sum().reset_index()\nx_total['year'] = 'Total'","metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2022-11-28T16:02:02.439806Z","iopub.execute_input":"2022-11-28T16:02:02.440313Z","iopub.status.idle":"2022-11-28T16:02:02.454374Z","shell.execute_reply.started":"2022-11-28T16:02:02.440259Z","shell.execute_reply":"2022-11-28T16:02:02.452402Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"layoff_merged = pd.concat([y_total, lay_off_agg, x_total], ignore_index=True)\nlayoff_merged = layoff_merged.pivot_table(values='contribution', index='industry', columns='year')\nlayoff_merged.fillna(0, inplace=True)\nlayoff_merged.sort_index(level=0, ascending=True, inplace=True)","metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2022-11-28T16:02:02.456282Z","iopub.execute_input":"2022-11-28T16:02:02.456745Z","iopub.status.idle":"2022-11-28T16:02:02.485488Z","shell.execute_reply.started":"2022-11-28T16:02:02.456706Z","shell.execute_reply":"2022-11-28T16:02:02.48382Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"layoff_merged.index = pd.CategoricalIndex(layoff_merged.index, categories =\n    lay_off_agg.groupby(lay_off_agg.industry).agg({'percentage_laid_off':'sum'}).reset_index().sort_values(by='percentage_laid_off', ascending=False)['industry'].unique()\n    )\nlayoff_merged.sort_index(level=0, inplace=True)\n\nlayoff_merged.loc[layoff_merged.index[-1], 'Total'] = 99","metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2022-11-28T16:02:02.486738Z","iopub.execute_input":"2022-11-28T16:02:02.487821Z","iopub.status.idle":"2022-11-28T16:02:02.504179Z","shell.execute_reply.started":"2022-11-28T16:02:02.487765Z","shell.execute_reply":"2022-11-28T16:02:02.501992Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"x_total.loc[len(x_total.index)] = ['Total', 0, 0]\nx_total.sort_values(by='contribution', ascending=False, inplace=True)","metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2022-11-28T16:02:02.506245Z","iopub.execute_input":"2022-11-28T16:02:02.506704Z","iopub.status.idle":"2022-11-28T16:02:02.518258Z","shell.execute_reply.started":"2022-11-28T16:02:02.506667Z","shell.execute_reply":"2022-11-28T16:02:02.517434Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"fig,ax = plt.subplots(figsize=(16,16), dpi=72)\n\nfig.set_facecolor('none')\nax.set_facecolor('none')\n\nsns.heatmap(layoff_merged, cmap=HEAL, annot=True, linewidths=1, linecolor='white', cbar=False, vmax=6, ax=ax)\n\nax.set_title('Finance, Retail, Healthcare Industry led the layoffs head-on YoY\\nLucky for us, Data contributes ~2.4%\\n', fontsize=15, fontname='Bahnschrift', weight='bold', color=HEAL[2], loc='left')\nplt.figtext(0.1, 0.1, 'source: Layoffs.fyi, 2022, updated daily', wrap=True, horizontalalignment='left', fontsize=12, style='oblique')\n\nfor t in ax.texts: t.set_text(t.get_text() + \" %\")\nlabels = [item.get_text() for item in ax.get_yticklabels()]\nlabels[-1] = 'Total'\nax.set_yticklabels(labels)\n\nax.set_xlabel(None)\nax.set_ylabel(None)\nax.xaxis.tick_top()","metadata":{"_kg_hide-input":true,"jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2022-11-28T16:02:02.51906Z","iopub.execute_input":"2022-11-28T16:02:02.519304Z","iopub.status.idle":"2022-11-28T16:02:03.44041Z","shell.execute_reply.started":"2022-11-28T16:02:02.519282Z","shell.execute_reply":"2022-11-28T16:02:03.438972Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"layoffs_by_country = layoffs.copy()\nlayoffs_by_country['country']  = layoffs_by_country['country'].apply(lambda x: 'United States of America' if x == 'United States' else 'Rest of the world')\nlayoffs_by_country = layoffs_by_country.groupby([layoffs_by_country['country']]).agg(counts=('country','count')).reset_index().sort_values(by='counts', ascending=False)","metadata":{"_kg_hide-input":true,"jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2022-11-28T16:02:03.442351Z","iopub.execute_input":"2022-11-28T16:02:03.442839Z","iopub.status.idle":"2022-11-28T16:02:03.462196Z","shell.execute_reply.started":"2022-11-28T16:02:03.442797Z","shell.execute_reply":"2022-11-28T16:02:03.461419Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"fig, ax = plt.subplots(1,2, figsize=(10,10), dpi=72)\nax[0].set_facecolor(BG)\nax[1].set_facecolor(BG)\nfig.set_facecolor(BG)\n\ns = sns.barplot(data=layoffs_by_country, x='country', y='counts', palette=[HEAL[4], HEAL[2]], edgecolor='black', lw=1)\nshow_values(s, space=0.13)\n\nax[1].spines.top.set_visible(False)\nax[1].spines.right.set_visible(False)\nax[1].set_xlabel('')\nax[1].set_ylabel('Number of Companies 2020 - 22')\n\nax[1].text(0.6,800, 'USA Layoffs 2x compared\\nto rest of the world', fontsize=10, ha='left', fontname='Bahnschrift', weight='bold', color=HEAL[2])\nax[1].vlines(0.55, 570, 1100, edgecolor='black', lw=1)\n\nax[1].hlines(570, 0.52, 0.55, edgecolor='black', lw=1)\nax[1].hlines(1100, 0.52, 0.55,edgecolor='black', lw=1)\n\n\nax[0].spines.top.set_visible(False)\nax[0].spines.bottom.set_visible(False)\nax[0].spines.right.set_visible(False)\nax[0].spines.left.set_visible(False)\nax[0].set_xlabel('')\nax[0].set_ylabel('')\n\nplt.figtext(0.1, 0.1, 'source: Layoffs.fyi, 2022, updated daily', wrap=True, horizontalalignment='left', fontsize=12, style='oblique')\nax[0].text(0.05, 0.75, 'Key Reasons for Layoffs', fontsize=19, ha='left', fontname='serif', weight='heavy', color=HEAL[2])\n\nax[0].text(0.1, 0.65, '1. Economic Uncertainty', fontsize=14, ha='left', fontname='sans-serif', weight='light', color=HEAL[2])\nax[0].text(0.1, 0.55, '2. Decrease in E-Commerce\\n   Activity', fontsize=14, ha='left', fontname='sans-serif', weight='light', color=HEAL[2])\nax[0].text(0.1, 0.50, '3. Pressure From Investors', fontsize=14, ha='left', fontname='sans-serif', weight='light', color=HEAL[2])\nax[0].text(0.1, 0.45, '4. Declining Revenue', fontsize=14, ha='left', fontname='sans-serif', weight='light', color=HEAL[2])\nax[0].text(0.1, 0.40, '5. Signs of a Mature Industry', fontsize=14, ha='left', fontname='sans-serif', weight='light', color=HEAL[2])\nax[0].text(0.1, 0.31, '6. Ingrained Mentality That\\nLayoffs Increase Profitability', fontsize=14, ha='left', fontname='sans-serif', weight='light', color=HEAL[2])\nax[0].get_xaxis().set_visible(False)\nax[0].get_yaxis().set_visible(False)","metadata":{"_kg_hide-input":true,"jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2022-11-28T16:02:03.463702Z","iopub.execute_input":"2022-11-28T16:02:03.464731Z","iopub.status.idle":"2022-11-28T16:02:03.723395Z","shell.execute_reply.started":"2022-11-28T16:02:03.464703Z","shell.execute_reply":"2022-11-28T16:02:03.721581Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"___\n### <div style=\"font-family: Trebuchet MS; background-color: #272628; color: #FFFFFF; padding: 20px; line-height: 1.5;\">From Tech Boom to Tech Gloom?</div>  \n\n<div style=\"font-family: Trebuchet MS; background-color: #FFFFFF; color: #000000; padding: 12px; line-height: 1.5;\">2021 was the safest year, but the futre does'nt look like. One of the biggest reasons of Layoffs looks to be \"Economic Uncertainty\",<br>An article from Makeuseof.com reads<br>\n    <blockquote cite=\"https://www.makeuseof.com/why-tech-companies-laying-off-employees/\">\n        <p>Unusual and Uncertain macroeconomic environment\" that led to its decision to lose \"talented Amazonians.\" The New York Times reports that Amazon has since pared back its workforce to almost 80,000 from April to September, with thousands more expected to be laid off in the coming days.<br>\n\nDebates about whether the U.S. is in a recession escalated after data from the U.S. Bureau of Economic Analysis in July 2022 showed a shrinking economy for the second straight quarter. You can't confidently say that the future looks rosy. The International Monetary Fund (IMF) states that economic conditions hang in a balance, depending on the course of the war in Ukraine, monetary policy, and the pandemic. You can conclude that layoffs are a company's way to survive uncertain times.</p>\n    </blockquote></div>","metadata":{}},{"cell_type":"markdown","source":"<center>\n<a href=\"https://www.kaggle.com/pranav941\" rel=\"Law of the Jungle\"><img src=\"https://imgur.com/Yp3cBnP.jpg\" width=\"1200\"/></a></center>","metadata":{}},{"cell_type":"markdown","source":"___","metadata":{}},{"cell_type":"code","source":"# First ask in 2022\nplatform_by_student = fetch_ts_by_category(frames=[df_22], year=['2022'], cat_query=4, ele_query=6)\n\nfig, ax = plt.subplots(figsize=(10,9))\nax.set_facecolor(BG)\nfig.set_facecolor(BG)\n\ns = sns.barplot(data=platform_by_student, x='Element', y='Percent_of_respondents', hue='Category', palette=[HEAL[2], HEAL[4]], edgecolor='black', lw=1)\n# show_values(s, space=0.13)\n\nlabels = ['Online Courses', 'Kaggle', 'Video Platforms', 'University Courses', 'Social Media', 'Other', 'None/I do not study']\nax.set_xticklabels(labels)\n\nlegend_vals=plt.legend()\nlegend_vals.get_texts()[0].set_text('Not a Student')\nlegend_vals.get_texts()[0].set_text('Student')\n\nplt.xticks(rotation=90)\nax.spines.top.set_visible(False)\nax.spines.right.set_visible(False)\nax.set_xlabel('')\nax.set_ylabel('')\n\nplt.title('Student or not, Everyone accepts the need\\nto keep up with Evolution', loc='left', fontsize=14, ha='left', fontname='Bahnschrift', weight='bold', color=HEAL[2])","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-11-28T16:06:24.887726Z","iopub.execute_input":"2022-11-28T16:06:24.888111Z","iopub.status.idle":"2022-11-28T16:06:25.266725Z","shell.execute_reply.started":"2022-11-28T16:06:24.888077Z","shell.execute_reply":"2022-11-28T16:06:25.265323Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### <div style=\"font-family: Trebuchet MS; background-color: #272628; color: #FFFFFF; padding: 20px; line-height: 1.5;\">The Bare minimum to step foot into the Jungle</div>  \n<div style=\"font-family: Trebuchet MS; background-color: #6FA4B5; color: #FFFFFF; padding: 12px; line-height: 1.5;\">As said by the hundreds of articles and thousands of professionals, The stepping stone in the field of Data is-<br><b>A. Learn a Data Manipulation Language<br>B. Learn a way to analyze and communicate insights</b><br><br>We'll have a quick overview on few thousands of Job Descriptions (approx 2000) which have been scrapped from the web with love for you, we'll try to figure out which crucial deficit can we fill in to fight upcoming times of uncertainity</div>","metadata":{}},{"cell_type":"markdown","source":"<center>\n<a href=\"https://www.kaggle.com/pranav941\" rel=\"some text\"><img src=\"https://imgur.com/jI94ftD.jpg\" width=\"900\"/></a></center>","metadata":{}},{"cell_type":"markdown","source":"___","metadata":{}},{"cell_type":"code","source":"# Supply vs Demand\nkw_pgm = pd.read_csv('/kaggle/input/data-analyst-skill-analysis/kw_programming.csv', usecols=['keywords', 'counts', 'percentage'])\nkw_pgm['val'] = 'Chances of beign in Job Description'\n\npgm = fetch_ts([df_22], 12, year=['2022'], merge=False, append_year=False)","metadata":{"_kg_hide-input":true,"jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2022-11-28T16:02:04.126056Z","iopub.execute_input":"2022-11-28T16:02:04.127235Z","iopub.status.idle":"2022-11-28T16:02:04.184182Z","shell.execute_reply.started":"2022-11-28T16:02:04.127098Z","shell.execute_reply":"2022-11-28T16:02:04.182379Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Programming Languages\npgm = fetch_ts([df_22], 12, year=['2022'], merge=False, append_year=False) #.groupby('Element')['Percent_of_respondents'].mean().reset_index().sort_values(by='Percent_of_respondents', ascending=False)\npgm_rec = fetch_ts([df_20, df_21], 13, year=['2020', '2021'], merge=False, append_year=False).groupby('Element')['Percent_of_respondents'].mean().reset_index().sort_values(by='Percent_of_respondents', ascending=False)\n\npgm['val'] = 'What kagglers use regularly'\npgm['lower'] = pgm['Element'].str.lower()\n\npgm_rec['val'] = 'What Kagglers recommend learning first'\npgm_rec['lower'] = pgm_rec['Element'].str.lower()","metadata":{"_kg_hide-input":true,"jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2022-11-28T16:02:04.186589Z","iopub.execute_input":"2022-11-28T16:02:04.187256Z","iopub.status.idle":"2022-11-28T16:02:04.249751Z","shell.execute_reply.started":"2022-11-28T16:02:04.187214Z","shell.execute_reply":"2022-11-28T16:02:04.247976Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"pgm = pgm[['Element','Percent_of_respondents','val','lower']]\n\npgc = pd.concat([pgm, pgm_rec], ignore_index=True)\nnew_pgm = pgc.merge(kw_pgm, left_on=\"lower\", right_on=\"keywords\", how=\"left\").dropna()","metadata":{"_kg_hide-input":true,"jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2022-11-28T16:02:04.251584Z","iopub.execute_input":"2022-11-28T16:02:04.251927Z","iopub.status.idle":"2022-11-28T16:02:04.272199Z","shell.execute_reply.started":"2022-11-28T16:02:04.251896Z","shell.execute_reply":"2022-11-28T16:02:04.270316Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"a = new_pgm[['Element', 'Percent_of_respondents', 'val_x']]\n\n# First set 'B' cause we will replace a with a\nb = a[a.val_x == 'What Kagglers recommend learning first']\na = a[a.val_x == 'What kagglers use regularly']\n\nc = new_pgm[['Element', 'percentage', 'val_y']]\nc.rename({'percentage':'Percent_of_respondents', 'val_y':'val_x'}, axis=1, inplace=True)\nc.drop_duplicates(inplace=True)","metadata":{"_kg_hide-input":true,"jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2022-11-28T16:02:04.275075Z","iopub.execute_input":"2022-11-28T16:02:04.275518Z","iopub.status.idle":"2022-11-28T16:02:04.291642Z","shell.execute_reply.started":"2022-11-28T16:02:04.275483Z","shell.execute_reply":"2022-11-28T16:02:04.290152Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"pgm_final = pd.concat([b,a,c], ignore_index=True)\n\n# Why top 3 ?\n# Cause average learner doesnt learn more than 2 / 3\n\ntop_5 = list(\n        pgm_final.groupby('Element')['Percent_of_respondents'].sum().reset_index().sort_values(by='Percent_of_respondents', ascending=False)['Element'][:3].values )\npgm_final = pgm_final[pgm_final['Element'].isin(top_5)]","metadata":{"_kg_hide-input":true,"jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2022-11-28T16:02:04.293701Z","iopub.execute_input":"2022-11-28T16:02:04.29531Z","iopub.status.idle":"2022-11-28T16:02:04.317866Z","shell.execute_reply.started":"2022-11-28T16:02:04.29525Z","shell.execute_reply":"2022-11-28T16:02:04.316749Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"kw_bi = pd.read_csv('/kaggle/input/data-analyst-skill-analysis/kw_bi.csv', usecols=['keywords', 'counts', 'percentage'])\nkw_bi.keywords.replace(['power_bi','tableau','looker'], ['Microsoft Power BI','Tableau','Looker'], inplace=True)\nkw_bi['val'] = 'Chances of beign in Job Description'\nkw_bi.rename({'keywords':'Element','percentage':'Percent_of_respondents'}, axis=1, inplace=True)","metadata":{"_kg_hide-input":true,"jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2022-11-28T16:02:04.31964Z","iopub.execute_input":"2022-11-28T16:02:04.320536Z","iopub.status.idle":"2022-11-28T16:02:04.350289Z","shell.execute_reply.started":"2022-11-28T16:02:04.320457Z","shell.execute_reply":"2022-11-28T16:02:04.34864Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# BI Tools [44, 45], Fetch2x\nuse_of_bi = fetch_ts([df_22], 45, year=['2022'], merge=False, append_year=False)","metadata":{"_kg_hide-input":true,"jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2022-11-28T16:02:04.352058Z","iopub.execute_input":"2022-11-28T16:02:04.352546Z","iopub.status.idle":"2022-11-28T16:02:04.40194Z","shell.execute_reply.started":"2022-11-28T16:02:04.352505Z","shell.execute_reply":"2022-11-28T16:02:04.400383Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"use_of_bi['val'] = 'What kagglers use regularly'\ntemp_use_of_bi_22 = use_of_bi[ use_of_bi['Element'].isin(kw_bi.Element.unique()) ]\n\nuse_of_bi_final = pd.concat([temp_use_of_bi_22, kw_bi], ignore_index=True) # We need Element, PCT","metadata":{"_kg_hide-input":true,"jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2022-11-28T16:02:04.404331Z","iopub.execute_input":"2022-11-28T16:02:04.40484Z","iopub.status.idle":"2022-11-28T16:02:04.421548Z","shell.execute_reply.started":"2022-11-28T16:02:04.404797Z","shell.execute_reply":"2022-11-28T16:02:04.420073Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"fig, ax = plt.subplots(figsize = (16,9), dpi=72)\nfig.set_facecolor(BG)\n\ns = sns.barplot(data=pgm_final, x='Element', y='Percent_of_respondents', hue='val_x', palette=[STA_HEATMAP[0], HEAL[1], HEAL[4]], lw=1, edgecolor='black', ax=ax)\nshow_values(s, space=1)\n\nax.set_facecolor(BG)\nax.set_facecolor(BG)\n\nax.spines.right.set_visible(False)\nax.spines.top.set_visible(False)\n\nfor x, patch in enumerate(ax.patches) :\n        if x in [0,1,2]:\n            patch.set_color('None')\n            patch.set_capstyle('round')\n            patch.set_edgecolor('black')\n            patch.set_linewidth(1)\n\nax.set_xlabel('')\nax.set_ylabel('% of Opinion')\nax.set_ylim(0,100)\n\nax.yaxis.set_major_formatter(mtick.PercentFormatter())\nax.legend(title=False, facecolor=BG)\n\nax.annotate('45% higher', xy=(0.25, 35), xytext=(0.15, 74), arrowprops=dict(arrowstyle='<-['), zorder=3, color='green')\nax.annotate('50% lower', xy=(1.65, 10), xytext=(1.55, 50), arrowprops=dict(arrowstyle=']->'), zorder=3, color='red')\nplt.title('Is SQL being heavily under rated?\\nWhile Python is being over rated', fontsize=22, fontname='Bahnschrift', weight='bold', loc='center', color=HEAL[2])\n\nax.add_patch( Rectangle((0.5,0), 1,30, facecolor='none', hatch='/', edgecolor=HEAL[5], zorder=0, alpha=0.4)   )\nax.text( 0.5, 38, 'All the while \"R\" stays at a Sweet Spot', fontsize=13, color=HEAL[5], weight='light')\n\nax.hlines(35, 0.5, 1.5, lw=1, color='black')\nax.vlines(0.5, 33, 35, lw=1, color='black')\nax.vlines(1.5, 33, 35, lw=1, color='black')\nax.vlines(1, 35, 36, lw=1, color='black')\n\nplt.figtext(0.1, 0.06, 'source: Jobs on Google Search, Kaggle Survey (2020-22)', wrap=True, horizontalalignment='left', fontsize=9, style='oblique')\n\nax.patches[7].set_color(GREY_DARK)\nax.patches[7].set_edgecolor('black')\nax.patches[7].set_linewidth(1)","metadata":{"_kg_hide-input":true,"jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2022-11-28T16:02:04.423273Z","iopub.execute_input":"2022-11-28T16:02:04.424605Z","iopub.status.idle":"2022-11-28T16:02:04.801665Z","shell.execute_reply.started":"2022-11-28T16:02:04.424543Z","shell.execute_reply":"2022-11-28T16:02:04.800117Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### <div style=\"font-family: Trebuchet MS; background-color: #272628; color: #FFFFFF; padding: 20px; line-height: 1.5;\">Why is this difference so large?</div>  \n##### <div style=\"font-family: Trebuchet MS; background-color: #FFFFFF; color: #000000; padding: 12px; line-height: 1.5;\"><b>Kagglers use Python a lott!</b><br>It looks like if we head into an interview room together, and all we were asked about was Python in Data Science, We would ace it,<br>But can we say the same about SQL? It has a higher probability of being in a Job Description, In fact the most!<br> <b>Less than 10% of Kagglers recommend SQL to learn first,</b> Is it because <u>Python is easier to use and manipulate Dataframes? or Python does the job of Visualizing as well</u></div>  ","metadata":{}},{"cell_type":"markdown","source":"<center>\n<a href=\"https://www.youtube.com/watch?v=iNEwkaYmPqY\" rel=\"some text\"><img src=\"https://imgur.com/r0ASRS9.gif\" width=\"500\"/></a><br>cropped from Luke Barousse process explanation, more in references</center>","metadata":{}},{"cell_type":"markdown","source":"___","metadata":{}},{"cell_type":"code","source":"fig, ax = plt.subplots(figsize = (16,9), dpi=72)\nfig.set_facecolor(BG)\nax.set_facecolor(BG)\n\nb = sns.barplot(data=use_of_bi_final, x='Element', y='Percent_of_respondents', hue='val', palette=[HEAL[5], HEAL[4]], lw=1, edgecolor='black', ax=ax)\nshow_values(b, space=0.05)\nax.spines.right.set_visible(False)\nax.spines.top.set_visible(False)\n\nfor x, patch in enumerate(ax.patches) :\n        if x in [0,1,2]:\n            if x!= 1:\n                patch.set_color('None')\n            patch.set_capstyle('round')\n            patch.set_edgecolor('black')\n            patch.set_linewidth(1)\n\nax.set_xlabel('')\nax.set_ylabel('Kagglers who know the Tool')\nax.set_ylim(0,50)\n\nax.yaxis.set_major_formatter(mtick.PercentFormatter())\nax.legend(title=False, facecolor=BG)\n\n# ax.annotate('45%', xy=(0.25, 35), xytext=(0.215, 74), arrowprops=dict(arrowstyle=']-['), zorder=3)\n# ax.annotate('50%', xy=(1.75, 10), xytext=(1.72, 50), arrowprops=dict(arrowstyle=']-['), zorder=3)\nplt.suptitle('Only 1 out of 3 kagglers use a BI Tool', fontsize=25, fontname='Bahnschrift', weight='bold', color=HEAL[2])\nplt.title('', fontsize=25, fontname='Bahnschrift', weight='bold', loc='center', color=HEAL[2])\nplt.figtext(0.1, 0.06, 'source: Jobs on Google Search, Kaggle Survey (2020-22)', wrap=True, horizontalalignment='left', fontsize=9, style='oblique')\n\nax.add_patch( Rectangle((0.5,0), 1,30, facecolor='none', hatch='/', edgecolor=HEAL[2], zorder=0, alpha=0.4)   )\nax.text( 0.5, 38, 'Power BI leads by less than a Percent', fontsize=16, color=HEAL[5])\nax.text(1.15, 27, '30%', color='white', fontsize=15, weight='bold')\n\nax.hlines(35, 0.5, 1.5, lw=1, color='black')\nax.vlines(0.5, 33, 35, lw=1, color='black')\nax.vlines(1.5, 33, 35, lw=1, color='black')\nax.vlines(1, 35, 36, lw=1, color='black')\n\nfor x in [3,5]:\n    ax.patches[x].set_color(GREY_DARK)\n    ax.patches[x].set_edgecolor('black')\n    ax.patches[x].set_linewidth(1)","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-11-28T16:11:15.078905Z","iopub.execute_input":"2022-11-28T16:11:15.079384Z","iopub.status.idle":"2022-11-28T16:11:15.334687Z","shell.execute_reply.started":"2022-11-28T16:11:15.07935Z","shell.execute_reply":"2022-11-28T16:11:15.333367Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### <div style=\"font-family: Trebuchet MS; background-color: #272628; color: #FFFFFF; padding: 20px; line-height: 1.5;\">Would Kagglers ace low code - no code requirements</div>  \n#### <div style=\"font-family: Trebuchet MS; background-color: #FFFFFF; color: #000000; padding: 12px; line-height: 1.5;\">Low Code / No Code tools have been trend and is making it now more easier than ever for an individual to ride the tide of Data,<br>However its not recommended as much as Python or SQL. Only around 1 out of 10 Kagglers make use of BI Tools where as 3 out of 10 Jobs would require it.<br>Knowing a BI Tools would help you fill this Gap easily (if you dont already)</div>  ","metadata":{}},{"cell_type":"markdown","source":"___","metadata":{}},{"cell_type":"code","source":"# 2x Drop fail cus the row 'None' doesnt exist\ncc_platform = learning_curve([df_20, df_21], [35, 36], year=['2020','2021',])\ncc_Product = learning_curve([df_20, df_21], [38, 39], year=['2020','2021',])\ndata_storage = learning_curve([df_20, df_21], [40, 41], year=['2020','2021',])\nbig_data_products = learning_curve([df_20, df_21], [42, 43], year=['2020','2021',])\nbi_tools = learning_curve([df_20, df_21], [44, 45], year=['2020','2021',])","metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2022-11-28T16:02:05.058992Z","iopub.execute_input":"2022-11-28T16:02:05.060154Z","iopub.status.idle":"2022-11-28T16:02:05.881809Z","shell.execute_reply.started":"2022-11-28T16:02:05.060089Z","shell.execute_reply":"2022-11-28T16:02:05.879973Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<center>\n<a href=\"https://www.kaggle.com/pranav941\" rel=\"some text\"><img src=\"https://imgur.com/UPIZLX8.jpg\" width=\"500\"/></a><br>\n<a href=\"https://www.kaggle.com/code/iamleonie/head-in-the-clouds/notebook\" rel=\"some text\"><img src=\"https://imgur.com/kqv566G.jpg\" width=\"800\"/></a></center>","metadata":{}},{"cell_type":"code","source":"fig, (ax1, ax2, ax3, ax4) = plt.subplots(nrows=4, tight_layout=True, sharex=True, figsize=(16,9), dpi=72)\n\nsns.barplot(data=cc_platform, x='Value', y='Year', hue='Element', ax=ax1, palette=[HEAL[4], HEAL[0]], zorder=2,).set(title='Cloud Platforms')\nsns.barplot(data=cc_Product, x='Value', y='Year', hue='Element', ax=ax2, zorder=2).set(title='Cloud Products')\n# sns.barplot(data=data_storage, x='Value', y='Year', hue='Element', ax=ax3)\nsns.barplot(data=big_data_products, x='Value', y='Year', hue='Element', ax=ax3, zorder=2).set(title='Big Data Products')\nsns.barplot(data=bi_tools, x='Value', y='Year', hue='Element', ax=ax4, zorder=2).set(title='BI Tools')\n\nax1.set_xlim(-1,50)\n\naxes_list = [ax1, ax2, ax3, ax4]\n\n# Shrinking the Bars a bit, Helps the reader to diffrentiate a bit more easily\n# Adapted from\n#     https://stackoverflow.com/questions/34888058/changing-width-of-bars-in-bar-chart-created-using-seaborn-factorplot\n\nshrink_value = 0.3\nfor ax in axes_list:\n    for x, patch in enumerate(ax.patches) :\n        if x in [2,3]:\n            patch.set_color('None')\n            patch.set_capstyle('round')\n        if x == 1:\n            patch.set_color(HEAL[4])\n        if x == 0:\n             patch.set_color(HEAL[1])\n        patch.set_edgecolor('black')\n        patch.set_linewidth(1)\n        current_width = patch.get_height()\n        diff = current_width - shrink_value\n\n        patch.set_height(shrink_value)\n        patch.set_x(patch.get_x() + diff * .5)\n\nxmin = [30]#, 40]\nxmax = [50]#, 50]\n\n# Adapted from\n# https://stackoverflow.com/questions/62991535/how-to-draw-a-rectangular-on-subplotted-figure-using-matlibplot-in-python\n# Gets the job done asap\n\ntemp_color_val = [color_vals[1], color_vals[3]]\nfor i in range(0, len(xmin)): \n    min, max = xmin[i], xmax[i]\n    _,top = fig.transFigure.inverted().transform(ax1.transAxes.transform([0,1]))\n    _,bottom = fig.transFigure.inverted().transform(ax4.transAxes.transform([0,0]))\n    trans = matplotlib.transforms.blended_transform_factory(ax1.transData, fig.transFigure)\n    r = matplotlib.patches.Rectangle(xy=(min,bottom), width=max-min, height=top-bottom, transform=trans,\n                                    fc=temp_color_val[i], ec='none', lw=0, zorder=0)\n    fig.add_artist(r,)\n\nfor x,ax in enumerate(axes_list):\n    ax.set_facecolor(BG)\n    ax.set_xlabel('')\n    ax.set_ylabel('')\n    if x > 0:\n        ax.get_legend().remove()\n    ax.spines.bottom.set_visible(False)\n    ax.spines.right.set_visible(False)\n    ax.spines.top.set_visible(False)\n    ax.get_xaxis().set_visible(False)\n    ax.xaxis.set_major_formatter(mtick.PercentFormatter())\n    \n    ax.add_patch(\n        Rectangle(\n            (30,-1), 20, 3, facecolor=color_vals[1],\n        ) )\n\nfig.set_facecolor(BG)\nax4.spines.bottom.set_visible(True)\nax4.get_xaxis().set_visible(True)\n\nlegend_elements = [Patch(facecolor=HEAL[4], label='Highest will to learn'),\n                   Patch(facecolor='none', edgecolor='black', lw=1,  label='% Who already learnt'),\n                   Patch(facecolor=HEAL[1], edgecolor='black', lw=1,  label='Will to Learn in 2020')]\nax1.legend(handles=legend_elements, loc='upper right', facecolor=BG)\n\nax1.annotate('16%', xy=(27.5, 1.2), xytext=(43, 1.25), arrowprops=dict(arrowstyle=']->'), zorder=3)\nax2.annotate('12%', xy=(24.5, 1.2), xytext=(36, 1.25), arrowprops=dict(arrowstyle=']->'), zorder=3)\nax3.annotate('13%', xy=(28.5, 1.2), xytext=(40, 1.25), arrowprops=dict(arrowstyle=']->'), zorder=3)\nax4.annotate('14%', xy=(21, 1.2), xytext=(34, 1.25), arrowprops=dict(arrowstyle=']->'), zorder=3)\n\nplt.figtext(0, 0, 'source: Kaggle Survey 2021, 2022', wrap=True, horizontalalignment='left', fontsize=8, style='oblique')\nplt.figtext(0.05, 1,'Sky is the Limit for Cloud learners at an All time high', fontsize=15, fontname='sans-serif', weight='bold', color=HEAL[2])\nplt.show()","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-11-28T16:19:23.167868Z","iopub.execute_input":"2022-11-28T16:19:23.168289Z","iopub.status.idle":"2022-11-28T16:19:23.737105Z","shell.execute_reply.started":"2022-11-28T16:19:23.168261Z","shell.execute_reply":"2022-11-28T16:19:23.735691Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### <div style=\"font-family: Trebuchet MS; background-color: #272628; color: #FFFFFF; padding: 20px; line-height: 1.5;\">People are commited to the cloud</div>  \n<div style=\"font-family: Trebuchet MS; background-color: #FFFFFF; color: #000000; padding: 12px; line-height: 1.5;\">Around 25% of Kagglers already know one of the Cloud fronts, When Kagglers were a series of Questions about their commitments on Learning new tools, <b>On average, Around 35% of Kagglers asseerted commitment on Learning new data management tools / platforms</b>. The Learners group was highest in 2021 being on an average <b>14% Higher</b> than the group who already knew that skill. Concludingly, We can say, the learners group is bigger and we could see significant growth in the number of cloud practitioners in the future. (Hopefully through kaggles own surveys)</div>  \n<div style=\"font-family: Trebuchet MS; background-color: #FFFFFF; color: #000000; padding: 12px; line-height: 1.5;\"><b>By 2022, public cloud services will be essential for 90% of data and analytics innovation.</b><br>As data and analytics moves to the cloud, data and analytics leaders still struggle to align the right services to the right use cases, which leads to unnecessary increased governance and integration overhead.<br>The question for data and analytics is moving from how much a given service costs to how it can meet the workload’s performance requirements beyond the list price.<br>Data and analytics leaders need to <b>prioritize workloads that can exploit cloud capabilities and focus on cost optimization and other benefits such as change and innovation acceleration when moving to cloud.</b><br><b>-Gartner[11]</b><br>Can you think of one such Leader? Who does this job perfectly?<br>It's this platform you are reading on</div> \n","metadata":{}},{"cell_type":"markdown","source":"___","metadata":{}},{"cell_type":"code","source":"spent_cc = fetch_ts_by_category(frames=[df_21, df_22], year=['2021', '2022'], cat_query=28, ele_query=41)\n\ntop_5 = list(\n        spent_cc.groupby([spent_cc.Category]).agg({'Percent_of_respondents':'sum'}).reset_index().sort_values(by='Percent_of_respondents', ascending=False)['Category'][:5].values\n    )\n\nspent = spent_cc.groupby([spent_cc.Category, spent_cc.Element]).agg({'Percent_of_respondents':'sum'}).reset_index()#['Element'].agg(pd.Series.mode).reset_index()\nspent = spent[spent['Category'].isin(top_5)]\n\nspent['Company'] = spent.apply(lambda row: give_me_company(row), axis=1)\nspent = spent[~(spent['Company'] == 'None or Other')]","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-11-28T16:19:45.987047Z","iopub.execute_input":"2022-11-28T16:19:45.987486Z","iopub.status.idle":"2022-11-28T16:19:47.209779Z","shell.execute_reply.started":"2022-11-28T16:19:45.987459Z","shell.execute_reply":"2022-11-28T16:19:47.208102Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# hbar below\nspent_role = fetch_ts_by_category(frames=[df_21, df_22], year=['2021', '2022'], cat_query=27, ele_query=41)\nspent_role = spent_role.groupby([spent_role.Category]).agg({'Percent_of_respondents':'mean'}).reset_index().sort_values(by='Percent_of_respondents', ascending=False)\nspent_role = spent_role[spent_role.Category.str.contains(pat='Data')]\nspent_role.rename({'Category':'Element'}, axis=1, inplace=True)\nspent_role['Role'] = spent_role.apply(lambda row: give_me_role(row), axis=1)\n\nspent_role = spent_role.groupby([spent_role.Role])['Percent_of_respondents'].sum().reset_index()\nspent_role['share'] = ( spent_role.Percent_of_respondents / spent_role.Percent_of_respondents.sum() ) * 100","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-11-28T16:19:50.733503Z","iopub.execute_input":"2022-11-28T16:19:50.734061Z","iopub.status.idle":"2022-11-28T16:19:51.801267Z","shell.execute_reply.started":"2022-11-28T16:19:50.734028Z","shell.execute_reply":"2022-11-28T16:19:51.800036Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"fig, ax = plt.subplots(figsize=(16,6), dpi=72, tight_layout=True)\nfig.set_facecolor(BG)\nax.set_facecolor(BG)\n\nsns.barplot(data=spent, x='Percent_of_respondents', y='Category', hue='Company', palette=[HEAL[4],HEAL[1],HEAL[0]], ci=None, ax=ax)\n\nlabels = ['Technology','Energy','Insurance','Marketing', 'Online Services']\nax.set_yticklabels(labels)\n\nfor x,patch in enumerate(ax.patches):\n    if x in [0,1,2,3,4]:\n        patch.set_color(HEAL[4])\n    if x in [10,11,12,13,14]:\n        patch.set_color('none')\n    patch.set_edgecolor('black')\n    patch.set_linewidth(1)\n\nax.spines.right.set_visible(False)\nax.spines.top.set_visible(False)\nax.set_ylabel('')\nax.set_xlabel('average Percent of responders')\n\nax.add_patch(\n        Rectangle(\n            (15,-1), 10, 6, facecolor=color_vals[1], zorder=0,\n        ) )\n\nplt.yticks(rotation = 45)\nplt.figtext(0, 0.0, 'source: Kaggle Survey (2020-22)', wrap=True, horizontalalignment='left', fontsize=9, style='oblique')\nplt.figtext(0.1, 1,'Amazon dominates the Data Storage sector across Industries & Commercial usage', fontsize=15, fontname='sans-serif', weight='bold', color=HEAL[2])\n\nfor x in ax.get_yticks():\n    ax.axhline(x, lw=1, color='grey', alpha=0.1, zorder=0)\n\nax.annotate('18% Dominance by Amazon', xy=(19.5, 3.8), xytext=(21, 3.81), arrowprops=dict(arrowstyle=']->'), zorder=3)\nax.annotate('Microsoft competes Head on', xy=(12, .9), xytext=(13, .95), arrowprops=dict(arrowstyle=']->'), zorder=3)\n# ax.text(18,1, 'Top 30%', color=HEAL[1], fontsize=25, weight='heavy')\n\nax.xaxis.set_major_formatter(mtick.PercentFormatter())\nax.set_xlim(-0.2,25)","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-11-28T16:20:19.026909Z","iopub.execute_input":"2022-11-28T16:20:19.027365Z","iopub.status.idle":"2022-11-28T16:20:19.506173Z","shell.execute_reply.started":"2022-11-28T16:20:19.027328Z","shell.execute_reply":"2022-11-28T16:20:19.504724Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"fig, ax = plt.subplots(figsize=(16,1.5), dpi=72, tight_layout=True)\nax.set_xlim(0,1)\n\nax.text(.6, 0.4, 'Data Engineer\\n31.9%', color='white', fontsize=20, weight='heavy')\nax.text(.13, 0.4, 'Data Scientist\\n10.6%', color='white', fontsize=16, weight='heavy')\nax.text(.005, 0.4, 'Data Analyst\\n5.83%', color='white', fontsize=16, weight='heavy')\n\nax.add_patch(\n   Rectangle(\n    (0,0),1,1, facecolor=HEAL[4],edgecolor=HEAL[2], lw=1,\n   )\n)\nax.add_patch(\n   Rectangle(\n    (0,0),0.21 + 0.12 ,1, facecolor=HEAL[2],edgecolor=HEAL[2], lw=1,\n   )\n)\nax.add_patch(\n   Rectangle(\n    (0,0),0.12,1, facecolor=HEAL[1],edgecolor=HEAL[2], lw=1,\n   )\n)\n\nplt.figtext(0.1, 1,'Which Data role harness the most of Data Storage', fontsize=17, fontname='Bahnschrift', weight='bold', color=HEAL[2])\nax.spines.right.set_visible(False)\nax.spines.top.set_visible(False)\nax.spines.bottom.set_visible(False)\nax.spines.left.set_visible(False)\nax.get_yaxis().set_visible(False)\nax.get_xaxis().set_visible(False)","metadata":{"_kg_hide-input":true,"jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2022-11-28T16:02:09.698045Z","iopub.execute_input":"2022-11-28T16:02:09.698636Z","iopub.status.idle":"2022-11-28T16:02:09.80203Z","shell.execute_reply.started":"2022-11-28T16:02:09.698583Z","shell.execute_reply":"2022-11-28T16:02:09.799221Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### <div style=\"font-family: Trebuchet MS; background-color: #272628; color: #FFFFFF; padding: 20px; line-height: 1.5;\">Amazon dominates overall sectors</div>  \n#### <div style=\"font-family: Trebuchet MS; background-color: #FFFFFF; color: #000000; padding: 12px; line-height: 1.5;\">Except Marketing, Google & Google Trends take the lead. With the power of Search, from reverse engineering it to find threats to finding the right customers for your product, Google has managed to make the most of their Cloud Servies.<br>By 2025, there will be over 100 zettabytes of data stored in the cloud. To put this in perspective, a zettabyte is a billion terabytes (or a trillion gigabytes).<br>In the same year, the total global data storage will exceed 200 zettabytes of data, meaning that around half of it will be stored in the cloud. By comparison, only 25 percent of all the computing data was stored this way in 2015.[7]</div>  ","metadata":{}},{"cell_type":"markdown","source":"<center>\n<a href=\"https://www.kaggle.com/pranav941\" rel=\"some text\"><img src=\"https://imgur.com/mDe9qRA.jpg\" width=\"900\"/></a></center>","metadata":{}},{"cell_type":"markdown","source":"___","metadata":{}},{"cell_type":"code","source":"# Cloud Platforms\n\nspent_cc = fetch_ts_by_category(frames=[df_21, df_22], year=['2021', '2022'], cat_query=28, ele_query=36)\n\ntop_5 = list(\n        spent_cc.groupby([spent_cc.Category]).agg({'Percent_of_respondents':'sum'}).reset_index().sort_values(by='Percent_of_respondents', ascending=False)['Category'][:5].values\n    )\n\ntop_5_ele = list(\n        spent_cc.groupby([spent_cc.Element]).agg({'Percent_of_respondents':'sum'}).reset_index().sort_values(by='Percent_of_respondents', ascending=False)['Element'][:5].values\n    )\n\nspent = spent_cc.groupby([spent_cc.Category, spent_cc.Element]).agg({'Percent_of_respondents':'sum'}).reset_index()#['Element'].agg(pd.Series.mode).reset_index()\nspent = spent[(spent['Category'].isin(top_5)) & (spent['Element'].isin(top_5_ele))]\n\nspent['Company'] = spent.apply(lambda row: give_me_company(row), axis=1)\nspent = spent[~(spent['Company'] == 'None or Other')]","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-11-28T16:12:14.756278Z","iopub.execute_input":"2022-11-28T16:12:14.756695Z","iopub.status.idle":"2022-11-28T16:12:16.079075Z","shell.execute_reply.started":"2022-11-28T16:12:14.75666Z","shell.execute_reply":"2022-11-28T16:12:16.077846Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"fig, ax = plt.subplots(figsize=(16,6), dpi=72, tight_layout=True)\nfig.set_facecolor(BG)\nax.set_facecolor(BG)\n\nsns.barplot(data=spent, x='Percent_of_respondents', y='Category', hue='Company', palette=[HEAL[4],HEAL[2],'#9DD6D2', 'grey'], ci=None, ax=ax, edgecolor='black', lw=1)\n\nlabels = ['Technology','Energy','Insurance','Marketing', 'Online Services']\nax.set_yticklabels(labels)\n\nfor x,patch in enumerate(ax.patches):\n    if x in [0,1,2,3,4]:\n        patch.set_color(HEAL[4])\n    if x in [15,16,17,18,19]:\n        patch.set_color('none')\n    patch.set_edgecolor('black')\n    patch.set_linewidth(1)\n\nax.spines.right.set_visible(False)\nax.spines.top.set_visible(False)\nax.set_ylabel('')\nax.set_xlabel('average Percent of responders')\n\n# Green Patch\nax.add_patch(\n        Rectangle(\n            (52,-1), 23, 6, facecolor=color_vals[1], zorder=0,\n        ) )\n\nplt.yticks(rotation = 45)\nplt.figtext(0, 0.0, 'source: Kaggle Survey (2020-22)', wrap=True, horizontalalignment='left', fontsize=9, style='oblique')\nplt.figtext(0.1, 1,'Amazon crushes the competition with ~2x preference', fontsize=17, fontname='Bahnschrift', weight='bold', color=HEAL[2])\n\nfor x in ax.get_yticks():\n    ax.axhline(x, lw=1, color='grey', alpha=0.1, zorder=0)    \n\nax.xaxis.set_major_formatter(mtick.PercentFormatter())\nax.set_xlim(0,75)","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-11-28T16:20:44.723714Z","iopub.execute_input":"2022-11-28T16:20:44.72417Z","iopub.status.idle":"2022-11-28T16:20:45.074361Z","shell.execute_reply.started":"2022-11-28T16:20:44.724116Z","shell.execute_reply":"2022-11-28T16:20:45.072921Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"role_share = fetch_ts_by_category(frames=[df_21, df_22], year=['2021', '2022'], cat_query=27, ele_query=36)\nrole_share = role_share.groupby([role_share.Category]).agg({'Percent_of_respondents':'mean'}).reset_index().sort_values(by='Percent_of_respondents', ascending=False)\n\nrole_share = role_share[role_share.Category.str.contains(pat='Data')]\n\nrole_share.rename({'Category':'Element'}, axis=1, inplace=True)\n\nrole_share['Role'] = role_share.apply(lambda row: give_me_role(row), axis=1)\n\nrole_share = role_share.groupby([role_share.Role])['Percent_of_respondents'].sum().reset_index()\nrole_share['share'] = ( role_share.Percent_of_respondents / role_share.Percent_of_respondents.sum() ) * 100","metadata":{"_kg_hide-input":true,"jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2022-11-28T16:02:11.801321Z","iopub.execute_input":"2022-11-28T16:02:11.801692Z","iopub.status.idle":"2022-11-28T16:02:13.156016Z","shell.execute_reply.started":"2022-11-28T16:02:11.801666Z","shell.execute_reply":"2022-11-28T16:02:13.15522Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"fig, ax = plt.subplots(figsize=(16,1.5), dpi=72, tight_layout=True)\nax.set_xlim(0,1)\n\nax.add_patch(\n   Rectangle(\n    (0,0),1,1, facecolor=HEAL[4], edgecolor=HEAL[2], lw=1,\n   )\n)\nax.add_patch(\n   Rectangle(\n    (0,0),0.22 + 0.14 ,1, facecolor=HEAL[2], edgecolor=HEAL[2], lw=1,\n   )\n)\nax.add_patch(\n   Rectangle(\n    (0,0),0.14,1, facecolor=HEAL[1], edgecolor=HEAL[2], lw=1,\n   )\n)\nax.text(.6, 0.4, 'Data Engineer\\n36%', color='white', fontsize=20, weight='heavy')\nax.text(.145, 0.4, 'Data Analyst\\n13%', color='white', fontsize=16, weight='heavy')\nax.text(.005, 0.4, 'Data Scientist\\n8.58%', color='white', fontsize=16, weight='heavy')\n\nplt.figtext(0.1, 1,'Which Data role harness the most of Cloud Platforms', fontsize=17, fontname='Bahnschrift', weight='bold', color=HEAL[2])\nax.spines.right.set_visible(False)\nax.spines.top.set_visible(False)\nax.spines.bottom.set_visible(False)\nax.spines.left.set_visible(False)\nax.get_yaxis().set_visible(False)\nax.get_xaxis().set_visible(False)","metadata":{"_kg_hide-input":true,"jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2022-11-28T16:02:13.157509Z","iopub.execute_input":"2022-11-28T16:02:13.158123Z","iopub.status.idle":"2022-11-28T16:02:13.262307Z","shell.execute_reply.started":"2022-11-28T16:02:13.158084Z","shell.execute_reply":"2022-11-28T16:02:13.261198Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### <div style=\"font-family: Trebuchet MS; background-color: #272628; color: #FFFFFF; padding: 20px; line-height: 1.5;\">The Head start pays off</div>  \n<div style=\"font-family: Trebuchet MS; background-color: #6FA4B5; color: #FFFFFF; padding: 20px; line-height: 1.5;\">Amazon had around <b>7 years before any significant challenger approached</b>, This gave them enough time to develop cutting edge tools and technologies for every business needs</div>\n<div style=\"font-family: Trebuchet MS; background-color: #FFFFFF; color: #000000; padding: 20px; line-height: 1.5;\">AThe COVID-19 pandemic profoundly affected many aspects of our lives, and work was no exception. Faced with lockdowns, social distancing and similar measures, a large portion of the workforce needed to move online. According to remote work statistics by Cloudwards, <b>approximately 34 percent of workers say that they prefer to work in the cloud and will look for a new job if they are required to return to the office</b>[7]</div>","metadata":{}},{"cell_type":"markdown","source":"___","metadata":{}},{"cell_type":"code","source":"# Big Data Products\nspent_cc = fetch_ts_by_category(frames=[df_21, df_22], year=['2021', '2022'], cat_query=28, ele_query=43)\n\ntop_5 = list(\n        spent_cc.groupby([spent_cc.Category]).agg({'Percent_of_respondents':'sum'}).reset_index().sort_values(by='Percent_of_respondents', ascending=False)['Category'][:5].values\n    )\n\ntop_5_ele = list(\n        spent_cc.groupby([spent_cc.Element]).agg({'Percent_of_respondents':'sum'}).reset_index().sort_values(by='Percent_of_respondents', ascending=False)['Element'][:7].values\n    )\n\nspent = spent_cc.groupby([spent_cc.Category, spent_cc.Element]).agg({'Percent_of_respondents':'sum'}).reset_index()#['Element'].agg(pd.Series.mode).reset_index()\n\nspent = spent[(spent['Category'].isin(top_5)) & (spent['Element'].isin(top_5_ele))]","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-11-28T16:21:20.22929Z","iopub.execute_input":"2022-11-28T16:21:20.229694Z","iopub.status.idle":"2022-11-28T16:21:21.941359Z","shell.execute_reply.started":"2022-11-28T16:21:20.229653Z","shell.execute_reply":"2022-11-28T16:21:21.939803Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def give_me_big_data(row):\n    if 'Amazon' in row['Element']:\n        return 'Amazon'\n    if 'Google' in row['Element']:\n         return 'Google'\n    if 'Microsoft' in row['Element']:\n         return 'Microsoft'\n    if 'IBM' in row['Element']:\n         return 'IBM'\n    if 'MySQL' in row['Element']:\n         return 'MySQL'\n    if 'Postgre' in row['Element']:\n         return 'PostgreSQL'\n    if 'SQLite' in row['Element']:\n         return 'SQLite'\n    return 'None or Other'\n\nspent['Company'] = spent.apply(lambda row: give_me_big_data(row), axis=1)\nspent = spent[~(spent['Company'] == 'None or Other')]","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-11-28T16:21:24.558581Z","iopub.execute_input":"2022-11-28T16:21:24.558998Z","iopub.status.idle":"2022-11-28T16:21:24.571626Z","shell.execute_reply.started":"2022-11-28T16:21:24.558964Z","shell.execute_reply":"2022-11-28T16:21:24.570504Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"fig, ax = plt.subplots(figsize=(16,9), dpi=72, tight_layout=True)\nfig.set_facecolor(BG)\nax.set_facecolor(BG)\n\nsns.barplot(data=spent, x='Percent_of_respondents', y='Category', hue='Company', ci=None, ax=ax, palette=[HEAL[1],'#9DD6D2',HEAL[4],HEAL[2],HEAL[0]], edgecolor='black', lw=1)\n\nlabels = ['Technology','Energy','Insurance','Marketing', 'Online Services']\nax.set_yticklabels(labels)\n\nfor x,patch in enumerate(ax.patches):\n    if x in [15,16,17,18,19]:\n        patch.set_color(HEAL[2])\n    if x in [10,11,12,13,14]:\n        patch.set_color(HEAL[4])\n    patch.set_edgecolor('black')\n    patch.set_linewidth(1)\n\nax.spines.right.set_visible(False)\nax.spines.top.set_visible(False)\nax.set_ylabel('')\nax.set_xlabel('average Percent of responders')\n\n# Green Patch\nax.add_patch(\n        Rectangle(\n            (52,-1), 23, 6, facecolor=color_vals[1], zorder=0,\n        ) )\n\nplt.yticks(rotation = 45)\nplt.figtext(0, 0.0, 'source: Kaggle Survey (2020-22)', wrap=True, horizontalalignment='left', fontsize=9, style='oblique')\nplt.figtext(0.1, 1,'Big Data tools faces the \"Rauh Welt Begriff\"\\nIts a rough & competetive world for them', fontsize=17, fontname='sans-serif', weight='bold', color=HEAL[2])\n\nfor x in ax.get_yticks():\n    ax.axhline(x, lw=1, color='grey', alpha=0.1, zorder=0)\n\nax.annotate('MySQL Dominates', xy=(56, .95), xytext=(60, .99), arrowprops=dict(arrowstyle=']->'), zorder=3)\nax.annotate('Closest Postgre can come to MySQL', xy=(44, 1.79), xytext=(50, 1.82), arrowprops=dict(arrowstyle=']->'), zorder=3)\n\nax.xaxis.set_major_formatter(mtick.PercentFormatter())\nax.set_xlim(-0.1,75)","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-11-28T16:26:41.949234Z","iopub.execute_input":"2022-11-28T16:26:41.949583Z","iopub.status.idle":"2022-11-28T16:26:42.722347Z","shell.execute_reply.started":"2022-11-28T16:26:41.949554Z","shell.execute_reply":"2022-11-28T16:26:42.721412Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<b>SQL is old, Boom. I said it,</b> Its atleast 50 years old when a Computer Scientist at IBM suggested a new system for organizig their Data, <b>Why do you need it 50yrs later?</b> One might as well focus on the 'sexier' part of Data Science? Heres 3 quick reasons why you should focus on query languages<br><b>A. SQL is Everywhere</b><br>Virtually all big tech companies use SQL. Uber, Netflix, Airbnb — the list goes on. Even Fortune 500 companies that have built their own high-performance database systems (Facebook, Google, Amazon) still frequently use SQL to query data and perform analyses.<br><b>B. SQL Is in Demand</b><br>If you want to get a job in data, you should focus on the skills employers actually want. As we learnt from our keywords model, more than 50% of listing mention SQL as their key skill<br><b>C. SQL is still the Top Language at Data Work</b><br>SQL is one of the most-used languages in the entire tech industry!. According to Stack Overflow’s 2022 developer survey, SQL eclipses even Python in terms of popularity. In fact, it’s the third-most-popular programming language among all professional developers","metadata":{}},{"cell_type":"code","source":"role_share = fetch_ts_by_category(frames=[df_21, df_22], year=['2021', '2022'], cat_query=27, ele_query=43)\nrole_share = role_share.groupby([role_share.Category]).agg({'Percent_of_respondents':'mean'}).reset_index().sort_values(by='Percent_of_respondents', ascending=False)\n\nrole_share = role_share[role_share.Category.str.contains(pat='Data')]\n\nrole_share.rename({'Category':'Element'}, axis=1, inplace=True)\n\nrole_share['Role'] = role_share.apply(lambda row: give_me_role(row), axis=1)\n\nrole_share = role_share.groupby([role_share.Role])['Percent_of_respondents'].sum().reset_index()\nrole_share['share'] = ( role_share.Percent_of_respondents / role_share.Percent_of_respondents.sum() ) * 100","metadata":{"_kg_hide-input":true,"jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2022-11-28T16:02:15.379388Z","iopub.execute_input":"2022-11-28T16:02:15.379686Z","iopub.status.idle":"2022-11-28T16:02:16.833048Z","shell.execute_reply.started":"2022-11-28T16:02:15.379657Z","shell.execute_reply":"2022-11-28T16:02:16.831976Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"fig, ax = plt.subplots(figsize=(16,1.5), dpi=72, tight_layout=True)\nax.set_xlim(0,1)\n\nax.add_patch(\n   Rectangle(\n    (0,0),1,1, facecolor=HEAL[4], edgecolor=HEAL[2], lw=1,\n   )\n)\nax.add_patch(\n   Rectangle(\n    (0,0),0.20 + 0.13 ,1, facecolor=HEAL[2], edgecolor=HEAL[2], lw=1,\n   )\n)\nax.add_patch(\n   Rectangle(\n    (0,0),0.13,1, facecolor=HEAL[1], edgecolor=HEAL[2], lw=1,\n   )\n)\nax.text(.6, 0.4, 'Data Engineer\\n40%', color='white', fontsize=20, weight='heavy')\nax.text(.145, 0.4, 'Data Analyst\\n12.42%', color='white', fontsize=16, weight='heavy')\nax.text(.005, 0.4, 'Data Scientist\\n8.34%', color='white', fontsize=16, weight='heavy')\n\nplt.figtext(0.1, 1,'Which Data role harness the most of Big Data Tools', fontsize=17, fontname='Bahnschrift', weight='bold', color=HEAL[2])\n\nax.spines.right.set_visible(False)\nax.spines.top.set_visible(False)\nax.spines.bottom.set_visible(False)\nax.spines.left.set_visible(False)\nax.get_yaxis().set_visible(False)\nax.get_xaxis().set_visible(False)","metadata":{"_kg_hide-input":true,"jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2022-11-28T16:02:16.834306Z","iopub.execute_input":"2022-11-28T16:02:16.834583Z","iopub.status.idle":"2022-11-28T16:02:16.93673Z","shell.execute_reply.started":"2022-11-28T16:02:16.834552Z","shell.execute_reply":"2022-11-28T16:02:16.935716Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### <div style=\"font-family: Trebuchet MS; background-color: #272628; color: #FFFFFF; padding: 20px; line-height: 1.5;\">Big Data Analytics roles to witness huge demand</div>  \n<div style=\"font-family: Trebuchet MS; background-color: #FFFFFF; color: #000000; padding: 12px; line-height: 1.5;\"><b>96% of companies are definitely planning or likely to plan to hire new staff with relevant skills to fill future big data analytics related roles</b> in 2022. This is most likely going to be the most in-demand role in 2022. Data Science, Cloud Computing and Machine Learning roles captured most of the e-recruitment market.<br><b>-Monster</b></div>  ","metadata":{}},{"cell_type":"markdown","source":"___","metadata":{}},{"cell_type":"markdown","source":"#### <div style=\"font-family: Trebuchet MS; background-color: #6FA4B5; color: #FFFFFF; padding: 12px; line-height: 1.5;\">How cloud played a role for this analysys</div>  \n\n<center>\n<a href=\"https://www.kaggle.com/pranav941\" rel=\"some text\"><img src=\"https://imgur.com/TJOgTUe.jpg\" width=\"900\"/></a></center>\n\n\n<div style=\"font-family: Trebuchet MS; background-color: #FFFFFF; color: #000000; padding: 12px; line-height: 1.5;\">After collecting Data from <b>Googles Job aggregation page using serpAPI</b>, The tool scrapped different job descriptions and extracted keywords, Few of the features to mention were<br>A. Job Title aka Role<br>B. Type of Job (Full time, Contract, etc)<br>C. Work Location (US)<br>D. Preffered Skills<br>E. Salary<br>\nThe Data was stored into <b>Google's Cloud storage platform and fed forward to Big Query</b>, Where the tables were stored since having to run the script and uploading everyday wasnt very much like a Good Data Professional, This <b>process was automated and ran everyday automatically</b>. Further, The Keywords were extracted and stored into a Public Table (Please refer references) for everyone to use.<br>Harnessing the power of a Cloud compute Platform is not that difficult, <b>A quick started guide on Big Query could be found in Kaggles own Learning Platform (kaggle.com/learn) </b></div>  ","metadata":{}},{"cell_type":"markdown","source":"___","metadata":{}},{"cell_type":"markdown","source":"<center>\n<a href=\"https://www.kaggle.com/pranav941\" rel=\"some text\"><img src=\"https://imgur.com/N1V9dYL.jpg\" width=\"900\"/></a></center>","metadata":{}},{"cell_type":"markdown","source":"___","metadata":{}},{"cell_type":"code","source":"weights_without_role = fetch_ts([df_22], 25, ['2022'], merge=False, append_year=False)\nweights_without_role = weights_without_role[~(weights_without_role.Element.isin(['Other storage services (i.e. google drive)','No, I do not download pre-trained model weights on a regular basis']))]\n\nfig, ax = plt.subplots(figsize=(16,9), dpi=72)\nax.set_facecolor(BG)\nfig.set_facecolor(BG)\n\nplt.title('Kaggle leads the way into the Future', fontsize=17, fontname='Bahnschrift', weight='bold', color=HEAL[2])\n\n_ = plt.pie(weights_without_role['Percent_of_respondents'], labels = weights_without_role['Element'], colors = STA_HEATMAP, autopct='%.0f%%')","metadata":{"_kg_hide-input":true,"jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2022-11-28T16:02:16.938968Z","iopub.execute_input":"2022-11-28T16:02:16.939317Z","iopub.status.idle":"2022-11-28T16:02:17.131905Z","shell.execute_reply.started":"2022-11-28T16:02:16.939285Z","shell.execute_reply":"2022-11-28T16:02:17.130717Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### <div style=\"font-family: Trebuchet MS; background-color: #272628; color: #FFFFFF; padding: 20px; line-height: 1.5;\">Kaggle becomes the 'Hub' for Pre-Trained models</div>  \n#### <div style=\"font-family: Trebuchet MS; background-color: #6FA4B5; color: #FFFFFF; padding: 12px; line-height: 1.5;\">Building a model from scratch requires a great deal of time and effort. When innovations are happening at a break-neck speed, concentrating all the efforts on building models could prove counterproductive. Apart from time and effort, building models could also set companies back financially, especially startups.<br>Enter pre-trained models. Put simply, it is a model created by a third party to solve a business problem. While it may not be 100 percent accurate, it saves time and effort in building a model from the ground up.<br>-Shraddha Goled, Analytics Mag India</div>  \n##### <div style=\"font-family: Trebuchet MS; background-color: #FFFFFF; color: #000000; padding: 12px; line-height: 1.5;\"><b>\"Not everyone should code\" & \"Not everyone should build models\" - Pranav.</b><br>Using a pre trained weight has its own advantages,<br>A. I do not need <b>enormous amounts of Data</b> to build a model from Scratch<br>B. They are simply <b>cost effective and saves Time</b><br>C. Only Thing I need is a Internet connection and a <b><u>platform to host my Notebook on</u></b>, Just like this one</div>  ","metadata":{}},{"cell_type":"code","source":"research_weight = fetch_ts_by_category(frames=[df_22], year=['2022'], cat_query=8, ele_query=25)\nresearch_weight = research_weight[~(research_weight['Element'].isin(['Other storage services (i.e. google drive)','No, I do not download pre-trained model weights on a regular basis']))]\nresearch_weight = research_weight.groupby([research_weight.Category])['Percent_of_respondents'].sum().reset_index()\nresearch_weight['X'] = 'Researcher using Pre-Trained Weights'\n\nfig, ax = plt.subplots(figsize=(5,9))\nax.set_facecolor(BG)\nfig.set_facecolor(BG)\n\ns = sns.barplot(data=research_weight, x='X', y='Percent_of_respondents', hue='Category', palette=[HEAL[2], HEAL[4]], edgecolor='black', lw=1)\nshow_values(s, space=0.05)\nplt.title('Researchers prefer the Smart way', fontsize=17, fontname='Bahnschrift', weight='bold', color=HEAL[2])\n\nax.yaxis.set_major_formatter(mtick.PercentFormatter())\n\nax.spines.top.set_visible(False)\nax.spines.right.set_visible(False)\nax.set_xlabel('')\nax.set_ylabel('')","metadata":{"_kg_hide-input":true,"jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2022-11-28T16:02:17.133712Z","iopub.execute_input":"2022-11-28T16:02:17.134046Z","iopub.status.idle":"2022-11-28T16:02:17.466712Z","shell.execute_reply.started":"2022-11-28T16:02:17.134017Z","shell.execute_reply":"2022-11-28T16:02:17.464692Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"___","metadata":{}},{"cell_type":"markdown","source":"### <div style=\"font-family: Trebuchet MS; background-color: #272628; color: #FFFFFF; padding: 20px; line-height: 1.5;\">A Quick summary</div>  ","metadata":{}},{"cell_type":"markdown","source":"Lets face it, Data professionals tend to emphasize on <b>Python every now and then, which could potentially undermine other Tools</b>, 8 out of 10 respondents would statisfy the requirement of SQL on a Job Description even tho its being reccommended so less.  \nKagglers are definitely committed to the cloud, Now more than ever, They are opting in to learn Cloud Tools & Products, One would not want to miss this tide and consider learning a Big Data management tool, We could possibly its outcomes in coming surveys.  \nAs stated by Cloudwards, <b>3 out of 10 workers would prefer to work in the Cloud</b> and would consider looking for a new job if asked to return to office, Cloud is not just a cost and time saving tool, Its a leisure one would not afford loosing.  \nBig Data & Analytics would possibily see a huge rise in Demand, all the while the <b>deficit being high as well</b>, This would be a great opportunity for <b>aspirants to munch into and get ahead of the competition. </b>\nSupplementing on the Cloud, We realized that not everything should be built from scratch, <b>Building ML Models can be tedious but finding one on Kaggle is not.</b>  \nCombined with the power of Low Code - No Code platforms, I would highly encourage the use of Weights to try and find a solution, futhermore contributing to open-source development, <b>Save time, Save money, Save Silion & Ultimately Save the planet.  </b>\nHaving Cloud proffeciency would not only cut down costs on organizational levels but also help you down at an Individual level, You do not have to worry about how fast your silicon is, how would you store and handle the enormous amounts of information at Hand.\n","metadata":{}},{"cell_type":"markdown","source":"___","metadata":{}},{"cell_type":"code","source":"top = pd.read_csv('/kaggle/input/data-analyst-skill-analysis/kw_top_tools.csv')\nfig, ax = plt.subplots(figsize=(16,9))\nax.set_facecolor(BG)\nfig.set_facecolor(BG)\n\nbar = sns.barplot(data=top, x='keywords', y='percentage',palette=HEAL, edgecolor='black', lw=1)\nshow_values(bar, space=0.05)\n\nplt.title('If you havent realized yet, You have already figured out you can easily statisfy\\nmost of these Requirements for a Data Analyst', loc='left', fontsize=17, fontname='Bahnschrift', weight='bold', color=HEAL[2])\n\nlabels = [str (item.get_text()).capitalize() for item in ax.get_xticklabels()]\nax.set_xticklabels( labels )\n\nax.spines.top.set_visible(False)\nax.spines.right.set_visible(False)\n\nax.set_xlabel('Tool')\nax.set_ylabel('Probability of Occurence')","metadata":{"_kg_hide-input":true,"jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2022-11-28T16:02:17.46877Z","iopub.execute_input":"2022-11-28T16:02:17.469397Z","iopub.status.idle":"2022-11-28T16:02:17.903006Z","shell.execute_reply.started":"2022-11-28T16:02:17.469342Z","shell.execute_reply":"2022-11-28T16:02:17.901592Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"___\n### References  \n[1] Talks at Google - [Storytelling with Data](https://talksat.withgoogle.com/talk/storytelling-with-data)  \n[2] Harvard Business Review - [Why planning is not Strategy](https://online.hbs.edu/blog/post/why-is-strategic-planning-important)  \n[3] Matplotlib - [Documentation](https://matplotlib.org/stable/index.html)  \n[4] Layoffs.fyi - [Layoffs data compiler](https://layoffs.fyi/)  \n[5] Wired - [Data is the new oil](https://www.wired.com/insights/2014/07/data-new-oil-digital-economy/)  \n[6] Harvard Business Review - [How to Protect your job in a Recession](https://hbr.org/2008/09/how-to-protect-your-job-in-a-recession)  \n[7] Cloudwars - [Cloud Computing Statistics](https://www.cloudwards.net/cloud-computing-statistics/)  \n[8] Analytics Mag India - [Why Companies prefer Pre-Trained Models](https://analyticsindiamag.com/why-do-companies-prefer-pre-trained-models/)  \n[9] Forbes - [Data Analytics Profession, Three Trends that Matter](https://www.forbes.com/sites/bernhardschroeder/2021/06/11/the-data-analytics-profession-and-employment-is-exploding-three-trends-that-matter/?sh=1fbef7063f81)  \n[10] MIT - [How covid-19 is disrupting data analytics strategies](https://mitsloan.mit.edu/ideas-made-to-matter/how-covid-19-disrupting-data-analytics-strategies)  \n[11] Gartner - [10 Trends in Data Analytics after 2020](https://www.gartner.com/smarterwithgartner/gartner-top-10-trends-in-data-and-analytics-for-2020)  \n[12] Luke Barousse's Open source project - [Skills for a Data Analyst](https://www.kaggle.com/code/lukebarousse/data-analyst-skill-analysis#More-EDA-of-Top-Skills) | [Video on the process](https://youtu.be/iNEwkaYmPqY) | [subReddit](https://www.reddit.com/r/DataNerd/)  \n  \n___","metadata":{}},{"cell_type":"markdown","source":"Talking about open source, I have now more than ever realized the value of Libraries and re-usable code, Thanks to kaggle for hosting this competition, I would make a solution available in a seperate notebook soon which could be used to slice data easily for any past and future kaggle surveys using a simple 'Look Up' spreadsheet.  \n  \n<b>Thanks for taking your precious time out and going through the Kernel and reading so far, I would be really happy to hear your thoughts (criticism is welcomed) on improvements and suggestions,\nHappy kaggling!</b>","metadata":{}}]}